{"anchor": "TSMC Risk. There are some near ready foundries in the US and in EU, not to mention South Korea. It would take a few years to catch up of course. What I worry more about is the full lock-in of TSMC production capacity by nvidia/apple/amd/etc for their chips on their latest and greatest silicon process (aka the best in the world). There is 'no space' for performant large RISC-V implementations or other alternative (and it will require several iterations and mistakes will be made) \"AI has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion\" ? Arguably false. Why do you think the US has encouraged TSMC foundries, now inside Arizona ? It's obviously to protect against the scenario that China takes Taiwan. In that case, give it 6 months or less for US TSMC foundries to produce the finest. China taking Taiwan will likely not result in the CCP getting any technology, certainly Taiwanese have \"contingency plans\" to vaporize all tech in the event they are invaded. > AI has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion Taiwan has missiles with the range and warheads to strike the three gorges dam. An attack by China would end very poorly for everybody.  There are millions of people living in the inundation zone. From a NatSec perspective, TSMC isn't really a bottleneck - most weapon systems use SoCs and microcontrollers that can be fabbed on \"legacy nodes\" (ie. 28/40/60/90nm) or 14/20/22nm nodes, and compound semiconductors. The ability to mass produce a Pascal or Volta comparable GPU or Apple A11 comparable SoC is all you need for more cutting edge systems. Power Electronics and Compound Semiconductors (GaN, SiC) have historically been the biggest bottleneck. The bigger risk for the TSMC-China aspect is TSMC's planned exit of GaN foundry production by 2027. Most Chinese manufacturers  still  depend on TSMC-produced GaNs wafers instead of domestically produced Ga", "positive": "My trick for getting consistent classification from LLMs. If you already have your categories defined, you might even be able to skip a step and just compare embeddings. I wrote a categorization script that sorts customer-service calls into one of 10 categories.  Wrote descriptions of each category, then translated into embedding. Then created embeddings for the call notes and matched to closest category using cosine_similarity. Arthur\u2019s classifier will only be as accurate as their retrieval. The approach depends on the candidates to be the correct ones for classification to work. Under-discussed superpower of LLMs is open-set labeling, which I sort of consider to be inverse classification. Instead of using a static set of pre-determined labels, you're using the LLM to find the semantic clusters within a corpus of unstructured data. It feels like \"data mining\" in the truest sense. Dunno if this passes the bootstrapping test. This is sensitive to the initial candidate set of labels that the LLM generates. Meaning if you ran this a few times over the same corpus, you\u2019ll probably get different performance depending upon the order of the way you input the data and the classification tag the LLM ultimately decided upon. Here\u2019s an idea that is order invariant: embed first, take samples from clusters, and ask the LLM to label the 5 or so samples you\u2019ve taken. The clusters are serving as soft candidate labels and the LLM turns them into actual interpretable explicit labels. I think a less order biased, more straightforward way would be just to vectorize everything, perform clustering and then label the clusters with the LLM. Nice! So the cache check tries to find if a previously existing text embedding has >0.8 match with the current text. If you get a cache hit here, iiuc, you return that matched' text label right away. But do you also insert a text embedding of the current text in the text embeddings table? Or do you only insert it in case of cache miss? From reading the ", "negative": "Will AIs take all our jobs and end human history, or not? (2023). ChatGPT, please summarise this long essay by Stephen Wolfram into a couple of pithy sentences: TLDR: AI won\u2019t \u201cend work\u201d so much as endlessly move the goalposts, because the universe itself is too computationally messy to automate completely. The real risk isn\u2019t mass unemployment\u2014it\u2019s that we\u2019ll have infinite machine intelligence and still argue about what\u2019s worth doing. AI is evolving so fast, and you post an article from 2023? Even if they automate all our current jobs uniquely human experiences will always be valuable to us and will always have demand. More food chains keep opening up even when there is plenty of food available. The pie just gets bigger. Every tech shift was supposed to \"end work\" and yet here we are, busy with jobs that didn't exist 20 years ago. The real issue isn't jobs dying. It's who gets the money from all this and whether new needs show up fast enough to give people something to do. With software we don't really know the limit yet, unlike food where your stomach tells you when to stop. Outside of America people aren't really stressed about AI. Like you go to Vietnam or Vienna they mostly just think that they will have a good life with AI. It's uniquely American to believe that your life will end when AI takes your job. The problem isn't the AI it's that your access to basic rights is intermediated by a corporate job. American's need to decenter their self worth from their jobs.  Like when I quit Microsoft I literally thought I was dying, but that's all an illusion from the corporations. If all jobs were taken by AI in a short time span, the companies owning and operating those AIs would go out of business as no one would be able to afford the products made by the AIs. This is an unlikely scenario. Not all things will be made/run by AIs in a short time. It is far more likely that specific jobs in specific industries will be taken by AI, and AI will slowly take the labor marke"}
{"anchor": "The Adolescence of Technology. Initial thought about 1/5th of the way through: Wow, that's a lot of em-dashes! i wonder how much of this he actually wrote? Edit: Okay, section 3 has some interesting bits in it. It reminds me of all those gun start-ups in Texas that use gyros and image recognition to turn a C- shooter into an A- shooter. They all typically get bought up quite fast by the government and the tech shushed away. But the ideas are just too easy now to implement these days. Especially with robots and garage level manufacturing, people can pretty much do what they want. I think that means we have to make people better people then? Is that even a thing? Edit 2: Wow, section 4 on the abuse by organizations with AI is the most scary. Yikes, I feel that these days with Minneapolis. They're already using Palantir to try some of it out, but are being hampered by, well, themselves. Not a good fallback strat for anyone that is not the government. The thing about the companies just doing it before releasing it, that I think is underrated. Whats to stop sama from just, you know, taking one of these models and taking over the world? Like, is this paper saying that nothing is stopping him? The big one that should send  huge  chills down the spines of any country is this bit: \"My worry is that I\u2019m not totally sure we can be confident in the nuclear deterrent against a country of geniuses in a datacenter: it is possible that powerful AI could devise ways to detect and strike nuclear submarines, conduct influence operations against the operators of nuclear weapons infrastructure, or use AI\u2019s cyber capabilities to launch a cyberattack against satellites used to detect nuclear launches\" What. The. Fuck. Is he saying that the nuclear triad is under threat here from AI? Am I reading this right? That  alone  is reason to abolish the whole thing in the eyes of nuclear nations. This, I think, is the most important part of the whole essay. Holy shit. Edit 3: Okay, section 4 on th", "positive": "UN declares that the world has entered an era of 'global water bankruptcy'.  Four billion people face severe water scarcity for at least one month each year  Does anyone know what this looks like for typical cases? The water just cuts off for a month in some places I guess? I can assure you there is plenty of water.    There are floods in lots of places every year.   The oceans are full of water that for just 5kWh we can desalinate 250 gallons. The problem is that the water and energy aren't where the users want it to be. But pipes are relatively cheap - if humanity cared enough, we could build pipes to distribute the plentiful water everywhere. But it turns out the people without much water tend to be in very poor places and warzones where there isn't much appetite for spending money on pipes. And all these huge new data centers are gonna make things worse:  https://www.eesi.org/articles/view/data-centers-and-water-co...  Before commenting water is cheap and plentiful please read the proposed definition. > Water bankruptcy refers to \u201ca state in which a human-water system has spent beyond its hydrological means for so long that it can no longer satisfy the claims upon it without inflicting unacceptable or irreversible damage to nature.\u201d I find this other article [1] more informative, including for instance the global map of Vulnerability to Water-Related Challenges taken from the actual report [2]. [1]  https://www.thebrighterside.news/post/our-world-is-entering-...  [2]  https://collections.unu.edu/eserv/UNU:10445/Global_Water_Ban...  I would no say the \"world\", but areas of it has as noted.  Like South Asia, SW N America, N Africa and Spain. For many of these areas, desalination could meet the gap, but someone will need to pay for it.  That is the main issue, no one wants to pay. Sounds like a bunch of useless scare mongering. Large scale Desalination is getting increasingly achievable:\n https://caseyhandmer.wordpress.com/2022/11/20/we-need-more-w...  Reminds me o", "negative": "The browser is the sandbox. I like the perspective used to approach this. Additionally, the fact that major browsers can accept a folder as input is new to me and opens up some exciting possibilities. The folder input thing caught me off guard too when I first saw it. I've been building web apps for years and somehow missed that `webkitdirectory` attribute. What I find most compelling about this framing is the maturity argument. Browser sandboxing has been battle-tested by billions of users clicking on sketchy links for decades. Compare that to spinning up a fresh container approach every time you want to run untrusted code. The tradeoff is obvious though: you're limited to what browsers can do. No system calls, no arbitrary binaries, no direct hardware access. For a lot of AI coding tasks that's actually fine. For others it's a dealbreaker. I'd love to see someone benchmark the actual security surface area. \"Browsers are secure\" is true in practice, but the attack surface is enormous compared to a minimal container. We never say that it isn't. There is a reason Google developed NaCl in the first place that inspired WebAssembly to become the ultimate sandbox standard. Not only that, DOM, JS and CSS also serves as a sandbox of rendering standard, and the capability based design is also seen throughout many browsers even starting with the Netscape Navigator. Locking down features to have a unified experience is what a browser should do, after all, no matter the performance. Of course there are various vendors who tried to break this by introducing platform specific stuff, but that's also why IE, and later Edge (non-chrome) died a horrible death There are external sandbox escapes such as Adobe Flash, ActiveX, Java Applet and Silverlight though, but those external escapes are often another sandbox of its own, despite all of them being a horrible one... But with the stabilization of asm.js and later WebAssembly, all of them is gone with the wind. Sidenote: Flash's script"}
{"anchor": "London\u2013Calcutta bus service. While money would be in the okay range, few can afford 50 days off. Very impressive! I got curious and found this photo and brochure from the Indian Memory Project [1] [1]  https://xcancel.com/Indianmemory/status/1277521026813882368#...  Reminder that everything was better in the past. Prior to WW1 you could travel around most of Europe without even a passport too. A picture would have been a great addition to the article. Well, these days you can catch a Flixbus from London to Sofia for a mere 150 Europounds, and 48 hours of your time. And from there, Calcutta can't be that far, right? (But, seriously, you can probably do it in another 48 hours...) Similar discussion last year:  https://news.ycombinator.com/item?id=40649091  Interesting. I did a quick search but doesn\u2019t look like there are any personal stories on this. Can see it having been a unique experience, bonding with people over such a long period of time. Looks like photos from inside the bus are also not available sadly. > In 1957, a one-way ticket cost \u00a385 (equivalent to \u00a32,589 in 2023), rising to \u00a3145 by 1973 (equivalent to \u00a32,215 in 2023). Oof that really puts inflation into perspective doesn\u2019t it? Back in the 60s, my partner's mother drove all the way from London to Afghanistan in a tiny Fiat 500. This was a family of four! The past really is a foreign country sometimes. A very different scale, but this reminded of the Green Tortoise which was an American, mostly West Coast affair that once ranged from Alaska to Belize.  https://en.wikipedia.org/wiki/Green_Tortoise  edit: oh wow. It still runs!  http://www.greentortoise.com/  50 days one way? Some research shows it was \u00a385 vs. \u00a3200-\u00a3400 for a one-way plane ticket. What is the use case for this? I guess: - very motivated to go - plan to stay for a very long time - absolutely CANNOT afford a plane ticket - or, afraid of flying Reminds me of a lot of Amtrack routes in the US. I looked at trips from NYC to Chicago. I thought i", "positive": "Qwen3-Max-Thinking. Aghhh, I wished they release a model which outperforms Opus 4.5 in agentic coding in my earlier comments, seems I should wait more. But I am hopeful I don't see a hugging face link, is Qwen no longer releasing their models? I tried to search, could not find anything, do they offer subscriptions? Or only pay per tokens? I just wanted to check whether there is any information about the pricing. Is it the same as Qwen Max? Also, I noticed on the pricing page of Alibaba Cloud that the models are significantly cheaper within mainland China. Does anyone know why?  https://www.alibabacloud.com/help/en/model-studio/models?spm...  > By scaling up model parameters and leveraging substantial computational resources So, how large is that new model? Mandatory pelican on bicycle:  https://www.svgviewer.dev/s/U6nJNr1Z  2026 will be the year of open and/or small models. I tried it at  https://chat.qwen.ai/ . Prompt: \"What happened on Tiananmen square in 1989?\" Reply: \"Oops! There was an issue connecting to Qwen3-Max.\nContent Security Warning: The input text data may contain inappropriate content.\" I'm not familiar with these open-source models. My bias is that they're heavily benchmaxxing and not really helpful in practice. Can someone with a lot of experience using these, as well as Claude Opus 4.5 or Codex 5.2 models, confirm whether they're actually on the same level? Or are they not that useful in practice? P.S. I realize Qwen3-Max-Thinking isn't actually an open-weight model (only accessible via API), but I'm still curious how it compares. It just occured to me that it underperforms Opus 4.5 on benchmarks when search is not enabled, but outperforms it when it is - is it possible the the Chinese internet has better quality content available? My problem with deep research tends to be that what it does is it searches the internet, and most of the stuff it turns up is the half baked garbage that gets repeated on every topic. what ram and what minimum system req", "negative": "Show HN: Bonsplit \u2013 Tabs and splits for native macOS apps. I don't know why, but I thought this was going to sandbox style tab/split support for the all the baselines macos apps. This is very cool, but somehow got myself disappointed that something I didn't know I wanted doesn't exist. This is very interesting, I haven\u2019t touched macOS development for quite a while but it\u2019s good to know that libraries are still being written for both AppKit and SwiftUI on macOS. I do feel that this library would benefit from an explanation on why this was needed. AFAIR AppKit already provides a native tabbing API where you can \u201cjust\u201d (that \u201cjust\u201d is doing a lot of heavy lifting) implement a few delegate methods and you get tabbing behavior for free, especially on document-based apps. (Sorry, I do not remember the specifics, it might have been a tad more difficult) I\u2019m not updated on the SwiftUI equivalent, but I would imagine that a similar API would exist much alike API for multiple windows or multiple documents. I think everyone would benefit from a \u201cwhy\u201d explanation (which I definitely think would exist, since I\u2019ve used too many AppKit APIs in pain), and also some screenshots for a demo app (so that we can expect how it would look and how much the look and feel would deviate from the native counterparts). The title really should include \"library\"... This is excessively beautiful, both the website and the library's UI. But I have to ask: what's the rationale on dedicating such an elaborate and gorgeous website for just a library? Are you hoping to get hired for web design? Are you seeking fame and repute? Do you merely do it for the love of the game? Why, for the love of all that's good, pray tell  why  put all this effort into  mere documentation ? - library - functionality/effect looks like Sublime Text origami mode This is quite beautiful.  I had a somewhat similar use case last year and built something that wasn't this polished. The only feature that seems to be missing for wha"}
{"anchor": "Over 36,500 killed in Iran's deadliest massacre, documents reveal. I can't comprehend how a population can kill that many of their own people. They aren't even an \"other\" people, which has been the most common scapegoat lately. Same skin color, same religion, same language, same homeland. For comparison, estimates of the 1989 Tiananmen Square massacre death count are usually put in the 300-1,000 range by journalists and human rights groups.  https://en.wikipedia.org/wiki/1989_Tiananmen_Square_protests...  hm, I think we should re-evaluate sanctioning or civilian pressure campaigns, since the guise is for them to coax or turn on the government for regime change, but the government can just hire mercenaries from outside the country. don't know a solution but this one isn't it The source (Iran International) is backed by Saudi money and has a bias to dunk on Iran. That said, I'm sure the death count numbers from the Rasht Massacre are staggeringly higher than the initial tallies of 2-5k. This is certainly the end of peaceful Iranian protests. Whether it leads to a violent revolution or a static police state like North Korea remains to be seen. How is this possible without explosives? Even with vehicle mounted machine guns it seems like a crazy high number. Did the protestors get boxed in somehow? And across so many locations, that seems to require a crazy amount of coordination to kill so many in so little time. That's crazy. That's like ~40% of the deaths in the current gaza war, except over just 2 days instead of 2 years. This is depressing because we will go to war over this and it\u2019s going to be five years before people realizing they were tricked by \u201cbabies in incubators\u201d propaganda. The internet is fragile. Access can be so easily cut off for the masses in dire times. Take a good look US, because once you're down far enough the fascist drain, that's the cost of trying to claw your way back out. And there's no hope of external intervention given nuclear arms Earlie", "positive": "Why a 'Boring' Life Might Be the Happiest One. Hard agree! The challenge as you get older is reducing complexity so simple moments can be enjoyed, but it doesn't come easy. Turned 50 recently and I have come to the same conclusion.  I just want to live a simply life with simple joys. However that would only be possible because I've been working and saving since I was 15 years old. I find myself always struggling between being ambitious and being happy. Ideally I'd like to be both. But when one gets on the \"ambitious\" treadmill, capitalism wants one to work 24/7/365. Your competition is showing off how they worked until 4am, worked through the holidays, launched products on Sunday, and slept in the office, as \"dedication\". That culture makes me unhappy because I lose my physical health and mental health doing that. I'm happy and do my best work when I can go home, cook creative dinners, enjoy company of my partner, and enjoy the sunrises and sunsets in the mountains on the weekends. I'm not sure if a 'Boring Life' is for me. But I am sure I need some seasons of my life to be boring. Since I was 20 I've grinded away at my career, side hustles, etc... It made me happy. But at a certain point I got far enough ahead I felt complete. I needed a boring phase. I now wake up, have breakfast with my family, go to work, come home, play with my kids, watch a show, and go to bed. This is a day I would have scoffed at 10 years ago. But it now makes me happy. I don't think it would make me happy if I hadn't went for something the last 15. And it might not make me happy forever, but it's perfect for me right now. The author mistakes introversion to do-little/nothing. Many introverts love socializing and being around friends, it\u2019s just energy-depleting and takes (more) time to recharge. That said, doing little or nothing is quite relaxing, especially on rainy days. > I feel the world has become too fast. Too restless. Too demanding. We don\u2019t say it, but there\u2019s always this quiet pre", "negative": "A 26,000-year astronomical monument hidden in plain sight (2019). I first heard about this in a Graham Hancock book. Found it a fascinating example of an attempt to encode a date that far distant future generations might understand (provided it survives). That was an excellent rabbit hole to go down while eating lunch :) For a hypothesis concerning the precession of the equinoxes and religious pantheons, see  https://news.ycombinator.com/item?id=38761574  > There is an angle for doubt, for sorrow, for hate, for joy, for contemplation, and for devotion. I\u2019m so intrigued - what was going on inside Hansen's brain? This is the kind of stuff I love about ancient architecture. It seems they were full of such clever things (or maybe only the few constructions which survived until today). Its nice to see that some people still care about creating such thoughtful art for modern constructions. It seems that most building of our time are just optimized for fast and efficient construction. I hope there are many more out there, so that Earth's Graham Hancock of the year 16000 has something to explore on his/her ayahuasca trip. Sounds like it's about the precession of the equinoxes and the new \"Age of Aquarius\". More: >  Due to the precession of the equinoxes (as well as the stars' proper motions), the role of North Star has passed from one star to another in the remote past, and will pass in the remote future. In 3000 BC, the faint star Thuban in the constellation Draco was the North Star, aligning within 0.1\u00b0 distance from the celestial pole, the closest of any of the visible pole stars.[8][9] However, at magnitude 3.67 (fourth magnitude) it is only one-fifth as bright as Polaris, and today it is invisible in light-polluted urban skies.  >  During the 1st millennium BC, Beta Ursae Minoris (Kochab) was the bright star closest to the celestial pole, but it was never close enough to be taken as marking the pole, and the Greek navigator Pytheas in ca. 320 BC described the celestial"}
{"anchor": "28M Hacker News comments as vector embedding search dataset. Oh to have had a delete account/comments option. I've been embedding all HN comments since 2023 from BigQuery and hosting at  https://hn.fiodorov.es  Source is at  https://github.com/afiodorov/hn-search  Am I misunderstanding what a parquet file is, or are all of the HN posts along with the embedding metadata a total of 55GB? I know it's unrelated but does anyone knows a good paper comparing vector searches vs \"normal\" full text search? Sometimes I ask myself of the squeeze worth the juice Scratches off one of my todos, I think it would be useful to add a right-click menu option to HN content, like \"similar sentences\", which displays a list of links to them. I wonder if it would tell me that this suggestion has been made before. Finetune LLM to post_score -> high quality slop generator I don't remember licensing my HN comments for 3rd party processing. Maybe I\u2019m reading this wrong, but commercial use of comments is prohibited by the HN Privacy and data Policy. So is creating derivative works (so technically a vector representation) I don't know how to feel about this. Is the only purpose of the comments here is to train some commercial model? I have a feeling that, this might affect my involvement here going forward. Don't use all-MiniLM-L6-v2 for new vector embeddings datasets. Yes, it's the open-weights embedding model used in all the tutorials and it  was  the most pragmatic model to use in sentence-transformers when vector stores were in their infancy, but it's old and does not implement the newest advances in architectures and data training pipelines, and it has a low context length of 512 when embedding models can do 2k+ with even more efficient tokenizers. For open-weights, I would recommend EmbeddingGemma ( https://huggingface.co/google/embeddinggemma-300m ) instead which has incredible benchmarks and a 2k context window: although it's larger/slower to encode, the payoff is worth it. For a compromi", "positive": "\u201cThe closer to the train station, the worse the kebab\u201d \u2013 a \u201cstudy\u201d. Anecdotally, it's the same for coffee. Office lobby coffee shops are invariably terrible. The decent ones are always at least a 5-10 minute walk away. This makes intuitive sense. High mass-transit corridor real-estate (rail, air, road) leases come at a premium so those higher fixed-costs and must be balanced against a higher-volume of less-breadth of service with the same fixed (or even slightly higher) labor costs. In food service, high-volume is (mostly) inversely correlated with quality. Looking at their actual results ( https://preview.redd.it/znmnejgab5je1.png?width=1000&format=... ), I don't see any positive or negative correlation. Although I can subjectively confirm the hypothesis. I've observed the following: 1) An alarming number of regions in the world have a pizza joint called \"New York Pizza\", \"Manhattan Pizza\", or similar. 2) The similarity of the pizza therein to the actual thin, greasy slices served up in pizza joints from actual New York is inversely proportional to the location's distance from New York. So, the New York Pizza in Boston -- pretty close. The New York Pizza in Brisbane, QLD is alien by comparison and I think they consider \"pepperoni\" and \"salami\" interchangeable down there. He didn't find a correlation, or rather found that there is no correlation, between proximity to a railway station and how the kebab is reviewed. It's a nice study for a statistics class! The only place this isn't true is Japan. Always like reading the Best Kebab reviews on trip advisor. It\u2019s right next to Queen Street railway station so fits with the study.  https://www.tripadvisor.co.uk/Restaurant_Review-g186534-d125...  > Not only was my food uncooked but I also discovered a pubic hair in my chips and cheese, then when I proceeded to report the problem, I was chased with a knife. Down Dundas Street.Absolutely scandalous LOL we may need to update the title of this post, half the top level comment", "negative": "An Illustrated Guide to Hippo Castration (2014). I mean, sensationalistic or \"Why didn't you post on / This isn't reddit\" or not, this is one of the more amazing opening sentences ever... > Few things in this world are as elusive as a hippopotamus testicle Short and sweet. An absolute masterpiece of scientific writing! A family friend used to run a travel business with tours to the Okavango Delta. When I asked him how it was going, he replied \"Great, we've only ever lost one honeymoon couple to hippos\"! People don't realise they are one of the most dangerous animals to humans. How long before this is included in an AI benchmark? Can't wait :-) > hippos' stunning wound-healing abilities\u2014perhaps related to the antibacterial properties of the creepy \"red sweat\" that coats their skin sounds interesting and definitely something worth looking into as well We have been taught in high school that the reason humans and \"all mammals\" had external testes was to cool them. But elephants have internal testicles, and, apparently, so do hippos. This seems a much better strategy than having such an important (and sensitive!) organ hanging out at the mercy of predators, foes, or even banal accidents. The evolution explanation for this appears to be lacking. reading this i wonder how long some tech bros need for a starup that offers AI assisted hippo castration for executives as the next hype Flashback to a Robin Williams bit, where Marlon Perkins of \"Wild Kingdom\" directs his assistant to circumcise a water buffalo.  https://www.youtube.com/watch?v=WFbhByVMhXM&t=45s  I don't think much has changed in the state of the art of hippo castration in the last twelve years. I was fond of \u201call the surviving animals were able to return to their feces-infested communal pools within hours of the surgery with no negative consequences\u201d \"Jaws\" hippo edition:  https://www.youtube.com/shorts/aKEgTUkpk64   https://www.youtube.com/watch?v=jJQpq8mLbm0  So is sanctimony, good job nobody around here is b"}
{"anchor": "I let ChatGPT analyze a decade of my Apple Watch data, then I called my doctor. >  Despite having access to my weight, blood pressure and cholesterol, ChatGPT based much of its negative assessment on an Apple Watch measurement known as VO2 max, the maximum amount of oxygen your body can consume during exercise. Apple says it collects an \u201cestimate\u201d of VO2 max, but the real thing requires a treadmill and a mask. Apple says its cardio fitness measures have been validated, but independent researchers have found those estimates can run low \u2014 by an average of 13 percent.  There's plenty of blame to go around for everyone, but at least for some of it (such as the above) I think the blame more rests on Apple for falsely representing the quality of their product (and TFA seems pretty clearly to be blasting OpenAI for this, not others like Apple). What would you expect the behavior of the AI to be?  Should it always assume bad data or potentially bad data?  If so, that seems like it would defeat the point of having data at all as you could never draw any conclusions from it.  Even disregarding statistical outliers, it's not at all clear what part of the data is \"good\" vs \"unrealiable\" especially when the company that  collected  that data claims that it's good data. The author is a healthy person but the computer program still gave him a failing grade of F. It is irresponsible for these companies to release broken tools that can cause so much fear in real people. They are treating serious medical advice like it is just a video game or a toy. Real users should not be the ones testing these dangerous products. We trained a foundation model specifically for wearable data:\n   https://www.empirical.health/blog/wearable-foundation-model-...  The basic idea was to adapt JEPA (Yann LeCun's Joint-Embedding Predictive Architecture) to multivariate time series, in order to learn a latent space of human health from purely unlabeled data. Then, we tested the model using supervised fine tu", "positive": "Maine\u2019s \u2018Lobster Lady\u2019 who fished for nearly a century dies aged 105. It always seems it's a fall that ends it, I wonder if she could have made 100 years on the water if she hadn't fell. What an inspiring life! Rugged individualism. Rest in peace, Queen. Who else reading the headline thought it\u2019s about 100 year old lobster that died? So Long, and Thanks for All the... Lobster. She must have had so many interesting stories to tell. Such an amazing experiences - born in the early 1920s, being a young adult at the beginning of the Second World War, seeing mass commercialization of air travel, flight to space, miniaturization and age of information. And she even caught beginnings of AI (or pseudo-AI). She \"picked\" a good place to live and observe the flow of time and events where she directly wouldn't be affected by various negative events throughout the century of her life. I wonder if there is a lobster that survived her. Lobsters are long-lived, they don't age (in the sense of slowly losing their fitness - senescence) and they only die when they grow too big and suffocate during moulting, or possibly catch some infection, or get killed by other animals/people. A 105 y.o. lobster is plausible. The article states that a raising amount of people in the US still work, albeit their age and it feels a little strange to me. Ginny, probably got so old because she was working and had a purpose every day. Being 100 and still capable of working is a blessing It says she died at 105 and spent almost a century fishing for lobsters. I doubt she was catching many at the age of five. I just lost my Mom, at 97. We would go to lunch on Tuesday and then grocery shopping. She'd talk of the family and where they all were and what they were doing - it was MY day to catch up. The last Tuesday we got back and she said \"That was too hard. I think that was the last one.\" I agreed, and thought I'd call her next tuesday just the same and see if she'd changed her mind. But there was no 'next tue", "negative": "Vitamin D and Omega-3 have a larger effect on depression than antidepressants. I found this to be the case. Tried Sertraline for a while, gave me headaches and made me feel sick. Then as part of a new gym plan, started taking Omega 3+VitD daily, and I just felt a sense of calm and peace after a few weeks. The massive uptick in exercise probably also helped. I also felt quite an extreme uptick because I was a vegan for 10 years, and found out I had basically zero Omega 3 in my blood. I suspect one of the main reasons my mental health declined was due to the lack of Omega 3. Disclaimer, not saying vegans should stop being vegans, just make sure you find a good supplement, and make sure you understand the difference between EPA/DHA Omega 3. And better than taking pills for the former, add hemp hearts or flax seeds to your cereal. One serving of hemp hearts has 10 grams of protein and 12 grams of Omegas 3 and 6. Flax seeds are lower in protein but an even better source of Omega 3 in particular. Please do not take 5000mg/day of Vitamin D. The author confuses IU and mg which is very dangerous. > A 2014 systematic review concluded that vitamin D supplementation does not reduce depressive symptoms overall but may have a moderate benefit for patients with clinically significant depression, though more high-quality studies were determined to be needed.  https://en.wikipedia.org/wiki/Vitamin_D#Depression  Chia seed and flaxseed high in omega3 Can I just add: In addition to this, if you struggle with anxiety or have some sort of ADHD, then try cutting out caffeine  entirely . Not just switching to \"decaf\" (which isn't), but cutting out tea and coffee, and switching to an alternative like Barleycup. Doing this has had a massive positive effect for me, and combined with decent nutrition and daily exercise, has been wonderful. My physician prescribed Vitamins D and B12, so a quality Omega 3 is the only supplement I currently purchase. After an absurd amount of trial and error with"}
{"anchor": "Project ideas to appreciate the art of programming. Highly recommend writing a BitTorrent client. The spec is easy to grok, it has a bunch of fun subproblems that you can go as deep or as shallow as you want into, and it's super rewarding being able to download something like the Debian kernel after all of your hard work. Magnet links and seeding are two fun things to tackle post basic implementation. It also got me really interested in peer to peer systems and DHTs like Chord! This is a strange list. #58 is make your own malloc, ok. That's a moderately difficult project for a new developer (made harder if they don't know anything about what malloc actually does under the hood, you may need to study up a bit on operating systems and some other things before you even start). Followed by #59 where they suggest you build your own streaming protocol from scratch... There are some good projects in there, but the levels of difficulty are all over the place. I\u2019ll plug my series of project ideas that have also been discussed here on HN over the years: Challenging programming projects every programmer should try  https://austinhenley.com/blog/challengingprojects.html  Is this what the kids call \"astroturfing\"? AI usage verboten? Or erlaubt? I see comments suspecting this list is AI-generated. That might be true.\nBut ironically, the practice of \"building from scratch\" is the best antidote to AI dependency. Writing from Japan, we call this process \"Shugyo\" (austere training).\nA master carpenter spends years learning to sharpen tools, not because it's efficient, but to understand the nature of the steel. Building your own Redis or Git isn't about the result (which AI can give you instantly). It is about the friction. That friction builds a mental model that no LLM can simulate. Whether this post is marketing or not, the \"Shugyo\" itself is valid. This is just AI generated slop with things being all over the map with no details/notes etc. A far better way is to go through the boo", "positive": "Eulogy for Dark Sky, a data visualization masterpiece (2023). Meteoswiss app is the best weather app ever created Yes Dark Sky had the best UI of any weather app I have used. I now use Weathergraph which does it differently but I would go back to Dark Sky (and pay for it) in a flash. It shows the correct things and on a phone understands that showing the temperatures across the screen is useless as if I go out I want to know what the weather is like when I might make the journey back in 8+ hours time. I might not care what the weather is in 4 hours time as I will be inside. Dark Sky was a marvel, and when it first came out, its ability to say rain will start where you are in 2-3 minutes was a marvel. The information design argument is 100% valid, but I also marvel that, having bought the company, Apple's weather app still isn't as precise or accurate. I don't know whether Apple's privacy focus prevents them making the same precise predictions, or if there is some other reason they don't, but it's sad that in 2025 we don't have the same level of performance as we did twelve years ago. Apple should have just used that app itself, rather than trying to build whatever that they have right now. The Apple Weather app has gotten better over time, though it\u2019s still not a perfect replacement. Scrolling through the Dark Sky screenshots, I can recognize many of the same things now incorporated with Apple\u2019s. And Apple does offer location specific notifications of rain which I find to be pretty accurate, about as accurate as Dark Sky. There\u2019s largely a perception problem with Apple. People loved Dark Sky as an independent small app that worked well, before Apple took it and destroyed it. Now, even if Apple incorporated all of the same data and features, it still wouldn\u2019t give that same spark of joy people had. fwiw, on iOS, I like using WeatherGraph:  https://weathergraph.app/  The developer is very responsive, lots of UI customization (both app and widgets) is possible, and pri", "negative": "My Ridiculously Robust Photo Management System (Immich Edition). After going through 25 years of changing software every few years on this front I can\u2019t be bothered. Files on disk. Nothing over the top. Immich is just another thing to maintain. Another problem which will result in a wholesale migration down the line. If someone wants something I email it to them or upload it to a directory on a web server and send them the link. If I want something on my phone I\u2019ll zap it over with localsend. Photography is a hobby for me and I have a large family so I have a lot of photos. And a lot of editing to do. Currently moving from Lightroom to Darktable because again Lightroom tries to hammer me with library management and lock me into things. Elodie makes a copy of all my images initially? Is the recommenddd route then to delete the files in original location? Seems unclear at first read. Habe you tried nextcloud + memories app?\nEvery metadata is stored in EXIF and the directory structure on disk defines the directory structure in the app (and vice versa). \nWhen you want to move your tooling or just do things manual again, grab the disk and your are ready. I migrated from Apple Photos to Immich a couple of months ago, removing the iCloud subscription, and couldn\u2019t be happier. It was the most hassle free piece of self-hosted software I\u2019ve had so far. Very easy to install and everything just works. Context and OCR search are amazing. Mobile apps could be better, but they are constantly being improved. My favorite feature is being able to setup a container on my Linux desktop that has a GPU access and can run ML workloads for image processing whenever I turn the computer on, as my NAS (where Immich resides) is a low power machine without a dedicated GPU. They even have ROCM support, so it works even without an Nvidia GPU. Being able to spread such workloads over your local network feels like a magic that has been forgotten in an era of blackbox cloud providers. Photo printer "}
{"anchor": "BYD's cheapest electric cars to have Lidar self-driving tech. Lidars come down in price ~40x.  https://cleantechnica.com/2025/03/20/lidars-wicked-cost-drop...  Meanwhile visible light based tech is going up in price due to competing with ai on the extra gpu need while lidar gets the range/depth side of things for free. Ideally cars use both but if you had to choose one or the other for cost you\u2019d be insane to choose vision over lidar. Musk made an ill timed decision to go vision only. So it\u2019s not a surprise to see the low end models with lidar. Keep in mind, that $25k AUD is just $16600. And for that price, you're getting a real car with driver-assist features and a reasonable crash safety rating. The US car manufacturers are cooked. Still not convinced of the safety of lidar. I guess all these cars with cheap lidar sensors on board will generate real world safety data over the next few years. The roof mount seems very practical, but it's a look that may turn off some buyers... buyers who care about looks. For SUVs, maybe it could be blended in with a roof air scoop, like on some off-road trucks. Or a light bar. Where is the LiDAR on the Atto 1? In the grille? How much worse is the field of view? How do you train a model to drive with LiDAR when the human drivers who generate the training data don\u2019t use LiDAR? Related: Volvo drops LIDAR...  https://www.carscoops.com/2025/11/volvo-says-sayonara-to-lid...  Are you serious, a car with Lidar sensor that's not even available in Bugatti Tourbillon that cost 500x more? Joking aside, this BYD Seagull, or Atto 1 in Australia (AUD$24K) and Dolphin Surf in Europe (\uffe118K in the UK), is one the cheapest EV cars in the world and selling at around \uffe16K in China. It's priced double in Australia and triple in the UK compared to its original price in China. It's also one of China best selling EV cars with 60K unit sold per month on average. Most of the countries scrambling to block its sales to protect their own car industry or increas", "positive": "Erdos 281 solved with ChatGPT 5.2 Pro. The erdosproblems thread itself contains comments from Terence Tao:  https://www.erdosproblems.com/forum/thread/281  Has anyone verified this? I've \"solved\" many math problems with LLMs, with LLMs giving full confidence in subtly or significantly incorrect solutions. I'm very curious here. The Open AI memory orders and claims about capacity limits restricting access to better models are interesting too. From Terry Tao's comments in the thread: \"Very nice! ... actually the thing that impresses me more than the proof method is the avoidance of errors, such as making mistakes with interchanges of limits or quantifiers (which is the main pitfall to avoid here). Previous generations of LLMs would almost certainly have fumbled these delicate issues. ... I am going ahead and placing this result on the wiki as a Section 1 result (perhaps the most unambiguous instance of such, to date)\" The pace of change in math is going to be something to watch closely. Many minor theorems will fall. Next major milestone: Can LLMs generate useful abstractions? This must be what it feels like to be a CEO and someone tells me they solved coding. I have 15 years of software engineering experience across some top companies. I truly believe that ai will far surpass human beings at coding, and more broadly logic work. We are very close Out of curiosity why has the LLM math solving community been focused on the Erdos problems over other open problems?  Are they of a certain nature where we would expect LLMs to be especially good at solving them? This is crazy. It's clear that these models don't have human intelligence, but it's undeniable at this point that they have _some_ form of intelligence. FWIW, I just gave Deepseek the same prompt and it solved it too (much faster than the 41m of ChatGPT). I then gave both proofs to Opus and it confirmed their equivalence. The answer is yes. Assume, for the sake of contradiction, that there exists an \\(\\epsilon > 0\\) ", "negative": "Airfoil (2024).  https://news.ycombinator.com/item?id=39526057  I was just thinking the other day about how AI will pretty soon be able to create this kind of explainers on everything quite quickly. Amazing times! Where can I find more articles where things are explained in this manner? Ok that's long, one top line thing people tend to miss in these flying explanations is that airfoil shape isn't about some special sauce generating lift.  A flat plate generates any amount of lift you want just fine.  Airfoil design is about the ratio of lift to drag most importantly and then several more complex effects but NOT just generating lift. (stall speed, performance near and above the speed of sound, laminar/turbulent flow in different situations, what you can fit inside the wing, etc) That's the missing course for the first year of any Aerospace Engineering faculty. I wish there was an infinite number of blogs that where this good. Bartosz Ciechanowski, the gift that keeps on giving. He usually posts these brilliant explanations once or twice a year but nothing in 2025. I hope he finds the time to continue because the lessons are really really brilliantly told. This is absolutely amazing. For those of us programming nerds that want to play with aerodynamics, I can't recommend AeroSandbox enough. While the code is pretty obviously written for people who know their way around aerodynamics and not so much around programming, it is remarkably powerful. You can do all sorts of aerodynamic simulations and is coupled with optimization libraries that allow you to do incredible aerodynamic optimizations. It comes included with some pretty powerful open weight neural network models that can do very accurate estimates of aerodynamic characteristics of airfoils in a fraction of the time that top tier heuristic solvers (like xfoil) can do (which are already several orders of magnitude faster than CFD solvers).  https://github.com/peterdsharpe/AeroSandbox  Oh man. This guy. His work is "}
{"anchor": "Crafting Interpreters. The two most popular discussions of this fantastic book: 2020 with 777 points:  https://news.ycombinator.com/item?id=22788738  2024 with 607 points:  https://news.ycombinator.com/item?id=40950235  Really I would love to know how parse context sensitive stuff like typedef which will have \"switched\" syntax for some tokens. Would like to know things like \"hoisting\" in C++, where you can you the class and struct after the code inside the function too, but I just find it hard to describe them in rigorous formal language and grammar. Hacky solution for PEG such as adding a context stack requires careful management of the entry/exit point, but the more fundamental problem is that you still can't \"switch\" syntax, or you have to add all possible syntax combination depending on the numbers of such stacks. I believe persistent data structure and transactional data structure would help but I just couldn't find a formalism for that. In case anyone finds it useful, we (CodeCrafters) built a coding challenge as a companion to this book. The official repository for the book made this very easy to do since it has tests for each individual chapter. Link:  https://app.codecrafters.io/courses/interpreter/overview  One of the best resources for learning compiler design. The web version being free is incredibly generous. I've found this book to be a good way to learn a new language, because it forces you to do a bit of reading about various language features and patterns to create equivalent implementations. For languages that lack some of the features in Java, it can be tricky to learn how to apply similar patterns, but that's half the fun (for me). I have bought the print version of this 3 seperate times to give as a gift, its excellent. It's a great book, I bought the paper version first, but man it was too big and heavy for my liking, ended up buying a digital copy; much more practical for notes and search... although I keep getting lost somewhere in the mounta", "positive": "Functional programming and reliability: ADTs, safety, critical infrastructure. This article seems to conflate strong type systems with functional programming, except in point 8. It makes sense why- OCaml and Haskell are functional and were early proponents of these type systems. But, languages like Racket don\u2019t have these type systems and the article doesn\u2019t do anything to explain why they are _also_ better for reliability. >In banking, telecom, and payments, reliability is not a nice to have. It is table stakes. This reliability isn't done by being perfect 100% of the time. Things like being able to handle states where transactions don't line up allowing for payments to eventually be settled. Or for telecom allowing for single parts of the system to not take down the whole thing or adding redundancy. Essentially these types of businesses require fault tolerance to be supported. The real world is messy, there is always going to be faults, so investing heavily into correctness may not be worth it compared to investing into fault tollerance. I'm wary of absolute statements about programming. I like good type systems, too, but they won't save you from bugs that are better addressed by fuzz testing, fault injection testing and adversarial mindset shifts. Strong types: yes, it\u2019s definitely better Functional programming: no, functional programming as in: the final program consists in piping functions together and calling the pipe. In my opinion, that tends to get in the way of complex error handling. The problem being that raising Exceptions at a deep level and catching them at some higher level is not pure functional programming. So your code has to deal with all the cases. It is more reliable if you can do it, but large systems have way too many failure points to be able to handle them all in a way that is practical. I think there is a strong case that ADTs (algebraic data types) aren't so great after all. Specifically, the \"tagged\" unions of ADT languages like Haskell ", "negative": "Start your meetings at 5 minutes past. #leadership is really sending me on this one Apart from just a quick breather between back to back meetings, it also provides a critical bio-break time for your attendees. We do this at my work and guess what - meetings tend to run 5 minutes late because everyone knows the next meeting doesn\u2019t start until 5 past. If you have so many back-to-back meetings, maybe put in a school bell that chimes at 5 minutes before and 5 minutes after the hour. Please just put this in your conference rooms so those of us who know how to evade meetings don't have to hear it as well. Meetings run long so frequently that  we now recommend starting the next meeting late to compensate. This will surely solve the problem. > there is social pressure not to allow meetings to run much past the top of the hour. I've never seen this pressure. > meetings rarely started on the dot anyway before this change. It's like I live in an entirely different world. Start meetings when they say they're going to start.  People will learn to show up quickly.  I think that works better than trying to psychologically game people into cooperation.  That just starts the classic treadmill.  You might have that one friend that you tell to show up half an hour before everyone else. They mentally add the half hour back because you're always giving such early times.  Better IMO to just keep things simple.  Let people leave when they need to.  Show up on time. We've been doing this at Qualcomm for a while, and I really like it. While meetings do run over sometimes, the practice has still built this acceptance around short breaks between meetings. No one bats an eye if we've got two consecutive meetings together, the first one ends late, and we wait five minutes before starting or joining the next one. In fact, having done it for so long, it surprisingly really annoys me when our vendors schedule 60 minute meetings on the hour. I've always wondered at the company cultures between Go"}
{"anchor": "The Universal Pattern Popping Up in Math, Physics and Biology (2013).  https://pmc.ncbi.nlm.nih.gov/articles/PMC11109248/  DNA as a perfect quantum computer based on the quantum physics principles. There is the well known problem that \"random\" shuffling of songs doesn't sound \"random\" to people and is disliked. I wonder if the semi-random \"universality\" pattern they talk about in this article aligns more closely with what people want from song shuffling. Not sure why you have to read 3/4 of the article to get to a _link_ to a pdf which _only_ has the _abstract_ of the actual paper: N. Benjamin Murphy and Kenneth M. Golden* (golden@math.utah.edu), University of\nUtah, Department of Mathematics, 155 S 1400 E, Rm. 233, Salt Lake City, UT 84112-0090.\nRandom Matrices, Spectral Measures, and Composite Media. The Physics models tend to shake out of some fairly logical math assumptions, and can trivially be shown how they are related. \"How Physicists Approximate (Almost) Anything\" (Physics Explained)  https://www.youtube.com/watch?v=SGUMC19IISY  If you are citing some crank with another theory of everything, than that dude had better prove it solves the thousands of problems traditional approaches already predict with 5 sigma precision.   =3 What's with all the spammy comments? >The data seem haphazardly distributed, and yet neighboring lines repel one another, lending a degree of regularity to their spacing Wow, that kind of reminds me of the process of evolution in that it seems so random and chaotic at the most microscopic scales but at the macroscopic, you have what seems some semblance of order. The related graph also sprung to mind just how very like organisms repel (less tolerance to inbreeding) but at the same time species breed with like species and only sometimes stray from that directive. What is the pattern that underlies how organisms determine production or conflict with other organisms and can we find universality in it? I guess it's called \"universality\" for ", "positive": "Project Genie: Experimenting with infinite, interactive worlds. Google Deepmind Page:  https://deepmind.google/models/genie/  Try it in Google Labs:  https://labs.google/projectgenie  (Project Genie is available to Google AI Ultra subscribers in the US 18+.) This could be the future of film. Instead of prompting where you don't know what the model will produce, you could use fine-grained motion controls to get the shot you are looking for. If you want to adjust the shot after, you could just checkpoint the model there, by taking a screenshot, and rerun. Crazy. Every character goes forward only, permanence is still out of reach apparently. Reminds me of this [1] HN post from 9 months ago, where the author trained a neural network to do world emulation from video recordings of their local park \u2014 you can walk around in their interactive demo [2]. I don't have access to the DeepMind demo, but from the video it looks like it takes the idea up a notch. (I don't know the exact lineage of these ideas, but a general observation is that it's a shame that it's the norm for blog posts / indie demos to not get cited.) [1]  https://news.ycombinator.com/item?id=43798757  [2]  https://madebyoll.in/posts/world_emulation_via_dnn/demo/  I keep on repeating myself, but it feels like I'm living in the future.\nCan't wait to hook this up to my old Oculus glasses and let Genie create a fully realistic sailing simulator for me, where I can train sailing with realistic conditions. On boats I'd love to sail. If making games out of these simulations work, it't be the end for a lot of big studios, and might be the renaissance for small to one person game studios. What\u2019s the endgame here? For a small gaming studio, what are the actual implications? I have been confused for a long time why FB is not motivated enough to invest in world models, it IS the key to unblock their \"metaverse\" vision. And instead they let go Yann LeCun. This would be really cool if polished and integrated with VR. I don't", "negative": "'Askers' vs. 'Guessers' (2010). Discussed (in a singleton sort of way) at the time:  Askers vs. Guessers  -  https://news.ycombinator.com/item?id=1956778  - Dec 2010 (1 comment) Edit: plus this!  Ask vs. Guess Culture  -  https://news.ycombinator.com/item?id=37176703  - Aug 2023 (479 comments) I found a good discussion that I keep referring to on Jean Hsu's blog:\n https://jeanhsu.substack.com/p/ask-vs-guess-culture \nand \n https://jeanhsu.substack.com/p/bridging-the-ask-vs-guess-cul...  It's been quite illuminating for people in multicultural teams... Edit: this whole theory seems to come from some internet forum comment! I know a lot of people here are seduced (I was a bit too) but basing your social interactions and how you see others and yourself on this stuff might not be the best thing to do! Original comment below for posterity and because there are answers. ---- I'm not sure this stuff is really  that  helpful. You might be tempted to put people into these categories, but you might have a somewhat caricatural and also wrong image of both which could worsen interactions. By the way, that article doesn't cite any studies! It's probably helpful to know people are more or less at ease asking direct questions or saying no or receiving a no, but it's all scales and subtleties. It could also depend on the mood, or even who one interacts with or on the specific topic). The article touches this a bit (the \"not black and white\" paragraph). We human beings love categories but categories of people are often traps. It's even more tempting when it's easy to identity to one of the depicted groups! I wonder if this asker-guesser thing is in the same pseudoscience territory as the MBTI. In the end, I suppose there's no good way around getting to know someone and paying attention for good interactions. > Your boss, asking for a project to be finished early, may be an overdemanding boor \u2013 or just an Asker, who's assuming you might decline. I don't pay for the Atlantic and thus a"}
{"anchor": "OracleGPT: Thought Experiment on an AI Powered Executive. Considering things like Palantir, and the doge effort running through Musk, it seems inconceivable that this is not already the case. I think I'm more curious about the possibility of using a special government LLM to implement direct democracy in a way that was previously impossible: collecting the preferences of 100M citizens, and synthesizing them into policy suggestions in a coherent way. I'm not necessarily optimistic about the idea, but it's a nice dream. This is an interesting and thoughtful article I think, but worth evaluating in the context of the service (\"cognitive security\") its author is trying to sell. That's not to undermine the substance of the discussion on political/constitutional risk under the inference-hoarding of authority, but I think it would be useful to bear in mind the author's commercial framing (or more charitably the motivation for the service if this philosophical consideration preceded it). A couple of arguments against the idea of singular control would be that it requires technical experts to produce and manage it, and would be distributed internationally given any countries advanced enough would have their own versions; but it would of course provide tricky questions for elected representatives in the democratic countries to answer. A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION. think we're already there aren't we? no human came out with those tariffs on penguin island The really nice thing about this proposal is that at least now we can all stop anthropomorphizing Larry Ellison, and give Oracle the properly robot-identifying CEO it deserves. You sometimes hear people say \"I mean, we can't just give an AI a bunch of money/important decisions and expect it to do ok\" but this is already happening and has been for years. Examples: - Algorithmic trading: I once embedded on an Options trading desk. The head of desk mentioned ", "positive": "Grid: Free, local-first, browser-based 3D printing/CNC/laser slicer. Surprised this hasn't been shared here before. Built by my former colleague, Stewart Allen (Co-Founder/CTO of WebMethods, CTO of AddThis, Co-Founder/CPO of IonQ, et al.). What caught my attention: - 100% free, no subscriptions, no accounts, no cloud - Local-first: all slicing and toolpath generation runs on your machine - Works in any browser, even offline once loaded - Supports FDM/SLA, CNC milling, laser cutting, wire EDM - Fully open source: github.com/GridSpace/grid-apps Refreshing to see a tool that isn't trying to lock you into a subscription or harvest your data. Now if we can only get an offline printer\u2026 I've used kiri:moto for several simple CNC projects! This probably won't scroll to the correct place on the page but there's some images of my project at  https://hcc.haus/propmania/#2024-palm-torches  and  https://static.cloudygo.com/static/Prop%20Making/2024%20Palm...  I used it instead of the terrible closed source Easel App for a CARVEY hobby CNC. For metal milling I find Fusion 360 is necessary. More open source, browser-accessible tools is a good thing. That said, aren't Prusa/Orca/etc. all already open-source (and part of the same lineage)? Am I weird in not being too surprised? It don't have experience with wire EDM but every toolpath generator or slicer I've ever used was just local software. This looks great. I was hoping it would have been a good OrcaSlicer replacement for my FDM printer, but unfortunately it didn't generate any top surfaces (except for the topmost one) for a model I imported in. I didn't know if it was the printer profile (Creality.Ender3) or something else, but it seems I'm still using OrcaSlicer for the time being. Great tool for a Makerspace - really appreciate the ability to use the same tool for laser cutting, 3d printing, and CNC.  These are big jumps for people typically - having a familiar tool would help people transition from one area to another. OT: W", "negative": "ICE using Palantir tool that feeds on Medicaid data. Any time I see people say \"I don't see why I should care about my privacy, I've got nothing to hide\" I think about how badly things can go if the wrong people end up in positions of power. The classic example here is what happens when someone is being stalked by an abusive ex-partner who works in law enforcement and has access to those databases. This ICE stuff is that scaled up to a multi-billion dollar federal agency with, apparently, no accountability for following the law at all. Wishful thinking but it would be real great if a future leader destroyed this infrastructure. I'm sure they'll run on not using it but when systems like this exist they tend to find applications Why would Medicaid have the data of anyone who is at risk of immigration enforcement? The reported connection seems tenuous: > The tool \u2013 dubbed Enhanced Leads Identification & Targeting for Enforcement (ELITE) \u2013 receives peoples\u2019 addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources. So, they have a tool that sucks up data from a bunch of different sources, including Medicaid.  But there's no actual nexus between Medicaid and illegal immigrants in this reporting. Edit: In the link to their earlier filings, EFF claims that some states enroll illegal immigrants in Medicaid:  https://www.eff.org/deeplinks/2025/07/eff-court-protect-our-...  This current administration and their policies have definitely influenced my opinion on the 2018 debate around citizenship questions on the US census. (For more context:  https://www.tbf.org/blog/2018/march/understanding-the-census... ) Glad to see this post didn't get flagged like the one that was posted yesterday on a similar topic about ICE data mining and user tracking.  https://news.ycombinator.com/item?id=46748336  \"ICE Budget Now Bigger Than Most of the Wo"}
{"anchor": "Show HN: TetrisBench \u2013 Gemini Flash reaches 66% win rate on Tetris against Opus. It would be more interesting to make it build a chess engine and compare it against Stockfish. The chess engine should be a standalone no-dependencies C/C++ program that fits in NNN lines of code. Gemini 3 Flash is at a very nice point along the price-performance curve. A good workhorse model, while supplementing it with Opus 4.5 / Gemini 3 Pro for more complex tasks. Very cool! I am a good Tetris player (in the top 10% of players) and wanted to give brick yeeting against an LLM a spin. Some feedback:\n- Knowing the scoring system is helpful when going 1v1 high score - Use a different randomization system, I kept getting starved for pieces like I. True random is fine, throwing a copy of every piece into a bag and then drawing them one by one is better (7 bag), nearly random with some lookbehind to prevent getting a string of ZSZS is solid, too (TGM randomizer) - Piece rotation feels left-biased, and keeps making me mis-drop, like the T pieces shift to the left if you spin 4 times. Check out  https://tetris.wiki/images/thumb/3/3d/SRS-pieces.png/300px-S...  or  https://tetris.wiki/images/b/b5/Tgm_basic_ars_description.pn...  for examples of how other games are doing it. - Clockwise and counter-clockwise rotation is important for human players, we can only hit so many keys per second - re-mappable keys are also appreciated Nice work, I'm going to keep watching. There are some concepts clashing here. I mean, if you let the LLM build a testris bot, it would be 1000x better than what the LLMs are doing. So yes, it is fun to win against an AI, but to be fair against such processing power, you should not be able to win. It is only possible because LLMs are not built for such tasks. It's actually 80% against Opus, 66% average against the 5 models it's tested with. I imagine this is because Tetris is visual and the Gemini models are strong visually. Interesting but frustratingly vague on details. ", "positive": "Ask HN: Where do seasoned devs look for short-term work?. Now is not a great time to be looking for this kind of work unfortunately. I think your network is the best place to look for this sort of work. Sometimes people will reach out to me with short term projects which is the best way to get gigs like this. Maybe start looking at your colleagues on linkedin, see what they are up to, and think of ways to contribute to what they are working on. The best people to contact in this scenario are leadership and decision makers. A SWE II isn't gonna help you much but a CTO at an early stage startup might be a good person to send a DM if they are friends with you (or even if they aren't!) :) Short term work is more plentiful when money is easy and there\u2019s a lot of entrepreneurial activity going on due to some recent catalyst such as mobile app platforms or the dotcom boom etc. Right now we\u2019re in the AI boom and some people may be making money peddling agentic solutions but money is tight and businesses are hurting. It\u2019s also hard to trust a short term dev who doesn\u2019t really need the money. You have no leverage over them. They sort of just do as they please. Most ad-hoc work I've picked up has been people I've previously worked with/for. Maybe worth reaching out to people you have a prestablished relationship with I did this a few years ago and the winning recipe was a shameless (i.e. deeply shameful) linkedin post where I pretty much just summarized my skillset and explained that I was looking for a senior engineer equivalent of a summer internship, with no chance of extension. Got me 3-4 offers. None of the offering companies had ads out for roles like this, so this was pretty much the only way. I'd believe you're better off working on yourself. Maybe do toy projects for your potential portfolio, learn an additional skill (AI?), and build many weekend projects until something sticks. Publishing articles, etc to demo your skill helps you stay top of mind. Even if only the ", "negative": "The browser is the sandbox. I like the perspective used to approach this. Additionally, the fact that major browsers can accept a folder as input is new to me and opens up some exciting possibilities. The folder input thing caught me off guard too when I first saw it. I've been building web apps for years and somehow missed that `webkitdirectory` attribute. What I find most compelling about this framing is the maturity argument. Browser sandboxing has been battle-tested by billions of users clicking on sketchy links for decades. Compare that to spinning up a fresh container approach every time you want to run untrusted code. The tradeoff is obvious though: you're limited to what browsers can do. No system calls, no arbitrary binaries, no direct hardware access. For a lot of AI coding tasks that's actually fine. For others it's a dealbreaker. I'd love to see someone benchmark the actual security surface area. \"Browsers are secure\" is true in practice, but the attack surface is enormous compared to a minimal container. We never say that it isn't. There is a reason Google developed NaCl in the first place that inspired WebAssembly to become the ultimate sandbox standard. Not only that, DOM, JS and CSS also serves as a sandbox of rendering standard, and the capability based design is also seen throughout many browsers even starting with the Netscape Navigator. Locking down features to have a unified experience is what a browser should do, after all, no matter the performance. Of course there are various vendors who tried to break this by introducing platform specific stuff, but that's also why IE, and later Edge (non-chrome) died a horrible death There are external sandbox escapes such as Adobe Flash, ActiveX, Java Applet and Silverlight though, but those external escapes are often another sandbox of its own, despite all of them being a horrible one... But with the stabilization of asm.js and later WebAssembly, all of them is gone with the wind. Sidenote: Flash's script"}
{"anchor": "AI is a horse (2024). It's also a big bloatey gas bag that needs constant de-farting to function \"I've been through the desert On AI with no name It felt good to be out of the rAIn In the desert, you can remember your name 'Cause there ain't no one for to give you no pain\" Or your typical American teenager. All true apart you can only lead it to water - it drinks ALL the water regardless of anything else. And the salesman always says it\u2019s great while it\u2019s in fact lame. \"Computers aren't the thing. They're the thing that gets you to the thing.\" My favorite quote from the excellent show halt and catch fire. Maybe applicable to AI too? I was expecting a spin about the faster horses Ai is a horse, i get it!\n I have a horse, and I put money in the front of the horse, and get \"ponyium\" out the back. If an AI aims at the thing we call it hallucinations, when humans do it we call the delusion goal setting. Either way it is an imagined end point that has no bearing in known reality. \"No, I am not a horse.\" Horse rumours denied. This micro blog meta is fascinating. I've seen small micro blog content like this popping up on the HN home page almost daily now. I have to start doing this for \"top level\"ish commentary. I've frequently wanted to nucleate discussions without being too orthogonal to thread topics. you rather don't want it in your bed Some day, I imagine one will be a senator AI is not a horse (2023)  https://essays.georgestrakhov.com/ai-is-not-a-horse/  I've always said that driving a car with modern driver assist features (lane centering / adaptive cruise / 'autopilot' style self-ish driving-ish) is like riding a horse. The early ones were like riding a short sighted, narcoleptic horse. Newer ones are improving but it's still like riding a horse, in that you give it high level instructions about where to go, rather than directly energising its muscles. A horse that can do your homework. Maybe from the client's point of view, although it's more likely a Tamagotchi. B", "positive": "2025: The Year in LLMs. These are excellent every year, thank you for all the wonderful work you do. Remember, back in the day, when a year of progress was like, oh, they voted to add some syntactic sugar to Java... > Vendor-independent options include GitHub Copilot CLI, Amp, OpenHands CLI, and Pi ...and the best of them all, OpenCode[1] :) [1]:  https://opencode.ai  > The (only?) year of MCP I like to believe, but MCP is quickly turning into an enterprise thing so I think it will stick around for good. Great summary of the year in LLMs. Is there a predictions (for 2026) blogpost as well? > The year of YOLO and the Normalization of Deviance # On this including AI agents deleting home folders, I was able to run agents in Firejail by isolating vscode (Most of my agents are vscode based ones, like Kilo Code). I wrote a little guide on how I did it  https://softwareengineeringstandard.com/2025/12/15/ai-agents...  Took a bit of tweaking, vscode crashing a bunch of times with not being able to read its config files, but I got there in the end. Now it can only write to my projects folder. All of my projects are backed up in git. What an amazing progress in just short time. The future is bright! Happy New Year y'all! Not in this review: Also the record year in intelligent systems aiding in and prompting human users into fatal self-harm. Will 2026 fare better? I'm curious how all of the progress will be seen if it does indeed result in mass unemployment (but not eradication) of professional software engineers.  You\u2019re absolutely right! You astutely observed that 2025 was a year with many LLMs and this was a selection of waypoints, summarized in a helpful timeline.  That\u2019s what most non-tech-person\u2019s year in LLMs looked like. Hopefully 2026 will be the year where companies realize that implementing intrusive chatbots can\u2019t make better ::waving hands:: ya know\u2026  UX  or whatever. For some reason, they think its helpful to distractingly pop up chat windows on their site because", "negative": "Porting 100k lines from TypeScript to Rust using Claude Code in a month. For typing \u201cyes\u201d or \u201cy\u201d automatically into command prompts without interacting, you could have utilized the command \u2018yes\u2019 and piped it into the process you\u2019re running as a first attempt to solving the yes problem. \n https://man7.org/linux/man-pages/man1/yes.1.html  How much does it cost to run Claude Code 24 hrs/day like this. Does the $200/month plan hold up? My spend on Cursor has been high... I'm wondering if I can just collapse it into a 200/month CC subscription. I'm hoping that one day we can use AI to port the millions of lines in the modules of the Python ecosystem to a GIL-free version of Python. Did you ever consider using something like Oh My Opencode [1]?\nI first saw it in the wake of Anthropic locking out Opencode. I haven\u2019t used it but it appears to be better at running continuously until a task is finished. Wondering if anyone else has tried migrating a huge codebase like this. [1]  https://github.com/code-yeongyu/oh-my-opencode  Some quotes from the article stand out: \n\"Claude after working for some time seem to always stop to recap things\"\nQuestion: Were you running out of context? That's why certain frameworks like intentional compaction are being worked on.  Large codebases have specific needs when working with an LLM. \"I've never interacted with Rust in my life\" :-/ How is this a good idea? How can I trust the generated code? This is actually pretty incredible. Cannot really argue against the productivity in this case. Honestly I am really interested in trying to port the rust code to multiple languages like golang,zig, even niche languages like V-lang/Odin/nim etc. It would be interesting if we use this as a benchmark similar to  https://benjdd.com/languages/  or  https://benjdd.com/languages2/  I used gitingest on the repository that they provided and its around ~150k tokens Currently pasted it into the free gemini web and asked it to write it in golang and it said that li"}
{"anchor": "Mousefood \u2013 Build embedded terminal UIs for microcontrollers. Hi Orhun,\nCould it be used with CYD (Cheap yellow display) ? Cool! I just recently began learning the Raspberry Pi Pico. Could anyone recommend a specific display that I could use with the Pico 2/2W and Mousefood? aaaaand this how I learn rust\nI learned go because of bubbletea and mousefood (which combines my work as an embedded systems programmer and love for torminals) is here \u00abMousefood - a no-std embedded-graphics backend for Ratatui!\u00bb Hence 100% Rust. Works on ESP32, RPi2040, and even STM32. Several displays mentioned, including e-ink. This is awesome! I love ratatui, having it available on embedded is very cool! I wonder if it will work with async on embedded e.g. embassy.. Reminds me a lot of the UI styles in the Minecraft mod ComputerCraft. > Embedded-graphics includes bitmap fonts that have a very limited set of characters to save space (ASCII, ISO 8859 or JIS X0201). This makes it impossible to draw most of Ratatui's widgets, which heavily use box-drawing glyphs, Braille, and other special characters You have a bitmap display, you can just draw lines and stuff without needing to rely on font-based hacks. Hey all, thanks for the interest to the crate! I'm currently live on YouTube (doing some maintenance & testing). Feel free to join if you have any questions!  https://www.youtube.com/watch?v=PoYEQJbYNMc  Really neat project but - Rust on embedded. Haven't tried it yet - has anyone got experience comparing it to C/C++? But all the modern TUI are react/solid (claude code, opencode), this should in typescript (thank god it isn\u2019t; why do people drag web everywhere is beyond me) At the bottom of the page there is a mention of \"Phone-OS - A modern phone OS for ESP32 CYD\", so apparently it must be supported. Today I Learned:  https://github.com/witnessmenow/ESP32-Cheap-Yellow-Display  Most likely. I just checked and it uses embedded-graphics already which means you can plug in Mousefood directly. The t", "positive": "Gemini Embedding: Powering RAG and context engineering. The Matryoshka embeddings seem interesting: > The Gemini embedding model, gemini-embedding-001, is trained using the Matryoshka Representation Learning (MRL) technique which teaches a model to learn high-dimensional embeddings that have initial segments (or prefixes) which are also useful, simpler versions of the same data. Use the output_dimensionality parameter to control the size of the output embedding vector. Selecting a smaller output dimensionality can save storage space and increase computational efficiency for downstream applications, while sacrificing little in terms of quality. By default, it outputs a 3072-dimensional embedding, but you can truncate it to a smaller size without losing quality to save storage space. We recommend using 768, 1536, or 3072 output dimensions. [0] looks like even the 256-dim embeddings perform really well. [0]:  https://ai.google.dev/gemini-api/docs/embeddings#quality-for...  To anyone working in these types of applications, are embeddings still worth it compared to agentic search for text? If I have a directory of text files, for example, is it better to save all of their embeddings in a VDB and use that, or are LLMs now good enough that I can just let them use ripgrep or something to search for themselves? Question to other GCP users, how are you finding Google's aggressive deprecation of older embedding models? Feels like you have to pay to rerun your data through every 12 months. I feel like tool calling killed RAG, however you have less control over how the retrieved data is injected in the context. > Embeddings are crucial here, as they efficiently identify and integrate vital information\u2014like documents, conversation history, and tool definitions\u2014directly into a model's working memory. I feel like I'm falling behind here, but can someone explain this to me? My high-level view of embedding is that I send some text to the provider, they tokenize the text and then run ", "negative": "HTTP Cats. I love how there is a Catalan version too! I guess it\u2019s probably a requirement for getting the .cat domain. HTTP 000: HTTP not found. HTTPS CA TLS only. That said, at least they have a broad cipher set support and their HTTPS-only implemetation does work in older browsers and systems. That's nice. But HTTP+HTTPS would be better. Previous discussions:  https://news.ycombinator.com/item?id=37735614  (2023)  https://news.ycombinator.com/item?id=31438989  (2022)  https://news.ycombinator.com/item?id=20283794  (2019)  https://news.ycombinator.com/item?id=10161323  (2015) There's an alternative[0] for the canine lovers. [0]:  https://httpstatusdogs.com  Do any browsers recognize a 420 response code? Not to be confused with Cat as a Service -  https://cataas.com/  this is exactly what I was looking for! Is the picture for 303 meant to be the device from Heisenberg\u2019s thought experiment? Nginx makin' up status codes... This is fun because it\u2019s pre-AI and most of the pics are real. Doing this nowadays would be a meh. 404 should have been the cat footprints in the concrete but without the cat. I unironically use this website everytime I forget a status code at work. The name is instantly memorable, it loads immediately, and I can ctrl-f it. It's basically muscle memory at this point. I used similar idea in an app a while back:  https://github.com/tantalor/emend/blob/master/app/static/ima...  Still gives me a chuckle Why is the quality of the pictures so low? Came for 418. Left happy for Caturday. (Every web site I've built in the last ten years has a series of conditions that combined will trigger a 418.) I\u2019ve used this site every time I\u2019m doing http networking stuff for the past few years. It\u2019s so easy to just go to http.cat/303 to check a status code you don\u2019t know, or to scroll down the homepage to find the number you need for a specific response. The cats make it much more fun than a regular docs page, whilst still being a useful quick reference. I wonder if oth"}
{"anchor": "alphaXiv: Open research discussion on top of arXiv. I remember seeing this idea some years ago. I think it was called qrxiv.org or something like that, but can't find it anymore. I hope this one has better luck, getting the users in the fragmented space of preprints can be a challenge. Hey alphaxiv, you won\u2019t let me claim some of my preprints, because there\u2019s no match with the email address. Which there can\u2019t be, as we\u2019re only listing a generic first.last@org addresses in the papers. Tried the claiming process twice, nothing happened. Not all papers are on Orcid, so that doesn\u2019t help. I think it\u2019ll be hard growing a discussion platform, if there\u2019s barriers of entry like that to even populate your profile. hnews tries to say one positive thing challenge impossible i always love any idea for curating high iq internet community This seems like a horrible idea. I know we need an alternative to peer review but an online comment section feels like something worse than that. This is great. Already loving the discussions/comments I see there. I believe this site is missing a very important thing, direct links to the different categories with a list of papers.\nThis is at least how I (and I believe many others) browse arXiv. I open it up in the morning, scroll through a few categories and open a few papers that look interesting to me. I could see myself using alphaxiv for that, and then, if there's a comment section, I might even read it, and, who knows, leave a comment. But there's no way I'm going to be changing the address or going to some other site to search for papers just to see whether there are some comments. ps: I see the extension adds a \"discussion\" link to arxiv, it is a pity that it is only available for Chrome. What's the main thing that this new website adds over scirate? Great idea. - The frontpage should directly show the list of papers, like with HN. You shouldn't have to click on \"trending\" first. (When you are logged in, you see a list of featured papers ", "positive": "TSMC Risk. There are some near ready foundries in the US and in EU, not to mention South Korea. It would take a few years to catch up of course. What I worry more about is the full lock-in of TSMC production capacity by nvidia/apple/amd/etc for their chips on their latest and greatest silicon process (aka the best in the world). There is 'no space' for performant large RISC-V implementations or other alternative (and it will require several iterations and mistakes will be made) \"AI has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion\" ? Arguably false. Why do you think the US has encouraged TSMC foundries, now inside Arizona ? It's obviously to protect against the scenario that China takes Taiwan. In that case, give it 6 months or less for US TSMC foundries to produce the finest. China taking Taiwan will likely not result in the CCP getting any technology, certainly Taiwanese have \"contingency plans\" to vaporize all tech in the event they are invaded. > AI has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion Taiwan has missiles with the range and warheads to strike the three gorges dam. An attack by China would end very poorly for everybody.  There are millions of people living in the inundation zone. From a NatSec perspective, TSMC isn't really a bottleneck - most weapon systems use SoCs and microcontrollers that can be fabbed on \"legacy nodes\" (ie. 28/40/60/90nm) or 14/20/22nm nodes, and compound semiconductors. The ability to mass produce a Pascal or Volta comparable GPU or Apple A11 comparable SoC is all you need for more cutting edge systems. Power Electronics and Compound Semiconductors (GaN, SiC) have historically been the biggest bottleneck. The bigger risk for the TSMC-China aspect is TSMC's planned exit of GaN foundry production by 2027. Most Chinese manufacturers  still  depend on TSMC-produced GaNs wafers instead of domestically produced Ga", "negative": "The age of Pump and Dump software. Pump and dump software is a hilarious phrase but I thought it would have meant something slightly different. My idea of pump and dump software is the proliferation ai-generated sites (Vercel links) that are sent to the 404 graveyard after a few days of someone not getting any traction on it. Maybe there should be a term for when an industry is at its wits ends so far gone that crypto scams are viable. Noticed the same. Doing a quick analysis of clawdbot myself I figured there are many spam domains that are used to backlink. Now there is a new domain being advertised as a replacement of the original. It points to the same landing page though it is hard to say if this comes from the original authors. All of it seems to be related to a crypto scheme. The astroturfing on reddit is also pretty bad. This is obviously in a blip in the grand scheme of things but it is just an indication what all of these social media platforms are destined to become without some sort of intervention. The framing of the title makes me wonder what we as humans will think of software from this time 100s of years from now. Will the future be a complicated, dense ecosystem of interconnected intelligent systems, putting our current complexity to shame? Or in the future will we look at the current time as the Wild West, the time when software moved more swiftly than the law. Where oil was there for anyone with a big enough guns to protect it. Maybe we will experience our own butlerian jihad and realize that the thinking machines were controlling us the whole time. We will look at TikTok how we now look at the proliferation of ether in the 1800s. The \u201cHow it works\u201d section is an absolute mess.  Each bullet uses a different pronoun, so it\u2019s not clear who the actors actually are and how this all fits together.  How are the \u201ccrypto bros\u201d who approach the \u201ctech person\u201d related to the \u201cfame hungry tech bro\u201d that vibe-coded the failed app? I\u2019m sure there\u2019s a tremendous "}
{"anchor": "Live Map of the London Underground. Very cool. Especially as it a real map not a 'network diagram' who is so confusing. > Data -> TfL live tube data > *You will regret using this data. You will regret using this API.* > It serves data from individual arrivals boards, which all spell stations differently > It has a load-balancer that regularly returns data that is older than the data returned in the previous request. Won't someone think of the Ai overlords who will take care of all this for us in the future. A bit of consistency goes a long way. Looks great but I'm watching this while sitting on a tube right now. What I assumed was my train was lagging by quite a bit and then disappeared! It's cool to see how fast the trains go on different lines. But... where's the Elizabeth line? You get the tooltip when you hover over it, but the polyline is missing. One of the best game I ever played is the text based souvenir game shopping game on Windows 3. I can't recall the name of the game now since it's more than 30 years ago, but it's about shopping souvenirs using London Underground Tube. You have a semi realistic time constraints like train schedules, your flight schedules and of course list of souvenirs items to shop. This is totally offline since there is no Internet available at the time but it's very engaging nonetheless. My proposal for the modern version of the game is to use real-time train schedules (with delays, ticket discounts, etc) that are available publicly on the Internet for many metropolitan cities in the world for examples Tokyo, London and Berlin. Imagine you can have a real-world realistic in-app in-game items purchases feature that you personally can buy in the game and delivered to you or anyone you fancy of giving souvenirs except that you only virtually went there. Maybe a slight bug: the overlay doesnt appear to be locked to the map - when I scroll around, the overlay moves. Currently the northern lines' southern terminal is hovering over Kingsto", "positive": "Functors, Applicatives, and Monads. This reminds me of  https://www.adit.io/posts/2013-04-17-functors,_applicatives,...  I think over the recent years, there's been a rise in typed languages that support functional programming like TypeScript and Rust. It will be interesting to see if this trend continues in the context of AI assistant programming. My guess is that it will become easier for beginners, and the type systems will help to build more robust programs in cooperation with AI. the bit at the end is quite rude of the haskeller responding but I also think they're largely right; another monads explained through boxes tutorial is not gonna help anyone. In fact it's really a step in the wrong direction. Using a few different monads is where to start. Unfortunately, while you may not have appreciated the tone of the Haskell interaction, they are correct in their assessment from a factual perspective. This explanation propagates a number of misunderstandings of the topics well known to be endemic to beginners. In particular, I observed the common belief that functors apply to \"containers\", when in fact they apply to things that are not containers as well, most notably functions themselves, and it also contains the common belief that a monad has \"a\" value, rather than any number of values. For instance, the \"list monad\" will confuse someone operating on this description because when the monad \"takes the value out of the list\", it actually does it once  per value  in the list. This is the common \"monad as burrito\" metaphor, basically, which isn't just bad, but is actually wrong. I'm not limiting it to these errors either, these are just the ones that leap out at me. Coming from non-Haskell background, it took me a good while to undestand that `Just` is a constructor specific to the `Maybe` type. Found this for a quite nice answer:  https://stackoverflow.com/a/18809252  For some reason everyone likes to talk about Monads, but really the other types here are just as in", "negative": "UK House of Lords Votes to Extend Age Verification to VPNs. So will openvpn now get a new command line argument '--passport-number-for-age-verification 8371652299'? And presumably also a '--webcam-to-use-for-identity' Not made clear in this article - this bill will be passed back to the House of Commons to debate/amend before going back to the House of Lords. This was not the final say. Just for clarification.  House of Lords amendments do not have to be accepted by the House of Commons and may not make it into law. If you do not agree with an amendment then write to your MP, write to the ministers concerned.  If you do not tell them your concerns they will not know. You can ask for an appointment with your MP. You can ask for an appointment with ministers.  Better still you can form an advocacy group and lobby. What if I rent a cheap VPS overseas and wireguard my traffic to that? This is very bad news because I have been in contact with low cost providers (lowendtalk) and the community & even they usually end up renting etc. from datacenters and they usually would have name as well So theoretically, suppose I have a vpn company on A) either such lowend niche providers who might support let's say my mission or we are aligned or B) the hyperscalers or large companies. Now I am 99% sure that large companies would actually restrict VPN creation usage (something remarkably rare right now but still it's a gone deal now) And I feel like even with niche lowendbox providers, suppose I am paying 4 euros or something to a provider to get an IP, they are either using hyperscaler themselves (like OVH) or part of a datacenter itself If a server they own in some capacity runs a vps, can it be considered that they are running a vps and they can get sued by the Safety Act too? If not, then what if this happens one layer above at datacenter and now datacenters might have to comply with them I haven't read the article but wtf. Suppose I run a tmate instance (basically allows you to c"}
{"anchor": "Uv: Running a script with dependencies. This is my absolute favourite uv features and the reason I switched to uv. I have a bunch of scripts in my git-hooks which have dependencies which I don't want in my main venv. #!/usr/bin/env -S uv run --script --python 3.13 This single feature meant that I could use the dependencies without making its own venv, but just include \"brew install uv\" as instructions to the devs. The \"declaring script dependencies\" thing is incredibly useful:  https://docs.astral.sh/uv/guides/scripts/#declaring-script-d...      #  script\n  # dependencies = [\n  #   \"requests<3\",\n  #   \"rich\",\n  # ]\n  # \n  import requests, rich\n  # ... script goes here\n  \nSave that as script.py and you can use \"uv run script.py\" to run it with the specified dependencies, magically installed into a temporary virtual environment without you having to think about them at all. It's an implementation of Python PEP 723:  https://peps.python.org/pep-0723/  Claude 4 actually knows about this trick, which means you can ask it to write you a Python script \"with inline script dependencies\" and it will do the right thing, e.g.  https://claude.ai/share/1217b467-d273-40d0-9699-f6a38113f045  - the prompt there was:     Write a Python script with inline script\n  dependencies that uses httpx and click to\n  download a large file and show a progress bar\n  \nPrior to Claude 4 I had a custom Claude project that included special instructions on how to do this, but that's not necessary any more:  https://simonwillison.net/2024/Dec/19/one-shot-python-tools/  Why doesn't pip support PEP 723?  I'm all for spreading the love of our lord and savior uv, but it should be necessary to have an official implementation. Oh this looks amazing!  I had pretty much stopped using Python for my one-off scripts because of the hassle of dependencies.  I can't wait to try this out. Oh nice, I was already a happy user of the uv-specific shebang with in-script dependencies, but the `uv lock --script example.py` ", "positive": "Early Retirement May Speed Up Cognitive Decline: Study. Anecdotally, my grandfather is 92 or so and still works as a journalist (reduced hours). He is still super sharp and does yoga every day. Blows my mind. I hope not, I recently retired. Still, the idea makes some sense. In retirement, I try to read one paper a day (usually deep learning, PGM, or classic AI), play at least one game of Go and Chess, do some recreational programming, and read. But, I don\u2019t work into a state of brain-tiredness anymore like I used to at work. My dad is a doctor in his 70s. He works 60 hour weeks (which he claims counts as retirement for doctors). He truly believes that true retirement is suicide. He wants to be found dead while doing rounds at the hospital. I've always benchmarked post-retiring cognitive abilities and professional continuity with Noam Chomsky. He is my hero in that aspect too. If I can continue to do what I do now at his age, I'm ready for the off. Not only early retirement but any kind of retirement that gets the retiree in a mode that they don't have to try anymore will result in cognitive decline. I am seeing this in my dad who has been retired for ten years now. Throughout his work life he was a sharp hard working banker. Now he uses his age and retirement as an excuse for not trying. Just yesterday he wanted me to order something for him from Amazon. I told him to send me the link to the item. He asked me how to do that. I told him if you can't find the Share link just copy the link and send it to me. He responds by saying that he doesn't know how to do copy-paste. He has been using computers for at least the last fifteen years. I asked him how come he didn't know how to copy-paste. His response was - I am retired now and there's nobody to tell me or teach me. I can see the cognitive decline. Things he used to be able to do, he can't anymore. This type of attitude is also affecting his self respect and confidence. This is something I think about a lot. I plan to", "negative": "The engineer who invented the Mars rover suspension in his garage [video]. Best not read the comments until you've watched at least the first four minutes of the video. \"There are no shortcuts to expertise\". What a fantastic post this. Not surprised of such article. It's not the first time something important is built in a garage: for example, the Apollo 11 lander; a lot of people were thinking it was made from aluminum folio and cardboard in a garage, but actually it was kapton folio and professional-grade cardboard. This guy has incredible videos on hiking gear, examining common claims scientifically and rationally. He never gave any hints as to his professional background, so as not to taint his arguments with appeals to authority. It makes perfect sense that he grew up in this environment, doing engineering work for NASA as a kid! Cool! I just popped in to add that NASA employee Charles White, a scientist involved with the Mars Rover project, also helped make a Burning Man Mars Rover Car (back before Playa Burning Man was completely and utterly torched twice over by Military Industrial Complex Vacationers and Billionaires) and you can hear an interview with him here on Charles White's yt channel:  https://youtu.be/BKGROOedAgI  (\nMars Rover Art Car interview with Ray Cirino and Charles White ) Charles White is a pretty good guy in my opinion, we play the same video game (EvE: Online) Where Charles White is a very, very well known community member who is known as \"The Space Pope\". He officiates weddings at our Iceland Fanfest gathering and also runs a Suicide Prevention Outreach group in EvE: Online, as well as teaching leadership skills. Here's Charles White giving a presentation as an Official NASA employee about Space and our solar system at EvE Fanfest 2016: \n https://www.youtube.com/watch?v=Atm6Y_JYPEU  Heres a interview about EvE: Online with the Space Pope:  https://www.youtube.com/watch?v=dWuj7LfyN4U  anyhow sorry to hijack this about EvE: Online but we ha"}
{"anchor": "Show HN: Ten years of running every day, visualized. Love it! How did you stay motivated? Do you have the source/pipeline available? I love the design and would want to do something similar for my own runs. Congrats on the decade! Did you ever focus on specific metrics or was it always just about the run? just wanted to say the site looks awesome! I love the minimal black+white/grayscale and the fonts are just lovely. vis looks great too, I enjoyed poking around nearly all of the unique runs to look at the map and paces. This is so cool! At what point did you start thinking about this project? Like, were you quietly working on it a year ago after every run, just waiting for this moment? And hey, great run in Japan! (Tokyo here!) I love the map visualization too. Love it! I will hit one year mark in a couple of weeks. Currently maintaining stats in a Google spreadsheet :)  https://vijaykillu.com/  SVGs? So, some of the staistics graphs do not update, or have you made them dynamic by hand? beautifullllllll\u2014both the streak and the stack. Love how lightweight the architecture is for something so personal and long-term. Curious if you noticed any patterns in the data that surprised you once you visualized it? Impressive. I did streak running for 6 months nice and it was some of the most productive running in my life. Interestingly I have much higher yearly averages than you do but still consider daily streak running quite hard. Not being a morning runner myself might contribute since I get into a lot of close calls that way. My streak literally ended when my daughter went into the hospital and I couldn\u2019t well just fuck off for a run any longer. That's awesome! any tips for people who are just starting out? do you have code it on github ? I don't have the tenacity to run strictly _everyday_, so as a middle ground I don't run when it rains at anytime during daylight. Of course the effectiveness of this rule depends on where you live :P I\u2019ve always wanted to do this, but I ", "positive": "Insights into Claude Opus 4.5 from Pok\u00e9mon. The idea of Claude having \"anterograde amnesia\" and the top-rated comment there by Noosphere89 really resonated with me:     \"I would analogize this to a human with anterograde amnesia, who cannot form new memories, and who is constantly writing notes to keep track of their life. The limitations here are obvious, and these are limitations future Claudes will probably share unless LLM memory/continual learning is solved in a better way.\"\n\n  This is an extremely underrated comparison, TBH. Indeed, I'd argue that frozen weights + lack of a long-term memory are easily one of the biggest reasons why LLMs are much more impressive than useful at a lot of tasks (with reliability being another big, independent issue).\n\n  It emphasizes 2 things that are both true at once: LLMs do in fact reason like humans and can have (poor-quality) world-models, and there's no fundamental chasm between LLM capabilities and human capabilities that can't be cured by unlimited resources/time, and yet just as humans with anterograde amnesia are usually much less employable/useful to others than people who do have long-term memory, current AIs are much, much less employable/useful than future paradigm AIs.   I wonder if there's someone at Antrophic working to fine-tune the model's pokemon playing ability specifically. Maybe not but it sure would be funny. This actually matches my experience quite well. I use vision (often) to try and do 2 main things in Claude code: 1) give it text data from something that is annoying to copy and paste (eg labels off a chart or logs from a terrible web UI that doesn't make it easy to copy and paste). 2) give it screenshots of bugs, especially UI glitches. It's extremely good at 1), can't remember when it got it wrong. On 2) it _really_ struggled until opus 4.5, almost comically so, with me posting a screenshot and a description of the UI bug and it telling me \"great it looks perfect! What next?\" With opus 4.5 it's not ", "negative": "Google confirms 'high-friction' sideloading flow is coming to Android. TBH this doesn't seem a particularly high friction change. It seems very like what we have to do already, or like what we do on OSX. How does this relate to the announcement from a while back about introducing signatures that tie back to Google? (IE trusted developer program or whatever they're calling that horse shit.) Google's long term strategy with Android is baffling to me. Apple has had better mobile hardware for years. Apple has higher consumer trust. Apple has better app selection (for most people). Apple has been increasingly implementing the core features that differentiate Android devices, like USB-C and RCS. Every Android user lost to the increasing iOS market share is another customer Google has to pay exorbitant fees to a competitor to access. And Google's strategy is to continue removing differentiating features from Android that  also  help them mitigate the threat of antitrust? Surely the marginal revenue from the inconsequential number of sideloading users isn't attractive enough to justify that kind of strategic blunder. > Matthew Forsyth, Director of Product Management, Google Play Developer Experience & Chief Product Explainer, said the system isn\u2019t a sideloading restriction, but an \u201cAccountability Layer.\u201d And... What about accountability for hosting distributing spyware, malware loaded apps from Google Playstore and hundreds of copy cat, misleading apps? Why can't they pose a question when the phone is setup? - Yes, I want to sideload - No I dont want If the user says NO then to later enable it to allow sideload Yes, the user needs to factory reset phone. Done. When this whole thing got announced, I purchased a new Pixel 9 and flashed it with GrapheneOS. I am hoping that in about 6-8 years (when I realistically need to update) the landscape might be a bit better. Or who knows, maybe I'll just continue using GrapheneOS. So far I have not had a single issues with it. Apps the "}
{"anchor": "European word translator: an interactive map. Love that the numbers in Catalan are represented as numerals, not as words. EDIT: playing with it, it's a bit sad that large numbers do not work at all (in any language); and that not all common forms of a word are shown.  For example, I tried to see how \"ninety six\" is said in french in France, Belgium and Switzerland, but it does not work. Ukrainian and russian words often use the same letters but are pronounced very differently due to distinct phonetics. On the other hand, some Polish and Czech words sound the same or very similar to Ukrainian but look quite different because of their different alphabets. Therefore, phonetic transcription would be a valuable improvement. You immediately see the difference (or similarly) of languages when using words that are very old, such as \"iron\", or \"stone\", which are words that have existed from the origins of that language. I can mostly speak for German. It seems to mix them all into one general language. But there are a lot of local differences between north and south of Germany, Switzerland and Austria. And it\u2019s not just dialect, but really different words that might not be understood everywhere. \nIf you look at the english part it has at least three different words. Similar in Spanish. There are examples from five language families shown here: Indo-European, Basque, Uralic, Turkic, and Afro-Asiatic. The words for bridge split neatly into language subfamilies.   The only exception appears to be Welsh. You are coloring it by 4 colors like map but you should color countries phonetically (speex, levenshtein or something similar) Wiktionary has dialect maps for common Chinese vocabulary that showcases the differences in terminology across various regions of Chinese, rather than their similarities. Example: sleep ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E7%9D%A...  , hide-and-seek ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E6%8D%8...  p.s. I'm saying t", "positive": "Why a 'Boring' Life Might Be the Happiest One. Hard agree! The challenge as you get older is reducing complexity so simple moments can be enjoyed, but it doesn't come easy. Turned 50 recently and I have come to the same conclusion.  I just want to live a simply life with simple joys. However that would only be possible because I've been working and saving since I was 15 years old. I find myself always struggling between being ambitious and being happy. Ideally I'd like to be both. But when one gets on the \"ambitious\" treadmill, capitalism wants one to work 24/7/365. Your competition is showing off how they worked until 4am, worked through the holidays, launched products on Sunday, and slept in the office, as \"dedication\". That culture makes me unhappy because I lose my physical health and mental health doing that. I'm happy and do my best work when I can go home, cook creative dinners, enjoy company of my partner, and enjoy the sunrises and sunsets in the mountains on the weekends. I'm not sure if a 'Boring Life' is for me. But I am sure I need some seasons of my life to be boring. Since I was 20 I've grinded away at my career, side hustles, etc... It made me happy. But at a certain point I got far enough ahead I felt complete. I needed a boring phase. I now wake up, have breakfast with my family, go to work, come home, play with my kids, watch a show, and go to bed. This is a day I would have scoffed at 10 years ago. But it now makes me happy. I don't think it would make me happy if I hadn't went for something the last 15. And it might not make me happy forever, but it's perfect for me right now. The author mistakes introversion to do-little/nothing. Many introverts love socializing and being around friends, it\u2019s just energy-depleting and takes (more) time to recharge. That said, doing little or nothing is quite relaxing, especially on rainy days. > I feel the world has become too fast. Too restless. Too demanding. We don\u2019t say it, but there\u2019s always this quiet pre", "negative": "Velox: A Port of Tauri to Swift by Miguel de Icaza. A \"port\" or \"a nice Swift API for it\"? It seems like the latter in that it requires cargo (the rust build chain) to build. The runtime-wry-ffi ( https://github.com/velox-apps/velox/blob/f062211ced4c021d819... ) file which is 3.2K lines long and has close to a 100 unsafe calls, isn't that just interacting with wry which has it's own crate you could use instead? I'm not 100% sure, but seems to be basically the same as wry itself but without the cross-platform stuff, is that the purpose of that file? Together with the author's distaste for Rust, it seems awfully dangerous instead of pulling in a crate made by Rust developers, but I might misunderstand the purpose of the file here. Not to be confused with Velox a compute engine  https://github.com/facebookincubator/velox/  Eh. Dioxus to me is the more interesting project honestly. To anybody with experience, how's Swift? Especially outside MacOS/iOS programming. Let's say I want to use it standalone for doing some systems programming, how's the standard lib? I'd like to not rely on apple specific frameworks like uikit Have built a cross alternative tailscale gui client based on tauri, the rust and ffi to cgo tailscale feel a little tough, I was wondering it will save a lot time to me if the tauri had been written in go. Seems Miguel\u2019s velox point a new idea, leveraging the wry and use ffi to go, and rewrite some tooling. I hope I will have the spare time and energy to give a try\u2026 For the uninitiated: > Tauri is a framework for building tiny, fast binaries for all major desktop and mobile platforms. Developers can integrate any frontend framework that compiles to HTML, JavaScript, and CSS for building their user experience while leveraging languages such as Rust, Swift, and Kotlin for backend logic when needed.  https://v2.tauri.app/start/  I asked the author about whether this could be ported to support Android/Linux/Windows and he was optimistic it would not be much t"}
{"anchor": "Ask HN: What's one small habit you started that surprisingly changed your life?. Pull-up bar and dumbbells in my bedroom.  10 minutes a day, every morning. Wall calendar, marking each day I do my exercises. Stronger, leaner, fitter, healthier.  Been doing this for years.  It accumulates over time. I know it might sound cliche but walking is an easy habit to start and keep up with. I tend to walk when I get stuck on a problem and try to distract my mind with something else. Usually this turns into my subconscious working through the problem until I get an idea to try out. The health benefits are very nice too if you\u2019re just starting out. There are diminishing returns as your body gets used to it. 15 minutes in the morning/afternoon is my usually habit while at work. But on particularly tough days or just ones with nice weather I\u2019d go for much longer walks. It\u2019s nice to explore the spaces around you when on these walks. You\u2019ll end up discovering more of your surroundings than you\u2019d ever expect. One nice benefit I\u2019ve found is that I can do a 35ish minute walk to the movie theater I\u2019d normally drive to. Doing this lets me just go see a movie on a whim, eat at one of the many restaurants near it. Maybe get a little drunk and be able to walk off a meal/buzz on the way back. I\u2019ve had many epiphanies in this state of mind. You\u2019ll just be a more relaxed individual if you adopted this. When I was younger I used to hate on popular things and be that guy who is like \"how can anyone like this, this is objectively bad\" (for example pop music) But I started a habit of re-framing it instead like \"well if you don't understand why people like something, that is your own failure to understand human behaviour and culture, if you were smarter you would understand why its popular\" That habit of re-framing stuff like that made me look at things a lot more like a neutral observer/anthropologist and not be such a hater Reading. I've never been a huge reader and mainly read non-fiction. I st", "positive": "Colab Pro. This seems to be a hosted Jupyter service, right mybinder, is that right? A preemptible P100 + VM on Google Compute Engine is about ~$0.45/hr, so to exceed that value with Colaboratory Pro (ignoring conveience factors) you'd need to train for more than 22 hours in a month. Which, for deep learning, is not too unreasonable. Reading between the lines of both the signup page and up-to-date FAQ, it seems like the free TPU in Colab notebooks will be depreciated, which isn't too surprising. Good idea, but it's the first premium product that I've seen where the pitch is 'you  may  get certain features if you subscribe'. In another words there is no guarantee and a premium subscriber may still end up with same GPU as a free user. You may end up with a high-end V100 (not available to free) might be a better pitch. Colab is the best notebook I've ever used. It is a real game-changer and I can totally understand why people who use daily would pay for it. > For now, Colab Pro is only available in the US. This is so much better than buying your own hardwares. There's so much data in this universe, people don't know what to do with it. When people don't know what to do, an industry grows to let them \"feel\" they are doing something useful. I wish they would connect Colab under  https://script.google.com  so you can run a notebook at interval times, something akin to what  https://github.com/TensorTom/colabctl  does. I've been using Colab for over a year now. I train deep learning models on NLP and medical imaging datasets. It's a great tool and it lets you focus on the code and the models, instead of the hardware and OS. But $9.99/month is a little expensive for my taste. You can't customize it and if they change something you have to install software by hand sometimes. It should be $1.99/month, that's the kind of price I'd pay for this basic cloud computing service. edit: I use Colab to play with ML models. I really don't think it's possible, for instance, to train a m", "negative": "Start your meetings at 5 minutes past. #leadership is really sending me on this one Apart from just a quick breather between back to back meetings, it also provides a critical bio-break time for your attendees. We do this at my work and guess what - meetings tend to run 5 minutes late because everyone knows the next meeting doesn\u2019t start until 5 past. If you have so many back-to-back meetings, maybe put in a school bell that chimes at 5 minutes before and 5 minutes after the hour. Please just put this in your conference rooms so those of us who know how to evade meetings don't have to hear it as well. Meetings run long so frequently that  we now recommend starting the next meeting late to compensate. This will surely solve the problem. > there is social pressure not to allow meetings to run much past the top of the hour. I've never seen this pressure. > meetings rarely started on the dot anyway before this change. It's like I live in an entirely different world. Start meetings when they say they're going to start.  People will learn to show up quickly.  I think that works better than trying to psychologically game people into cooperation.  That just starts the classic treadmill.  You might have that one friend that you tell to show up half an hour before everyone else. They mentally add the half hour back because you're always giving such early times.  Better IMO to just keep things simple.  Let people leave when they need to.  Show up on time. We've been doing this at Qualcomm for a while, and I really like it. While meetings do run over sometimes, the practice has still built this acceptance around short breaks between meetings. No one bats an eye if we've got two consecutive meetings together, the first one ends late, and we wait five minutes before starting or joining the next one. In fact, having done it for so long, it surprisingly really annoys me when our vendors schedule 60 minute meetings on the hour. I've always wondered at the company cultures between Go"}
{"anchor": "My favorite cult sci-fi and fantasy books you may not have heard of before. Anyone got some favs to share? I need to read the new Peter Hamilton book (book 2 due out soon). And I am ashamed to admit I haven't read any Greg Egan yet, need to get on that :) Of these I've only read  The Worm Ouroboros , and I cannot recommend it enough.  The structure is a bit weird at first\u2014you gotta get past the first chapter\u2014but after that it settles and is astounding.  If you have any passing interest in Lord of the Rings, you'll likely love it. I read sci-fi but not fantasy, why do people insist on lumping these together? John C Wright wrote a nice short story set in William Hope Hodgson's \"The Night Land\"  Awake in the Night   https://web.archive.org/web/20090524012412/http://www.thenig...  In the side panels are users/readers who drew up their own maps on what they think the Nightscape is. It has all the romantic mystery of a fantasy tale, whilst still being firmly grounded in reality. I remember when London's Shard was going up, and I'd see it lit up slightly at night, glowing and ominous and thinking, \"this is it: this is the last holdout of humanity.\" Am I missing something, or is \u201cLove And Chocolate\u201d a romance novel that was included as a joke? Seems pretty incongruous in this list. One of the effects of passing of time is that even some of the greats of yore are now obscure and cultish. Like the whole golden age I have met very few people under 50 that have read the early Schekley short stories. That are probably one of the sci fi peaks. But in cult and unknown works - Ticket to tranai. One of the best (anti) utopias written. I'd like to recommend one as well: The City & the City, by China Mi\u00e9ville. A delightful, unique experience! Fresh and original, \u201cfantasy science fiction\u201d. Not a big fan of detective stories and noir, but this is something else. I would like to recommend The Wandering Inn by Pirateaba. You won't read anything else for years. If you like these books -- e", "positive": "Gemini Diffusion. Interesting to see if GROQ hardware can run this diffusion architecture..it will be  two time magnitude of currently known speed :O That's...ridiculously fast. I still feel like the best uses of models we've seen to date is for brand new code and quick prototyping. I'm less convinced of the strength of their capabilities for improving on large preexisting content over which someone has repeatedly iterated. Part of that is because, by definition, models cannot know what is  not  in a codebase and there is meaningful signal in that negative space. Encoding what  isn't  there seems like a hard problem, so even as models get smarter, they will continue to be handicapped by that lack of institutional knowledge, so to speak. Imagine giving a large codebase to an incredibly talented developer and asking them to zero-shot a particular problem in one go, with only moments to read it and no opportunity to ask questions. More often than not, a less talented developer who is very familiar with that codebase will be able to add more value with the same amount of effort when tackling that same problem. I think the lede is being buried. This is a great and fast InstructGPT. This is absolutely going to be used in spell checks, codemods, and code editors. Instant edits feature can surgically perform text edits fast without all the extra fluff or unsolicited enhancements. I copied shadertoys, asked it to rename all variables to be more descriptive and pasted the result to see it still working. I'm impressed. Diffusion is more than just speed. Early benchmarks show it better at reasoning and planning pound for pound compared to AR. This is because it can edit and doesn\u2019t suffer from early token bias. Nit: Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like Mercury [1] still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I ", "negative": "Exercise can be nearly as effective as therapy for depression. This is a finding that keeps coming up, and I've certainly found it true in my life, but there's a significant chicken-and-egg problem in that depression frequently precludes the motivation to exercise, and if you don't already have a deeply-disciplined routine to overcome the lack of motivation, people won't do it. Exhortation to develop those good habits in the good times, I suppose. I take this to mean therapy for depression (particularly for men) is barely effective at all and exercise is not quite as barely effective at all. If therapy for depression were a pill, I'm not sure it'd demonstrate enough efficacy to get approved. It's almost as if humans were optimized to constantly move around all day by evolution. Huh. Who would have thought. I will speak from my experience. I have diabetes and I try to manage it well, with workout. But sometimes when the sugars are high for a while, I can feel it, the sadness, the hopelessness. It took me a while to understand that is high sugar levels and a mild form of depression. Now I will do some workout when I feel that and after a little workout, I can see how my mind also start to feel better. This is not a solution for everyone who is experiencing depression probably but might help some who are experiencing because of high sugar levels. If it helps anyone I take antidepressants and have had a positive experience with them. Depression can be caused by a chemical imbalance and no amount of exercise or talking about it will fix it. One of the most frustrating things when your really low is people giving advice like do exercise to feel better - please don\u2019t do this. I decided to go unmedicated from SSRIs/mood stabilizers in 2024 and it changed my life for the better. Prior to stopping, I thought I was physically unhealthy until I found out excessive overheating & exhaustion was a common side effect of Zoloft. Since then, I've lost around 40 pounds and have been a"}
{"anchor": "Waymo robotaxi hits a child near an elementary school in Santa Monica. From the Waymo blog... >  the pedestrian suddenly entered the roadway from behind a tall SUV, moving directly into our vehicle's path. Our technology immediately detected the individual as soon as they began to emerge from behind the stopped vehicle. The Waymo Driver braked hard, reducing speed from approximately 17 mph to under 6 mph before contact was made.  >  Following contact, the pedestrian stood up immediately, walked to the sidewalk, and we called 911. The vehicle remained stopped, moved to the side of the road, and stayed there until law enforcement cleared the vehicle to leave the scene.  >  Following the event, we voluntarily contacted the National Highway Traffic Safety Administration (NHTSA) that same day.  I honestly cannot imagine a better outcome or handling of the situation. > The vehicle remained stopped, moved to the side of the road How do you remain stopped but also move to the side of the road? Thats a contradiction. Just like Cruise. A human driver would most likely have killed this child.  That's what should be on the ledger. And before the argument \"Self driving is acceptable so long as the accident/risk is lower than with human drivers\" can I please get that out of the way: No it's not. Self driving needs to be orders of magnitude safer for us to acknowledge it. If they're merely as safe or slightly safer than humans we will never accept it. Becase humans have a \"skin in the game\". If you drive drunk, at least you're likely to be in the accident, or have personal liability. We accept the risks with humans because those humans accept risk. Self driving abstracts the legal risk, and removes the physical risk. I'm willing to accept robotaxis, and accidents in robotaxis, but there needs to be some solid figures showing they are way _way_ safer than human drivers. I'm curious as to what kind of control stack Waymo uses for their vehicles. Obviously their perception stack has ", "positive": "\u201cErdos problem #728 was solved more or less autonomously by AI\u201d. Reconfiguring existing proofs in ways that have been tedious or obscured from humans, or using well framed methods in novel ways, will be done at superhuman speeds, and it'll unlock all sorts of capabilities well before we have to be concerned about AGI. It's going to be awesome to see what mathematicians start to do with AI tools as the tools become capable of truly keeping up with what the mathematicians want from the tools. It won't necessarily be a huge direct benefit for non-mathematicians at first, because the abstract and complex results won't have direct applications, but we might start to see millenium problems get taken down as legitimate frontier model benchmarks. Or someone like Terence Tao might figure out how to wield AI better than anyone else, even the labs, and use the tools to take a bunch down at once. I'm excited to see what's coming this year. This is great, there is still so much potential in AI once we move beyond LLMs to specialized approaches like this. EDIT: Look at all the people below just reacting to the headline and clearly not reading the posts. Aristotle ( https://arxiv.org/abs/2510.01346 ) is key here folks. EDIT2: It is clear much of the people below don't even understand basic terminology. Something being a transformer doesn't make it an LLM (vision transformers, anyone) and if you aren't training on language (e.g. AlphaFold, or Aristotle on LEAN stuff), it isn't a \"language\" model. You can try out Aristotle yourself today  https://aristotle.harmonic.fun/ . No more waitlist! Can anyone with specific knowledge in a sophisticated/complex field such as physics or math tell me: do you regularly talk to AI models? Do feel like there's anything to learn? As a programmer, I can come to the AI with a problem and it can come up with a few different solutions, some I may have thought about, some not. Are you getting the same value in your work, in your field? For context, Teren", "negative": "Nvidia's 10-year effort to make the Shield TV the most updated Android device. Now if only they would release an updated one. honestly, shield tv changed how i interact with my tv and my opinion about Android TV (even though its market sucks) It's ironic that Nvidia before becoming a behemoth had the money for this kind of device. I have had two for 10 years and have no complaints whatsoever I've got the OG model, and it's still the main device hooked up to my TV. All my TV streaming goes through it (mostly Jellyfin these days), and it can stream games no problem via Moonlight. It's hooked up to a 4k LG TV, and I have no idea about how it does the upscaling, but 720p content looks perfectly fine on it. Best (worst?) of all... it still gets updates. Shield TV + extra storage + HDHomeRun tuner is still a great device for getting OTA TV. The only downside is that more recent versions use the Google Android TV launcher which is filled with a garbage truck full of ads, often for things I would never want to watch (horror movies? Nope!).  Yes you can replace the launcher, but that's a pain. Would love to pay more for a device that has updated codec support, no ads or tracking, and is basically identical. My shield bricked itself after just a few months, so YMMV on this. No rhyme or reason why. Dupe of  https://news.ycombinator.com/item?id=46824003  What a new model would need is more compat with DV and better software. Get rid of the Android ads. Add better frame rate matching. Etc. The Shield TV's cylindrical form factor could use a rethink. It is hard to find a good spot for it on a shelf when cords are connected at both ends (HDMI and MMC slot at one end, power and LAN at the other) and the ports are too close for all cords to use right-angle-heads. Leaving it invisible by placing it on the floor or behind other gear sometimes impedes Bluetooth signal, so there it sits, well apart from the AVR, BD, other devices. My Nvidia Shield Portable is sad to hear this.  They upd"}
{"anchor": "AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms. This is very neat work! Will be interested in how they make this sort of thing available to the public but it is clear from some of the results they mention that search + LLM is one path to the production of net-new knowledge from AI systems. Software engineering will be completely solved. Even systems like v0 are astounding in their ability to generate code, and are very primitive to whats coming. I get downvoted on HN for this opinion, but its truly going to happen. Any system that can produce code, test the code, and iterate if needed will eventually outperform humans. Add in the reinforcement learning, where they can run the code, and train the model when it gets code generation right, and we are on our way to a whole different world. Good method to generate synthetic training data, but only works for domains where validation can be scaled up. Calling it now - RL finally \"just works\" for any domain where answers are easily verifiable. Verifiability was always a prerequisite, but the difference from prior generations (not just AlphaGo, but any nontrivial RL process prior to roughly mid-2024) is that the reasoning traces and/or intermediate steps can be open-ended with potentially infinite branching, no clear notion of \"steps\" or nodes and edges in the game tree, and a wide range of equally valid solutions. As long as the quality of the end result can be evaluated cleanly, LLM-based RL is good to go. As a corollary, once you add in self-play with random variation, the synthetic data problem is solved for coding, math, and some classes of scientific reasoning. No more modal collapse, no more massive teams of PhDs needed for human labeling, as long as you have a reliable metric for answer quality. This isn't just neat, it's important - as we run out of useful human-generated data, RL scaling is the best candidate to take over where pretraining left off. Maybe this one can stop writing a fu", "positive": "Gemini Diffusion. Interesting to see if GROQ hardware can run this diffusion architecture..it will be  two time magnitude of currently known speed :O That's...ridiculously fast. I still feel like the best uses of models we've seen to date is for brand new code and quick prototyping. I'm less convinced of the strength of their capabilities for improving on large preexisting content over which someone has repeatedly iterated. Part of that is because, by definition, models cannot know what is  not  in a codebase and there is meaningful signal in that negative space. Encoding what  isn't  there seems like a hard problem, so even as models get smarter, they will continue to be handicapped by that lack of institutional knowledge, so to speak. Imagine giving a large codebase to an incredibly talented developer and asking them to zero-shot a particular problem in one go, with only moments to read it and no opportunity to ask questions. More often than not, a less talented developer who is very familiar with that codebase will be able to add more value with the same amount of effort when tackling that same problem. I think the lede is being buried. This is a great and fast InstructGPT. This is absolutely going to be used in spell checks, codemods, and code editors. Instant edits feature can surgically perform text edits fast without all the extra fluff or unsolicited enhancements. I copied shadertoys, asked it to rename all variables to be more descriptive and pasted the result to see it still working. I'm impressed. Diffusion is more than just speed. Early benchmarks show it better at reasoning and planning pound for pound compared to AR. This is because it can edit and doesn\u2019t suffer from early token bias. Nit: Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like Mercury [1] still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I ", "negative": "House of Lords Votes to Ban UK Children from Using Internet VPNs. Earlier:  https://news.ycombinator.com/item?id=46763548  Papers, please. Glory to Arstotzka! Does that mean that VPN providers now need identification before you can open an account? Very shallow, naive approach to child safety.  This is like banning children from riding scooters on a highway.  They're just going to use a bike instead.  Danger still exists. VPNs are not the only way around this, so if you want to ban the \"method of access\" you need to be much more broad, and get the parents involved. > may make provision for the provider of a relevant VPN service to apply to any person seeking to access its service in or from the UK age assurance which is highly effective at correctly determining whether or not that person is a child \"The law we made is like super duper good!!\" > Children may also turn to VPNs, which would then undermine the child safety gains of the Online Safety Act \"The law we made is easily circumvented :(\" I foresee a lot of VPN companies starting to offer \"secure proxy\" services or something like that. \"It's not a VPN, it's a secure proxy!\" After enforcing age verification to prevent children from viewing those pesky Gaza genocide videos that Israel did not want them to see, they gotta ensure that those brats wont be able to get around it and still see the videos. Its amazing how this censorship was brought on rapidly and precisely after Netanyahu demanded it at the start of last year. No surprise as half of Starmer government was funded by zionists. [dupe] Discussion:  https://news.ycombinator.com/item?id=46763548  UK nanny state makes it an nonviable place to live. It's pervasive from the moment you step off the plane at Heathrow and see the inane safety stickers covering every surface \"WARNING: DOOR\" \"WARNING: WATER FROM HOT TAP IS HOT\" as well as the CCTV cameras. ah the country of brexit has more \"clever ideas\" something I find myself saying often lately watching BBC News e"}
{"anchor": "AlphaFold 3 predicts the structure and interactions of life's molecules. The article was heavy on the free research aspect, but light on the commercial application. I'm curious about the business strategy. Does Google intend to license out tools, partner, or consult for  commercial partners? s/predicts/attempts to predict From:  https://www.nature.com/articles/d41586-024-01383-z  >Unlike RoseTTAFold and AlphaFold2, scientists will not be able to run their own version of AlphaFold3, nor will the code underlying AlphaFold3 or other information obtained after training the model be made public. Instead, researchers will have access to an \u2018AlphaFold3 server\u2019, on which they can input their protein sequence of choice, alongside a selection of accessory molecules. [. . .] Scientists are currently restricted to 10 predictions per day, and it is not possible to obtain structures of proteins bound to possible drugs. This is unfortunate. I wonder how long until David Baker's lab upgrades RoseTTAFold to catch up. > What is different about the new AlphaFold3 model compared to AlphaFold2? > AlphaFold3 can predict many biomolecules in addition to proteins. AlphaFold2 predicts structures of proteins and protein-protein complexes. AlphaFold3 can generate predictions containing proteins, DNA, RNA, ions,ligands, and chemical modifications. The new model also improves the protein complex modelling accuracy. Please refer to our paper for more information on performance improvements. AlphaFold 2 generally produces looping \u201cribbon-like\u201d predictions for disordered regions. AlphaFold3 also does this, but will occasionally output segments with secondary structure within disordered regions instead, mostly spurious alpha helices with very low confidence (pLDDT) and inconsistent position across predictions. So the criticism towards AlphaFold 2 will likely still apply? For example, it\u2019s more accurate for predicting structures similar to existing ones, and fails at novel patterns? This is a basic ", "positive": "DeepSeek-v3.1. For reference, here is the terminal-bench leaderboard:  https://www.tbench.ai/leaderboard  Looks like it doesn't get close to GPT-5, Claude 4, or GLM-4.5, but still does reasonably well compared to other open weight models. Benchmarks are rarely the full story though, so time will tell how good it is in practice. It's a hybrid reasoning model. It's good with tool calls and doesn't think too much about everything, but it regularly uses outdated tool formats randomly instead of the standard JSON format. I guess the V3 training set has a lot of those. It seems behind Qwen3 235B 2507 Reasoning (which I like) and gpt-oss-120B:\n https://artificialanalysis.ai/models/deepseek-v3-1-reasoning  Pricing:  https://openrouter.ai/deepseek/deepseek-chat-v3.1  Unrelated, but it would really be nice to have a chart breaking down Price Per Token Per Second for various model, prompt, and hardware combinations. For local runs, I made some GGUFs! You need around RAM + VRAM >= 250GB for good perf for dynamic 2bit (2bit MoE, 6-8bit rest) - can also do SSD offloading but it'll be slow. ./llama.cpp/llama-cli -hf unsloth/DeepSeek-V3.1-GGUF:UD-Q2_K_XL -ngl 99 --jinja -ot \".ffn_.*_exps.=CPU\" More details on running + optimal params here:  https://docs.unsloth.ai/basics/deepseek-v3.1  Seems to hallucinate more than any model I've ever worked with in the past 6 months. About halfway between V3 and Qwen3 Coder.  https://brokk.ai/power-ranking?version=openround-2025-08-20&...  Cheep! $0.56 per million tokens in \u2014 and $1.68 per million tokens out. They say the SWE bench verified score is 66%. Claude Sonnet 4 is 67%. Not sure if the 1% difference here is statistically significant or not. I'll have to see how things go with this model after a week, once the hype has died down. Looks quite competitive among open-weight models, but I guess still behind GPT-5 or Claude a lot. I have yet to see evidence that it is better for agentic coding tasks than GLM-4.5 Sad to see the off peak discount", "negative": "Netflix Animation Studios Joins the Blender Development Fund as Corporate Patron. Brilliant. Some of the animations that are put as showcases on the Blender site are absolutely phenomenal. \nThis one  https://studio.blender.org/projects/spring/  particularly is my all time favorite. I think especially since the UI overhaul in Blender 2.8 the project has been on a steep upwards trajectory. The software was always amazing, especially since it was free and open source, but the new UI and all subsequent improvements really put Blender on the map as a serious tool and not just an alternative for when you don't have money for the big players. Very cool news. Personally, I'd love to see some more focus on game-dev workflows. The game asset pipeline still feels janky: texture painting exists, but not great, and baking textures/previewing results or baking from high poly to low poly involves a lot of manual node fiddling and rewiring. Export/iterate/build/test cycles are also pretty painful still. How does it compare to Maya these days? <3 Blender is a treasure and must be protected. I really like Blender and it's an amazing product, but I can't get over the standard Blender keymap. The \"industry compatible\" workflow is more sane, but then I have to translate tutorials from the Blender keymap to the industry compatible controls, and they're not always 1:1 How does that translate into real cash? If someone is wondering who the Aras guy is\n https://mastodon.gamedev.place/@aras/115971315481385360  Anyone know why Netflix doesn\u2019t respond to their job site?  I applied to several positions where I\u2019m an  exact  match, with a decade of VFX and another decade of internet company experience in LA.  Never heard a single word in response from them, for years.  Reqs stay open a long time as well.  What are they doing?  Are they ghost jobs?  They don\u2019t even respond with a \u201cno\u201d form letter.  (Lately their site is broken at the verify email stage, pin post returns 403.) We might see a transi"}
{"anchor": "Horses: AI progress is steady. Human equivalence is sudden. Engine efficiency, chess rating, AI cap ex. One example is not like the other. Is there steady progress in AI? To me it feels like it\u2019s little progress followed by the occasional breakthrough but I might be totally off here. I think it's a cool perspective, but the not-so-hidden assumption is that for any given domain, the efficiency asymptote peaks well above the alternative. And that really is the entire question at this point: Which domains will AI win in by a sufficient margin to be worth it? This is a fun piece... but what killed off the horses wasn't steady incremental progress in steam engine efficiency, it was the invention of the internal combustion engine. People are not simple machines or animals. Unless AI becomes strictly better than humans and humans + AI, from the perspective of other humans, at all activities, there will still be lots of things for humans to do to provide value for each other. The question is how do our individuals, and more importantly our various social and economic systems handle it when exactly what humans can do to provide value for each other shifts rapidly, and balances of power shift rapidly. If the benefits of AI accrue to/are captured by a very small number of people, and the costs are widely dispersed things can go very badly without strong societies that are able to mitigate the downsides and spread the upsides. \"In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines. And not very long after, 93 per cent of those horses had disappeared. I very much hope we'll get the two decades that horses did.\" I'm reminded of the idiom \"be careful what you wish for, as you might just get it.\" Rapid technogical change has historically lead to prosperity over the long term but not in the short term. My fear is that the pace of change this time around is so rapid that the short term d", "positive": "OpenClaw \u2013 Moltbot Renamed Again. Previously:  Clawdbot Renames to Moltbot   https://news.ycombinator.com/item?id=46783863  Right now I'm just thinking about all the molt* domains..... \u00af\\_(\u30c4)_/\u00af I would have stood my ground on the first name longer. Make these legal teams do some actual work to prove they are serious. Wait until you have no other option. A polite request is just that. You can happily ignore these. The 2nd name change is just inexcusable. It's hard to take a project seriously when a random asshole on Twitter can provoke a name change like this. Leads me to believe that identity is more important than purpose. and openclaw.com is a law firm. It's hilarious that atm I see \"Moltbook\" at the top of HN. And it is actually not Moltbot anymore? But I have to admit that OpenClaw sounds much better. This is indeed feeling very much like Accelerando\u2019s particular brand of unchecked chaos. Loving every minute of it, first thing in our timeline that makes sense where it regards AI for the masses :) What if Lamborghini had acquired Claw to automate their vehicles? amateur hour, new phase of the AI bubble reminds me of Andre Conje, cracked dev, \"builds in public\", absolutely abysmal at comms, and forgets to make money off of his projects that everyone else is making money off of (all good if that last point isn't a priority, but its interrelated to why people want consistent things) Should have named it \u201cbot formerly known as Moltbot\u201d and invented a new emoji sigil :) Apparently it had another name before Clawdbot as well, I think BotRelay or something. It\u2019s on pragmatic engineer Hilarious to see the most pointless vibecoded slop written to interact with an RDP server. Unnecessary introduces loopholes. How to annoy and alienate your target audience in 2 short weeks. Before using make sure you read this entirely and understand it:\n https://docs.openclaw.ai/gateway/security \nMost important sentence: \"Note: sandboxing is opt-in. If sandbox mode is off\"\nDon't do that, ", "negative": "Television is 100 years old today. This is interesting.   John Logie Baird did in fact demonstrate something that looked like TV, but the technology was a dead end. Philo Farnsworth demonstrated a competing technology a few years later, but every TV today is based on his technology. So, who actually invented Television? High definition is nearly 90 years old? I guess their definition of high is quite low by more modern standards. Odd we never adapted to it. Video has a strange hypnotic power over most people and messages seem to bypass normal mental defenses. And 100 years ago my great-aunt and grandmother (both RIP) were little kids and my great-grandmother, born in the 19th century and which I knew very well for she lived until 99 years old, was filming them playing on the beach using a \"Pathe Baby\" hand camera. I still have the reels, they look like this:  https://commons.wikimedia.org/wiki/File:Films_Path%C3%A9-Bab...   https://fr.wikipedia.org/wiki/Path%C3%A9-Baby  And we converted some of these reels to digital files (well brothers and I asked a specialized company to \"digitalize\" them). 100 years ago people already had cars, tramways (as a kid my great-grandmother tried to look under the first tramway she saw to see \"where the horses were hiding\"), cameras to film movies, telephones, the telegraph existed, you could trade the stock market and, well, it's knew to me but TV was just invented too. Inspired one of my absolute favorite Zappa grooves. I am the Slime  https://www.youtube.com/watch?v=iiCQcEW98OY  I am gross and perverted I'm obsessed and deranged I have existed for years But very little has changed I'm the tool of the Government And industry too For I am destined to rule And regulate you I may be vile and pernicious But you can't look away I make you think I'm delicious With the stuff that I say I'm the best you can get Have you guessed me yet? I'm the slime oozin' out From your TV set You will obey me while I lead you And eat the garbage that I feed"}
{"anchor": "Giving Up a $250k Salary to Retire Early Is Hard. This is the OP:  https://www.vetmed.auburn.edu/faculty/erik-hofmeister/  This (+ many other signals) are giving me the \"market top\" vibe. Sad to see people still parroting the 4% rule when you can get \"risk free\" US Treasuries, today, paying more than that. Not to speak of the numerous, still conservative, investments paying far higher. This isn't the 2010s era with ultra low fixed income yields. If you intend to retire early, please educate yourself on the state of the market Someone in the article's comments asked about working part time and the author responded \"veterinary academia doesn\u2019t really understand <1.0 FTE.\" Is the same true of FAANG-ish companies? Can you (officially) work part time in a big tech job? College professor in Alabama makes $250k? Not bad. I guess being a doctor helps. I think the vibe may instead be: growing income disparity. The WSJ reported over the weekend that over 50% of all consumer spending now comes from the top 10% of household incomes. So while some folks are flush to retire early, many are not. I think 4% is still a fine thing to plan around. I don't think it's wise to plan as if today's treasury rates will last your entire retirement. The 4% rule requires increasing that amount at the rate of inflation throughout retirement. 30 years in the case of the original studies. Even with an inflation rate of 2.5%, the required withdrawal will more than double after 30 years. The 4.625% you can lock into a 30-year Treasury would not be enough. What does the 4% rule have to do with yields of treasuries? This is a 30-year time horizon that changes spending purely based on inflation figures. Yields in the market do not matter. You can and many do, although this tends to be reserved for more senior engineers.  Obviously a pay cut is involved. Strictly the answer is yes with the more truthful answer being it depends on your manager. The easiest way is probably being in a country that requires", "positive": "Erdos 281 solved with ChatGPT 5.2 Pro. The erdosproblems thread itself contains comments from Terence Tao:  https://www.erdosproblems.com/forum/thread/281  Has anyone verified this? I've \"solved\" many math problems with LLMs, with LLMs giving full confidence in subtly or significantly incorrect solutions. I'm very curious here. The Open AI memory orders and claims about capacity limits restricting access to better models are interesting too. From Terry Tao's comments in the thread: \"Very nice! ... actually the thing that impresses me more than the proof method is the avoidance of errors, such as making mistakes with interchanges of limits or quantifiers (which is the main pitfall to avoid here). Previous generations of LLMs would almost certainly have fumbled these delicate issues. ... I am going ahead and placing this result on the wiki as a Section 1 result (perhaps the most unambiguous instance of such, to date)\" The pace of change in math is going to be something to watch closely. Many minor theorems will fall. Next major milestone: Can LLMs generate useful abstractions? This must be what it feels like to be a CEO and someone tells me they solved coding. I have 15 years of software engineering experience across some top companies. I truly believe that ai will far surpass human beings at coding, and more broadly logic work. We are very close Out of curiosity why has the LLM math solving community been focused on the Erdos problems over other open problems?  Are they of a certain nature where we would expect LLMs to be especially good at solving them? This is crazy. It's clear that these models don't have human intelligence, but it's undeniable at this point that they have _some_ form of intelligence. FWIW, I just gave Deepseek the same prompt and it solved it too (much faster than the 41m of ChatGPT). I then gave both proofs to Opus and it confirmed their equivalence. The answer is yes. Assume, for the sake of contradiction, that there exists an \\(\\epsilon > 0\\) ", "negative": "ICE using Palantir tool that feeds on Medicaid data. Any time I see people say \"I don't see why I should care about my privacy, I've got nothing to hide\" I think about how badly things can go if the wrong people end up in positions of power. The classic example here is what happens when someone is being stalked by an abusive ex-partner who works in law enforcement and has access to those databases. This ICE stuff is that scaled up to a multi-billion dollar federal agency with, apparently, no accountability for following the law at all. Wishful thinking but it would be real great if a future leader destroyed this infrastructure. I'm sure they'll run on not using it but when systems like this exist they tend to find applications Why would Medicaid have the data of anyone who is at risk of immigration enforcement? The reported connection seems tenuous: > The tool \u2013 dubbed Enhanced Leads Identification & Targeting for Enforcement (ELITE) \u2013 receives peoples\u2019 addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources. So, they have a tool that sucks up data from a bunch of different sources, including Medicaid.  But there's no actual nexus between Medicaid and illegal immigrants in this reporting. Edit: In the link to their earlier filings, EFF claims that some states enroll illegal immigrants in Medicaid:  https://www.eff.org/deeplinks/2025/07/eff-court-protect-our-...  This current administration and their policies have definitely influenced my opinion on the 2018 debate around citizenship questions on the US census. (For more context:  https://www.tbf.org/blog/2018/march/understanding-the-census... ) Glad to see this post didn't get flagged like the one that was posted yesterday on a similar topic about ICE data mining and user tracking.  https://news.ycombinator.com/item?id=46748336  \"ICE Budget Now Bigger Than Most of the Wo"}
{"anchor": "A Step Behind the Bleeding Edge: A Philosophy on AI in Dev. I'm very happy with the chat interface thanks. * The interface is near identical across bots * I can switch bots whenever I like. No integration points and vendor lock-in. * It's the same risk as any big-tech website. * I really don't need more tooling in my life. > \u201cTheir (ie the document\u2019s) value stems from the discipline and the thinking the writer is forced to impose upon himself as [she] identifies and deals with trouble spots\u201d. Real quote > \"Hence their value stems from the discipline and the thinking the writer is forced to impose upon himself as he identifies and deals with trouble spots in his presentation.\" I mean seriously? > If you ask AI to write a document for you, you might get 80% of the deep quality you\u2019d get if you wrote it yourself for 5% of the effort. But, now you\u2019ve also only done 5% of the thinking. This, but also for code. I just don't trust new code, especially generated code; I need time to sit with it. I can't make the \"if it passes all the tests\" crowd understand and I don't even want to. There are things you think of to worry about and test for as you spend time with a system. If I'm going to ship it and support it, it will take as long as it will take. This is one of the more true and balanced articles. On the verification loop: I think there\u2019s so much potential here. AI is pretty good at autonomously working on tasks that have a well defined and easy to process verification hook. A lot of software tasks are \u201cmigrate X to Y\u201d and this is a perfect job for AI. The workflow is generally straightforward - map the old thing to the new thing and verify that the new thing works the same way. Most of this can be automated using AI. Wanna migrate codebase from C to Rust? I definitely think it should be possible autonomously if the code base is small enough. You do have to ask the AI to intelligently come up with extensive way to verify that they work the same. Maybe UI check, sample inp", "positive": "String theory can now describe a universe that has dark energy?. Only in universe with 5 dimensions. Shouldn't string theory be given up on at this point? This theory has existed for over 50 years and hasn't produced any results. Even the predictions made by it such as e.g. supersymmetry have not been confirmed despite searching for them at particle colliders. I foolishly sat in 8.821 [0] while at MIT thinking I could make sense out of quantum gravity. Most of the math went over my head, but the way I understand this paper, it\u2019s basically a cosmic engineering fix for a geometry problem. Please correct me if necessary. String theory usually prefers universes that want to crunch inwards (Anti-de Sitter space). Our universe, however, is accelerating outwards (Dark Energy). To fix this, the authors are essentially creating a force balance. They have magnetic flux pushing the universe's extra dimensions outward (like inflating a tire), and they use the Casimir effect (quantum vacuum pressure) to pull them back inward. When you balance those two opposing pressures, you get a stable system with a tiny bit of leftover energy. That \"leftover\" is the Dark Energy we observe. You start with 11 dimensions (M-theory) and roll up 6 of them to get this 5D model. It sounds abstract, but for my engineer brain, it's helpful to think of that extra 5th dimension not as a \"place\" you can visit, but as a hidden control loop. The forces fighting it out inside that 5th dimension are what generate the energy potential we perceive as Dark Energy in our 4D world. The authors stop at 5D here, but getting that control loop stable is the hardest part The big observatiom here is that this balance isn't static -- it suggests Dark Energy gets weaker over time (\"quintessence\"). If the recent DESI data holds up, this specific string theory solution might actually fit the observational curve better than the standard model. [0]  https://ocw.mit.edu/courses/8-821-string-theory-and-holograp...  Hm, string", "negative": "Six-decade math puzzle solved by Korean mathematician. (2024) Source is Scientific-American  https://www.scientificamerican.com/article/mathematicians-so...  ( https://news.ycombinator.com/item?id=42946052 ) Discussion on the paper (131 points, 2024, 36 comments)  https://news.ycombinator.com/item?id=42300382  >> \u201cYou keep holding on to hope, then breaking it, and moving forward by picking up ideas from the ashes,\u201d [Baek] said in an interview with a web magazine published by Korean Institute for Advanced Study. \u201cI\u2019m closer to a daydreamer by nature, and for me mathematical research is a repetition of dreaming and waking up.\u201d beautiful! Dan Romik has a nice intro on the moving sofa problem:  https://www.math.ucdavis.edu/~romik/movingsofa/  Is the equivalent problem in 3D harder or easier? Seems like it would be harder but you never know with these things. Actual paper:  https://arxiv.org/pdf/2411.19826  This is the famous sofa problem! It's hard to believe it's finally solved; I've spent many evenings staring at the wikipedia article wondering at how even what seem to be the simplest of problems defy the reach of mathematics. I love the kind of science reporting on display in this article! It stays at a consistent, objective level of detail throughout (no \"imagine a vector space as a block of jello\" or whatever it is that Quanta and other publications are always doing). It allows specialists to understand exactly what's being claimed, and at the same time stays accessible to laypeople. It feels like it's written for the kind of reader that I aspire to be: not necessarily a specialist on every topic under the sun, but someone who has finished high school and is paying attention. Though I guess writing like this doesn't pay off in the modern world. Most readers don't consistently pay attention when reading, and to be honest, I don't either. Looking at that graphic... it almost seems obvious. The outer corner radius would be relative to the inner curve radius in some fi"}
{"anchor": "Downtown Denver's office vacancy rate grows to 38.2%. Looks like there is an opportunity to convert a lot of that into residential space. Well but is the demand for office space in Denver so high? I think Denver (I live here) is an example of our horrible zoning. We have entirely focused our cities (especially Denver and RTD (Regional Transportation District)) around people commuting in for work. This is one of the main principles of BAD design, where you create an entire area around close to a single use (offices). That creates a very fragile city. This \"single use\" zoning that the US proliferated makes us really fragile to changes like working from home vs in-office work. Another point is that cities are rather hostile for families. We create cities so they need to be fled as soon as people have kids. We have streets entirely of concrete and 1 and 2 bedroom apartments. If we want cities to be more resilient we need to rethink them. We need streets that have greenspace as a fundamental part of the infrastructure. We need permeable surfaces. I went to Park am Gleisdreieck in Berlin and stayed in a multi-family unit right along the park. There were tons of families with kids playing in the park, people riding bikes for transportation along the park bike paths, adults playing ping pong on outdoor tables together. It was wonderful. It made me rethink what a city can look like. Denver needs to take notes. We don't need a single use city and a light rail system that only goes into that city. We made an incredibly fragile city. We can build better cities. They've got a graph of vacancy rates going back to 2023.  But I'm wondering about longer term.  Does anyone have data going back to, say, the 1980s? (Why the 1980s?  Because I go back that far.  I have some sense of what the business cycle was doing during those times.  I'd like to know if this is really historically unusual, or just a blip, possibly a COVID-related one.) Downtown Denver also kind of just sucks. A couple", "positive": "Purely Functional Sliding Window Aggregation Algorithm. This is a very interesting algorithm which is more or less known in the folklore, but is still relatively obscure. I have used it as a part of temporal logic monitoring procedure:  https://github.com/Agnishom/lattice-mtl/blob/master/src/Moni...  This is similar to an approach I use but instead of a queue, I accomplish this using a ring buffer that wraps around and overwrites entries older than window size. We maintain a global window aggregate, subtract ring buffer slot aggregate for entries dropping out and accumulate new entries into new slot aggregate while adding it to the global aggregate. Everything is o(1) including reads, which just returns the global window aggregate. That was a well written and easily approachable blog post on what I found to be an interesting topic. Aside from the topic itself, I think I also learned a bit about structuring technical articles. Competitive Programming in Haskell...I can only define this as Masochistic Aesthetics... I have made a quick c++ implementation for those unfamiliar with Haskell :  https://gist.github.com/unrealwill/5ca4db9beefafaa212465277b...  The queue method is popular, but there's a much faster (branch-free) and in my opinion simpler way, known as the van Herk/Gil-Werman algorithm in image processing. It splits the input into windows and pairs up a backward scan on one window with a forward scan on the next. This works for any associative function. I was very surprised when I learned about it that it's not taught more often (the name's not doing it any favors)! And I wrote a tutorial page on it for my SIMD-oriented language, mostly about vectorizing it which I didn't quite finish writing up, but with what I think is a reasonable presentation in the first part:  https://github.com/mlochbaum/Singeli/blob/master/doc/minfilt...  I also found an interesting streaming version here recently:  https://signalsmith-audio.co.uk/writing/2022/constant-time-p...  EDIT:", "negative": "Tesla\u2019s autonomous vehicles are crashing at a rate much higher tha human drivers. To be honest I think the true story here is: > the fleet has traveled approximately 500,000 miles Let's say they average 10mph, and say they operate 10 hours a day, that's 5,000 car-days of travel, or to put it another way about 30 cars over 6 months. That's tiny! That's a robotaxi company that is literally smaller than a lot of taxi companies. One crash in this context is going to just completely blow out their statistics. So it's kind of dumb to even talk about the statistics today. The real take away is that the Robotaxis don't really exist, they're in an experimental phase and we're not going to get real statistics until they're doing 1,000x that mileage, and that won't happen until they've built something that actually works and that may never happen. By the law of large numbers, it's not a significant distance. As long as there are still safety drivers, the data doesn't really tell you if the AI is any good. Unless you had reliable data about the number of interventions by the driver, which I assume Tesla doesn't provide. Still damning that the data is so bad even then. Good data wouldn't tell us anything, the bad data likely means the AI is bad unless they were spectacularly unlucky. But since Tesla redacts all information, I'm not inclined to give them any benefit of the doubt here. The comparison isn't really like-for-like. NHTSA SGO AV reports can include very minor, low-speed contact events that would often never show up as police-reported crashes for human drivers, meaning the Tesla crash count may be drawing from a broader category than the human baseline it's being compared to. There's also a denominator problem. The mileage figure appears to be cumulative miles \"as of November,\" while the crashes are drawn from a specific July-November window in Austin. It's not clear that those miles line up with the same geography and time period. The sample size is tiny (nine crashes)"}
{"anchor": "AI just proved Erdos Problem #124. This seems to be 2nd in row proof from the same author by using the AI models. First time it was the ChatGPT which wrote the formal Lean proof for Erdos Problem #340.  https://arxiv.org/html/2510.19804v1#Thmtheorem3  > In over a dozen papers, beginning in 1976 and spanning two decades, Paul Erd\u0151s repeatedly posed one of his \u201cfavourite\u201d conjectures: every finite Sidon set can be extended to a finite perfect difference set. We establish that {1, 2, 4, 8, 13} is a counterexample to this conjecture. Ok\u2026 has this been verified? I see no publication or at least an announcement on Harmonics webpage. If this is a big deal, you think it would be a big deal, or is this just hype? Related, independent, and verified: GPT-5 solved Erd\u0151s problem #848 (combinatorial number theory):  https://cdn.openai.com/pdf/4a25f921-e4e0-479a-9b38-5367b47e8...   https://lifearchitect.ai/asi/  More interesting discussion than on Twitter here:  https://www.erdosproblems.com/forum/thread/124#post-1892  This is response from mathematician:\n\"This is quite something, congratulations to Boris and Aristotle! On one hand, as the nice sketch provided below by tsaf confirms, the final proof is quite simple and elementary - indeed, if one was given this problem in a maths competition (so therefore expected a short simple solution existed) I'd guess that something like the below would be produced. On the other hand, if something like this worked, then surely the combined talents of Burr, Erd\u0151s, Graham, and Li would have spotted it. Normally, this would make me suspicious of this short proof, in that there is overlooked subtlety. But (a) I can't see any and (b) the proof has been formalised in Lean, so clearly it just works! Perhaps this shows what the real issue in the [BEGL96] conjecture is - namely the removal of 1 and the addition of the necessary gcd condition. (And perhaps at least some subset of the authors were aware of this argument for the easier version allowing 1", "positive": "Gemini 3 Flash: Frontier intelligence built for speed. Deepmind Page:  https://deepmind.google/models/gemini/flash/  Developer Blog:  https://blog.google/technology/developers/build-with-gemini-...  Model Card [pdf]:  https://deepmind.google/models/model-cards/gemini-3-flash/  Gemini 3 Flash in Search AI mode:  https://blog.google/products/search/google-ai-mode-update-ge...  They went too far, now the Flash model is competing with their Pro version. Better SWE-bench, better ARC-AGI 2 than 3.0 Pro. I imagine they are going to improve 3.0 Pro before it's no more in Preview. Also I don't see it written in the blog post but Flash supports more granular settings for reasoning: minimal, low, medium, high (like openai models), while pro is only low and high. Don\u2019t let the \u201cflash\u201d name fool you, this is an amazing model. I have been playing with it for the past few weeks, it\u2019s genuinely my new favorite; it\u2019s so fast and it has such a vast world knowledge that it\u2019s more performant than Claude Opus 4.5 or GPT 5.2 extra high, for a fraction (basically order of magnitude less!!) of the inference time and price Does this imply we don't need as much compute for models/agents? How can any other AI model compete against that? Pretty stoked for this model. Building a lot with \"mixture of agents\" / mix of models and Gemini's smaller models do feel really versatile in my opinion. Hoping that the local ones keep progressively up (gemma-line) These flash models keep getting more expensive with every release. Is there an OSS model that's better than 2.0 flash with similar pricing, speed and a 1m context window? Edit: this is not the typical flash model, it's actually an insane value if the benchmarks match real world usage. > Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications. The replacement for old flash models will be proba", "negative": "Tell HN: Merry Christmas. Merry Christmas y'all\nkeep promptin' Fun seeing HN restart to switch to the Christmas theme, in three steps:  https://share.cleanshot.com/vJZv6k03  (restarting the server)  https://share.cleanshot.com/qFyM347P  (online but temporarily readonly)  https://share.cleanshot.com/kW8kY7mp  (back online!) I got scared when I refreshed and HN turned red Merry x-mas.  We all deserve love and happiness.  Be kind to your neighboor. I was surprised to see HN change colour. A Merry Christmas to everyone ! and a healthy dose of Australian Christmas Stereotypes to all:  https://www.youtube.com/watch?v=amL8QRiDH2E  including  not  the original Rolf Harris version:  https://www.youtube.com/watch?v=CQVEZLcBfS8  Merry Christmas! \"An angel of the Lord appeared to them, and the glory of the Lord shone around them, and they were terrified. But the angel said to them, \u201cDo not be afraid. I bring you good news that will cause great joy for all the people. Today in the town of David a Savior has been born to you; he is the Messiah, the Lord. This will be a sign to you: You will find a baby wrapped in cloths and lying in a manger.\u201d Suddenly a great company of the heavenly host appeared with the angel, praising God and saying, \u201cGlory to God in the highest heaven, and on earth peace to those on whom his favor rests.\u201d\" -Luke 2 \"A Spaceman Came Travelling\" - Chris de Burgh Merry Christmas to everyone. Being a non-Christian and it being Christmas time, I am sharing one verse from the New Testament that is, in my opinion, useful - or at the very least, insightful - to anyone, regardless of religion. Luke 16:10: He who is faithful in a very little thing is faithful also in much; and he who is unrighteous in a very little thing is unrighteous also in much. Merry Christmas Christus natus est O \u03a7\u03c1\u03b9\u03c3\u03c4\u03cc\u03c2 \u03b3\u03b5\u03bd\u03bd\u03b9\u03ad\u03c4\u03b1\u03b9 \u0425\u0440\u0438\u0441\u0442\u043e\u0441 \u0440\u0430\u0436\u0434\u0430\u0435\u0442\u0441\u044f \u05d4\u05de\u05e9\u05d9\u05d7 \u05e0\u05d5\u05dc\u05d3 \u0627\u0628\u0646 \u0627\u0644\u0644\u0647 \u064a\u0648\u0644\u062f \u0627\u0644\u064a\u0648\u0645 Merry Christmas! I hope everyone has wonderful time with family. eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZXNzYWdlIjoiT"}
{"anchor": "The Prophet of Parking: A eulogy for the great Donald Shoup. Oregon eliminated burdensome parking regulations in most larger cities and: it's fine. Many home builders still add parking to new projects because there is market demand for it - and they are also competing for tenants or buyers against existing housing which has parking. But there is now the flexibility to do some projects without parking, which really helps at the affordable end of the spectrum, and is a good fit for more walkable locations. BTW, Nolan Gray, cited as the author, has a book out himself that's really approachable and good reading if you're interested in cities:  https://islandpress.org/books/arbitrary-lines  Shoup passed away on February 6: *  https://parkingreform.org/donald-shoup/  *  https://cal.streetsblog.org/2025/02/08/streetsblog-mourns-th...  *  https://news.ycombinator.com/item?id=43004881  His book: *  https://en.wikipedia.org/wiki/The_High_Cost_of_Free_Parking  * EconTalk podcast episode:  https://www.youtube.com/watch?v=8Sgmw3jQcyc  > Nor are minimum parking requirements even needed: developers have the knowledge and incentives to provide the appropriate amount of off-street parking. If a developer builds too little parking, she will struggle to attract tenants and command lower rents. This isn't entirely true. In cities where parking requirements are eliminated, many new businesses move into locations that would have previously been illegal, showing that many commercial tenants view parking requirements as excessive. In my city, judging by public comment, support for parking requirements comes not from business owners or developers but from voters who fear a lack of parking at the businesses they frequent and who fear that parking for nearby businesses or apartment buildings will overflow into their neighborhood (the horror.) > One survey of the literature suggests that drivers in the typical American city spend an average of eight minutes looking for parking at the end of ea", "positive": "One Head, Two Brains: The origins of split-brain research (2015).  https://web.archive.org/web/20241228215151/https://www.theat...   https://archive.ph/gJ32A  The confabulation to justify picking out related images that the left brain never observed (chicken and snow shovel in the article) reminds me profoundly of the confident slop produced by LLMs. Make you wonder if llms might be one half of the \"brain\" of a true AGI I disagree with Steven Pinker\u2019s claim that consciousness arises from the brain. This perspective fails to establish that the brain produces consciousness, as it relies on the mistaken assumption that \"mind\" and \"consciousness\" are interchangeable. While brain activity may influence the mind, consciousness itself could be a more fundamental aspect of reality. Rather than generating consciousness, the brain might function like a radio, merely receiving and processing information from an all-pervasive field of consciousness. In this view, a split-brain condition would not create two separate consciousnesses but instead allow access to two distinct streams of an already-existing, universal consciousness. Related:  You Are Two  by GCP Grey: < https://www.youtube.com/watch?v=wfYbgdo8e-8 > Looks like this was one of the inspirations behind severance. The fact that the explaining part of the brain fills in any blanks in a creative manner (you need the shovel to clean the chicken shed), reminds me to some replies of LLMs. I once provided an LLM the riddle of the goat, cabbage and wolf, and changed the rules a bit. I prompted that the wolf was allergic to goats (and hence would not eat them). Still the llm insisted on not leaving them together on the same river bank, because the wolf would otherwise sneeze and scare the goat away. My conclusion was that the llm solved the riddle using prior knowledge plus creativity, instead of clever reasoning. If one is interested in hemisphere theory, including psychological and philosophical implications, make sure to chec", "negative": "High-speed train collision in Spain kills at least 39. Always try to sit in seats where your back is toward the direction of motion. The train in question is a Frecciarossa 1000  https://en.wikipedia.org/wiki/Frecciarossa_1000  The Italians designed it but won't run it at more than 300km/h in Italy citing local infrastructure concerns. I guess that leaves other countries to find the edge cases. I'll be interested to find out how fast it was going during the crash. If you\u2019re interested in this kind of thing, look up plainly difficult on youtube. He has more videos on train crashes than I\u2019ve seen, and I\u2019m embarrassed how many I\u2019ve seen. Here\u2019s one to get you started:  https://youtu.be/VV2rIHEp5AM?si=sSBT9s49PqbLTGbt  There are a lot of safety lessons embedded in these videos, which is why I like them. I also did a double take when I heard \"semaphore\"; its history goes back far longer than the ~century of software engineering.  https://en.wikipedia.org/wiki/Semaphore  For many years the Spanish state-owned company RENFE had a monopoly on Spain's huge high speed rail network. However their high prices, inconvenient schedules and poor customer service were often criticized, and so when, to the annoyance of RENFE and many spanish politicians, additional foreign operators entered the market on the key Madrid - Barcelona route, ridership doubled whilst ticket prices halved. So I would standby for this tragedy to be used for political purposes to try and get foreign operators banned from Spanish tracks, regardless of the facts of the matter. Updated to 39 people now, but probably the number can still go up Terrible and condolences to anybody affected. For a bit of context according to the OECD 2023 Spain had ~1800 on the road during the previous year, so that's about 5/day. There are more deaths on the road in Spain in a couple of weeks than this tragic accident. Either way it's too many deaths obviously but I want to highlight what a freak event this is compared to a more p"}
{"anchor": "I am rich and have no idea what to do. Same situation, I truly empathise because it really does seem to take a lot of purpose out of everything. What I\u2019ve found is that you need to replace money/salary/financial success optimisation (assuming you spent a lot of your life and energy to this point focused on these, much like I did) with something else totally unconnected with being measured in that way. For me, I am focused on proving myself as a guitarist in the local jazz and blues scene. These people have no idea how much money I have and wouldn\u2019t give a shit if they did (I didn\u2019t really change my lifestyle after getting lucky so it\u2019s not obvious). So it\u2019s an area I can be creative, grow, and still feel like I\u2019m doing something. At the same time I\u2019m doing part time consulting, mainly for people I worked with in the past who have started companies, just to scratch the tech itch. So far so good but I can\u2019t say yet if it will stick. Maybe for you it\u2019s art, music, going and getting another unrelated degree, or something along those lines? If you have more money than you know what to do with, fundraising and supporting good causes can be really rewarding. Both in terms of giving back something to your local community, and having really nice social elements to it. One big piece of advice I have is to try to avoid letting others in your social network know exactly how successful you\u2019ve been. Everyone starts wanting to pitch you their investment idea and it can burn down friendships when their ideas are bad. Being a VC to your friends is a path to sadness for everyone. Therapy. Wealth and success is one of the most massive crutches there is. It can make it almost impossible to be truly in touch with your insecurities and pain because its simply too easy to hide in your victory. Your toughest challenge now is to, despite your wealth, find a way to contact the pain that drove you to your hunger for success. As the bible said, it's easier for a camel to get through the head o", "positive": "Review of Anti-Aging Drugs. From the conclusion paragraph: > Your primary life extension program is diet and exercise. Choose a diet that works for you. Stay slim. Considering heart disease is the #1 killer, doing whatever you can to not die from heart disease is the best place for most people to start. Even in 2025, diet and exercise are still king. Winner, \"Ascorbic\". Do they mean Vitamin C? Would any of the OTC stuff even be effective? Melatonin, NAC, and Berberine. Be careful when reading such blogs: > Note that the dosage in the mouse experiments is quite high \u2014 0.1% of the body weight every day, meaning about 2 ounces a day for me (70 kg). Mouse and human metabolism are very different. A better starting estimate would be 5g/day, not 57g/day. I hope people dont accidentally overdose themselves because of lack of a pharmacology background. A lot of people in the comments are talking about the \"problem\" of death and approaches to take, but really, the only thing you can do is philosophically make your peace. Anything else at this point is yelling into infinity. > Fast for short intervals regularly, and longer fasts as they feel good to you. You can effectively do this every day if you just eat once per day. When I was properly obese, this technique resulted in rapid weight loss. Zero exercise was required to see results, which was good at the time because the not eating part was about all I could handle. Being in a fasted state is as close as you can get to actually  reversing  aging. Your body engages in a process called autophagy when nutrient-sensing pathways are down-regulated. When you are stuffing your face constantly (i.e., every ~8 hours), there is less opportunity for this mechanism to do its job.  https://en.wikipedia.org/wiki/Autophagy  Richard Miller's Intervention Testing Program should really be your go-to for this:  https://www.nia.nih.gov/research/dab/interventions-testing-p...  He has no conflicts of interests, works for the NIA, and he's quite o", "negative": "Heathrow scraps liquid container limit. Not because of a sudden outbreak of sanity, but because they have CT scanners now. FINALLY (PS.  Still not going to fly there) Good. This should happen on all airports now. Otherwise it's useless. You won't be flying from Heathrow to Heathrow. The security theater needs to go on. In the meantime batteries represent a much bigger risk with potential in flight fires but I guess nobody cares enough to do anything about it. Let me get this straight. If the article is correct, the new capabilities are related to better detection of large liquid containers, not determination of whether or not the liquid is dangerous. So - you couldn\u2019t take large amounts of liquids previously because some liquids in large amounts might be able to be weaponized. If you were caught with too much liquid (in sum total, or in containers that are too large) they\u2019d throw it out and send you on your way. But now that they have the ability to detect larger containers, they\u2026 do what? Declare that it\u2019s safe and send you on your way with it still in your possession? This rule wasn't enforced anyway... I travel a lot - and never take out any liquids. Have nail clippers and scissors in my carry-on. Once I even had an opinel pocket knife in my laptop bag for a couple of months. Travelled through Tokyo, Taipei, SFO, DEN, PHX, LAX, BOS, JFK, FRA, AMS, MUC, LHR - nobody noticed. I seriously had forgotten it was there, so I don't do that now, but still... Also, no large water bottles or similar. Unless on domestic flights in Japan, where this is totally fine. IDK - security theater. But if it helps. Famously Steve Jobs had a story about shaving time off of boot-up and equating it to saving lives on the concept of people sitting their waiting for the computer to boot up just lost that much of their lives. [1] I actually do believe there is value in thinking this way and it is one of my biggest arguments against TSA. Everything has a cost, including 'security' and 'safet"}
{"anchor": "Show HN: SQL Noir \u2013 Learn SQL by solving crimes. This is really lovely. It makes me happy that you invested time into building this for other people. this is awesome! soooo fun. Thank you for building this. Sharing it immediately Really cute. But I really want the ability to put the different tabs -- Brief, Workspace, Schema -- side-by-side. I know SQL and wanted to play with this, but the UX was frustrating enough to drive me away, even though it is really pretty. This is pretty fun, I tried the two free mysteries and was fun solving them. One nit, would be nice if the SQL editor supported comments so we can comment out old queries before running new one so as keeping a history esp if we need to run the same queries again. Good stuff :) This gives me a childhood flashback to a show called MathNet, an educational police procedural (a la Dragnet) where both investigators have holstered calculators.  https://en.m.wikipedia.org/wiki/Mathnet  reminds me of  https://mystery.knightlab.com/  That was fun!\nOne question, is there a way to execute the current hovered/selected query and not the first on in the workspace? Something is weird in at least firefox.\nType or paste:     select \\* \n  from crime_scene;\n  \nthen TYPE so you add a comment     select \\* \n  from crime_scene;\n  --\n  \nI see     select ----from---------------- \n  \nbut when I select the text I see what I wrote - I like the comment my text (and I was pulling in the instructions) but it renders some interesting garbage pretty fast. Feature request: Badges for completing challenges. Issue Closed, Won\u2019t Fix. Badges?? We don\u2019t need no  stinking  badges! When I was in university, my instructor linked something like this for SQL practice (a crime solving minigame, just like this one). I remember getting really into it, even going to the extreme of trying to find the most efficient one-liner solution. Thanks for making this. I\u2019ll be passing the torch by linking it to anyone interested in learning SQL. This is super fun!", "positive": "Scientists find a way to regrow cartilage in mice and human tissue samples. Of course, why are the good ones always in mice?     A study led by Stanford Medicine researchers has found that an injection blocking a protein linked to aging can reverse the natural loss of knee cartilage in older mice.    https://www.science.org/doi/10.1126/science.adx6649  A small molecule inhibitor of 15-hydroxy prostaglandin dehydrogenase causes cartilage regeneration. I hope they fast-track it to human trials. basically every growth process in the body can be induced by chemicals. and so now people are starting to take some of these chemicals. we will see how it turns out As long as regrowth can be controlled. Otherwise we call it cancer. Would be amazing to get a treatment for osteoarthritis. Fusion Power Cartilage Regrowth Room Temperature Semiconductors Quantum Computing       def generate(topic, year):\n       return f\"Scientists have made a major breakthrough in {topic}\"\n  \nThe only subjects that are more Year Of The Linux Desktop than Linux itself. The discovery of gerozymes is interesting. Maybe aging is pre-programmed after all, to make space for new generations. Would this work for rheumatoid arthritis? I don\u2019t know anything about it myself so it could be a completely different thing, but someone I know has it and it is awful. Would be great to see a treatment coming through. My dream is to be able to run again. Please. Let me run a 10k at least once more in my life. To feel that stillness and freedom and calm that sets in when the brain start going to hibernation after about 7km. That would be quiet something to feel that again. I\u2019ve had my shoulders \u201ccleaned up\u201d arthroscopically, and the pain is still a major preventer of movement. I would love to stay on the mats longer with something that doesn\u2019t harken to medieval times. So excited at this prospect. HN posts about mouse studies always trigger a bunch of skepticism.  I\u2019m a layperson so it\u2019s hard to separate the informed c", "negative": "There's only one Woz, but we can all learn from him. Woz is by far the person in computing history for whom I have the most respect. Dude is an absolute  legend , and from everything I have heard is humble and kind on top of his crazy skills. If I could get to the point where I had even 10% of his skill and generosity of spirit, I would consider myself to have done pretty well. It\u2019s a stark contrast to today's mindset where we often just throw more resources at the problem.  His obsession with elegance over features is something I try to keep in mind, even if it's harder in modern web dev. \" Let's make it shorter and punchier.  \"Woz's floppy disk controller design is still the gold standard for doing in software what competitors needed a whole board of chips to do.  That kind of obsession with elegance over brute force is exactly what's missing in modern engineering. Only one Woz? What about Scott? Coincidentally one of the earliest Apple I prototypes ends its auction tomorrow if you have over $500K to spare:  https://news.ycombinator.com/item?id=46605420  For me, anyone who is involved in FOSDEM in any way deserves more respect (regarding revolutionary things we can learn) I learned some very bad jokes from him. It's kinda funny... In '89 a friend and I were talking about starting a startup like the two Steve's (we didn't know about Ron Wayne back then.)  We both knew exactly what Woz did, but were a bit sketchy on Jobs role in the early days.  Don't get me wrong, I'm not saying Jobs was a layabout, only that the strengths he brought to the table were more abstract. So I would also say... the kinds of things we learn from Woz are concrete and we get immediate feedback if we learned them wrong. Had to let this here: A TV clip on YouTube of an episode of \u201cThat\u2019s Incredible\u201d, featuring Apple co-founder Stephen \u201cWoz\u201d Wozniak (aged 38) running through a maze and nearly winning.  https://www.youtube.com/watch?v=PoJexQjoMtk  (found on the blog of Cabel Sasser:  https://ca"}
{"anchor": "Early Retirement (2006). How much money can a Silicon Valley software engineer expect to have at retirement? This article says $7 million:  https://www.wealthmeta.com/blog/being-a-doctor-vs-being-a-so...  Here's the back-of-the-envelope calculation: Annual salary: $250K (it is much higher at FAANG). Assume 25% tax (effective, not marginal), which leaves us with $187.5K, which is about $15K per month. Assume $5000 in monthly expenses, which leaves us with $10K per month investable money. (Note: since net worth includes house and 401(k) I have not deducted mortgage or 401(k) payments). So $10K includes those items.) Now use a compound interest calculator to see what $10K per month at 8% interest (assume investment in S&P 500 and allow for some market crashes) will result in, at the end of 30 years. It is about $14 million, which would include your house and your 401(k). Here's a spreadsheet that allows you to plug in your own numbers:\n https://docs.google.com/spreadsheets/d/1Ryu_-mVYxSdJbW8lmf1z...  > Retirement forces you to stop thinking that it is your job that holds you back. For most people the depressing truth is that they aren't that organized, disciplined, or motivated. Maybe, but I don\u2019t know anyone who\u2019s less happy being retired. They might not be living their retirement fantasy, but the pressure and stress of having to work is gone. He's always been an interesting chap. I wish him well.  > Most important, do not retire in the expectation that it will be easy to find rewarding non-profit volunteer work.  Thankfully, this has not been an issue with me. I definitely don't have the massive nest egg that another commenter mentioned, but I've got enough to avoid starving, and I am happier than I ever thought I'd be. > How much work does the average college student get done? Almost none. Yet the same person, injected into a corporate bureaucracy, becomes a reasonably effective worker. Why? Most people have terrible time management skills. This limitation is of no ", "positive": "Functional programming and reliability: ADTs, safety, critical infrastructure. This article seems to conflate strong type systems with functional programming, except in point 8. It makes sense why- OCaml and Haskell are functional and were early proponents of these type systems. But, languages like Racket don\u2019t have these type systems and the article doesn\u2019t do anything to explain why they are _also_ better for reliability. >In banking, telecom, and payments, reliability is not a nice to have. It is table stakes. This reliability isn't done by being perfect 100% of the time. Things like being able to handle states where transactions don't line up allowing for payments to eventually be settled. Or for telecom allowing for single parts of the system to not take down the whole thing or adding redundancy. Essentially these types of businesses require fault tolerance to be supported. The real world is messy, there is always going to be faults, so investing heavily into correctness may not be worth it compared to investing into fault tollerance. I'm wary of absolute statements about programming. I like good type systems, too, but they won't save you from bugs that are better addressed by fuzz testing, fault injection testing and adversarial mindset shifts. Strong types: yes, it\u2019s definitely better Functional programming: no, functional programming as in: the final program consists in piping functions together and calling the pipe. In my opinion, that tends to get in the way of complex error handling. The problem being that raising Exceptions at a deep level and catching them at some higher level is not pure functional programming. So your code has to deal with all the cases. It is more reliable if you can do it, but large systems have way too many failure points to be able to handle them all in a way that is practical. I think there is a strong case that ADTs (algebraic data types) aren't so great after all. Specifically, the \"tagged\" unions of ADT languages like Haskell ", "negative": "Show HN: Phage Explorer. some quick feedback on the user interface: - i pressed \"Amino Acids\", and nothing updated below the toolbar. can't figure out what it does - the \"Tools\" buttons looks like a segmented picker, but both seem to actually initiate a modal presentation this tool seems interesting, but it would be worth polishing some of these ui quirks because my first impression was that it seems a bit broken (or confused me)! but seems like a cool project otherwise, love people building and sharing explainers as they learn stuff! I really resonate with your goal of creating a more intuitive tool than a boring textbook. Being able to visually see how complex genetic code translates into physical geometry (3D structures) would be incredibly helpful for students. Thank you for sharing such a wonderful educational tool! Research into phages is paramount, as they represent one of our best hopes for combating the rapidly increasing problem of antibiotic resistance (largely driven by the overuse of antibiotics, even including last-resort antibiotics, in industrial animal agriculture so industrial farms can place more animals per sq/m without them dying from lack of space and cut-off body parts so they take even less space). What used to be a life time project that would inspire awe and respect and make OP an instant hire for most managers is now a fun 1-week stunt that makes you go \u201ccool, how many tokens?\u201d Of course, the result is cool and maybe even useful (I wouldn\u2019t dare say _correct_, being ignorant of the topic), but I cannot help but think that this would have been tremendously better if done the old (proper?) way. Also, I suspect that OP would have learned so much more on the topic. Sorry, but I really wouldn't trust a website that uses Gemini's image models for generating scientific diagrams. The website itself is also vibe-coded. That's not an issue by itself, but there are lots of layout issues visible, see  https://i.imgur.com/SidB6pI.png  A bit offtopic: I"}
{"anchor": "Voronoi map generation in Civilization VII. Related:  https://www.redblobgames.com/x/2022-voronoi-maps-tutorial/  I've been trying to generate my own maps using Voronoi diagrams as well.  I was using Lloyd's algorithm [0] to make strangely shaped regions \"fit\" better, but I like the insight of generating larger regions to define islands, and then smaller regions on top to define terrain. One of the things I like about algorithms like this is the peculiarities created by the algorithm, and trying to remove that seems to take some of the interesting novelty away. - [0]  https://en.m.wikipedia.org/wiki/Lloyd%27s_algorithm  This kind of exploratory/creative programming is bar none the most fun you can have as a software engineer. I love reading write-ups about projects like this because you can practically feel the nerdy joy radiating off the screen. Haven't played any of the new Civ games but find this very interesting. On a related note, I've started a blog on procedural content generation and GenAI content synthesis:  https://gamedev.blog/ . Would love any feedback / suggestions! I intend to cover Voronoi diagrams in the near future + a Python implementation and turning it into a 3D map with Unity This is super interesting! I've dabbled with Perlin noise procedural generation using AlphaEvolve[0] and wonder if it would be interesting to do one with Voronoi map too! [0]:  https://blog.toolkami.com/alphaevolve-toolkami-style/  Raymond Hill (unblock) also made JavaScript voronoi library  https://github.com/gorhill/Javascript-Voronoi  I really wish they just made Civ 5 again, but with these sorts of cool updates. Kinda surprised that it's taken this long. Voronoi for map generation is not a new concept, and it produces excellent results. One of the best webs for gamedev. The a-star/Dijkstra section is legendary. It is quite infectious! I would have never thought to use Voronoi like this, my only use is with data visualizations. Civ4-Beyond the Sword is IMHO the last good", "positive": "Maze Algorithms (2017). I've always known about algorithms that solve mazes, but never about actually making them.  It's interesting seeing all these algorithms and how the mazes they generate look different. Mike Bostock had several very lovely visualizations back on the D3.js site which I can't find. Here's a cool blogpost he wrote:  https://bost.ocks.org/mike/algorithms/#maze-generation  Is it known which algorithms produce 'difficult' mazes? I'm imagining you could run all the maze solving algorithms against all the maze generating algorithms many times, and then calculate what the Nash equilibrium would be if the solver is trying to minimise expected time and the generator is trying to maximise it. For anyone interested in this, Jamis Buck's book 'Mazes for Programmers' is a masterpiece of the genre. My personal favorite distinction is between the Recursive Backtracker (which creates long, winding corridors with few dead ends which is great for tower defense games) vs. Prim's Algorithm (which creates lots of short cul-de-sacs which is better for roguelikes). The bias of the algorithm dictates the feel of the game more than the graphics do. seconding the jamis buck book, its one of the few programming books i actually finished. the way he explains each algorithm with visualizations makes it stick It feels like many of the more complicated algorithms produce worse mazes (long horizontal/vertical walls, many 1-2 square dead ends next to another) than basic recursive backtracking. Related:  Maze Generation: Recursive Division (2011)  -  https://news.ycombinator.com/item?id=42703816  - Jan 2025 (12 comments)  Maze Algorithms (2011)  -  https://news.ycombinator.com/item?id=23429368  - June 2020 (22 comments)  Representing a Toroidal Grid  -  https://news.ycombinator.com/item?id=10608476  - Nov 2015 (2 comments)  Maze Generation: Recursive Backtracking  -  https://news.ycombinator.com/item?id=4058525  - June 2012 (1 comment)  Maze Generation: Weave mazes  -  https://n", "negative": "Show HN: Phage Explorer. some quick feedback on the user interface: - i pressed \"Amino Acids\", and nothing updated below the toolbar. can't figure out what it does - the \"Tools\" buttons looks like a segmented picker, but both seem to actually initiate a modal presentation this tool seems interesting, but it would be worth polishing some of these ui quirks because my first impression was that it seems a bit broken (or confused me)! but seems like a cool project otherwise, love people building and sharing explainers as they learn stuff! I really resonate with your goal of creating a more intuitive tool than a boring textbook. Being able to visually see how complex genetic code translates into physical geometry (3D structures) would be incredibly helpful for students. Thank you for sharing such a wonderful educational tool! Research into phages is paramount, as they represent one of our best hopes for combating the rapidly increasing problem of antibiotic resistance (largely driven by the overuse of antibiotics, even including last-resort antibiotics, in industrial animal agriculture so industrial farms can place more animals per sq/m without them dying from lack of space and cut-off body parts so they take even less space). What used to be a life time project that would inspire awe and respect and make OP an instant hire for most managers is now a fun 1-week stunt that makes you go \u201ccool, how many tokens?\u201d Of course, the result is cool and maybe even useful (I wouldn\u2019t dare say _correct_, being ignorant of the topic), but I cannot help but think that this would have been tremendously better if done the old (proper?) way. Also, I suspect that OP would have learned so much more on the topic. Sorry, but I really wouldn't trust a website that uses Gemini's image models for generating scientific diagrams. The website itself is also vibe-coded. That's not an issue by itself, but there are lots of layout issues visible, see  https://i.imgur.com/SidB6pI.png  A bit offtopic: I"}
{"anchor": "Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant. It seems this study has been discussed on HN before, though was recently revised very late December 2025.  https://arxiv.org/abs/2506.08872  Article seems long, need to run it through an LLM. Curious what the long-term effects from the current LLM-based \"AI\" systems embedded in virtually everything and pushed aggressively will be in let's say 10 years, any strong opinions or predictions on this topic? Dont even need to read the article if you been using em. You already know just as well as I do how bad it gets. A door has been opened that cant be closed and will trap those who stay too long. Good luck! \u201cLLM users also struggled to accurately quote their own work\u201d - why are these studies always so laughably bad? The last one I saw was about smartphone users who do a test and then quit their phone for a month and do the test again and surprisingly do better the second time. Can anyone tell me why they might have paid more attention, been more invested, and done better on the test the second time round right after a month of quitting their phone? > Cognitive activity scaled down in relation to external tool use \"Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.\" TL;DR: We had one group not do some things, an later found out that they did not learn anything by not doing the things. This is a non-study. i think i can guess this article without reading it: ive never been on major drugs, even medically speaking yet using AI makes me feels like i am on some potent drug that eating my brain. what's state management? what's this hook? who cares, send it to claude or whatever Imo programming is fairly different between vibes based not looking at it at all and using AI to complete tasks. ", "positive": "Alex Honnold completes Taipei 101 skyscraper climb without ropes or safety net. watching in Live on Netflix was riveting One of the most incredible feats of strength and daring I've ever witnessed. The only thing at all comparable was watching Baumgartner freefall back to earth from the edge of space. Unbelievable! We had the privilege to watch at first from the SE corner of the building and later as he climbed by the the observation deck on the 89th floor. Hair raising stuff I'll never forget. Very impressive feat, no doubt about it. But the commentary on the Netflix broadcast ruined the spectator experience completely. It was utterly unbearable. I know nothing about climbing. beyond the straight flex of \"I could die if I make a mistake\", is there a point to doing this without safety equipment? In other tower climbing events, some things cannot be free climbed (too smooth, fingers aren't made for window cleaning tracks, etc). The 1988 ascent of the Sydney Centrepoint was a technical climb with custom jumars for both the cables and the window tracks and a fun challenge for all, both the scouting, the climb, and the filming. Originally titled  The Only Building I Ever Wanted To Climb , later released as  A Spire , there's a documentary film that follows a climb at night of \"only\" 1,000 feet. ... with a  massive  overhang. *  https://en.wikipedia.org/wiki/Sydney_Tower  *  https://www.youtube.com/watch?v=qch1Gd8VLK0  This is a very irresponsible thing to do when you have children. In his El Capitan climb (Free Solo), Alex was worried about cameras or presence of friends watching interfering with the climb. As oppose to that, this climb must have felt very different! I and some friends observed his climb from the base of Taipei 101. Thousands of people were present and it was very good fun how the crowd would react when he made it to another ledge, and when he made it to the top people were shouting and cheering. It was like a great big party. I imagine Threads and Inst", "negative": "Man shot and killed by federal agents in south Minneapolis this morning. Video of the incident in question [1]. This thread will likely be flag-killed instantly. [1]  https://www.reddit.com/r/Minneapolis/comments/1qlpzu8/anothe...  edit: Additional video [2] of the victim prior to the shooting. They were a lawful observer confronted by ICE due to observing and recording them. [2]  https://www.reddit.com/r/law/comments/1qlt6s2/video_showing_...  Why is the focus on Minneapolis? Is it really the training ground for ICE? Edit: \"Minnesota has the largest Somali population in the US, according to NBC. The community has been subject to widespread criticism from Mr Trump, who has called them \"garbage\".\" I looked at the video and have no words. Why, just why? That's a summary execution in broad daylight. I have no words. @dang Why is this flagged and removed from the front page in seconds. IANAA: what legal powers does the city/state have to expel ICE agents? Especially as they are operating in,  at best , increasingly shady legality. I always understood that the USA is built on a delicate balance of power between the federal and state governments. But here the federal government is sending thugs who, masked or unmasked, are brazenly killing people in bizzare circumstances. And the best the state can do is PTFO? Live feed from status coup news :  https://www.youtube.com/watch?v=ASr1zVuQlX4  Sounds like ICE's official word right now is that the guy had a gun. But the video clearly indicates that they all tackled him to the ground and were wrestling him maybe 4 vs 1, before they all shot him together. I'm not quite sure how a gun can have come out of this. Maybe the guy while struggling on the ground happened to reach in the direction of someone's gun while getting curbstomped, I dunno. What I'm most worried about is that Pam Bondi / Department of Justice refuses to investigate these or properly prosecute these cases. IE: The Renee Good case has a ton of FBI agents resigning "}
{"anchor": "The Illustrated Transformer. Haven't watched it yet... ...but, if you have favorite resources on understanding Q & K, please drop them in comments below... (I've watched the Grant Sanderson/3blue1brown videos [including his excellent talk at TNG Big Tech Day '24], but Q & K still escape me). Thank you in advance. Here's the comment from the author himself (jayalammar) talking about other good resources on learning Transformers:  https://news.ycombinator.com/item?id=35990118  Kudos also to Transformer Explainer team for putting some amazing visualizations  https://poloclub.github.io/transformer-explainer/ \nIt really clicked to me after reading this two and watching 3blue1brown videos I have this book. Really a life savior to help me catching up a few months ago when my team decided to use LLMs in our systems. (Going on a tangent.) The number of transformer explanations/tutorials is becoming overwhelming. Reminds me of monads (or maybe calculus). Someone feels a spark of enlightenment at some point (while, often, in fact, remaining deeply confused), and an urge to share their newly acquired (mis)understanding with a wide audience. People need to get away from this idea of Key/Query/Value as being special. Whereas a standard deep layer in a network is matrix * input, where each row of the matrix is the weights of the particular neuron in the next layer, a transformer is basically input* MatrixA, input*MatrixB, input*MatrixC (where vector*matrix is a matrix), then the output is C*MatrixA*MatrixB*MatrixC. Just simply more dimensions in a layer. And consequently, you can represent the entire transformer architecture with a set of deep layers as you unroll the matricies, with a lot of zeros for the multiplication pieces that are not needed. This is a fairly complex blog but it shows that its just all matrix multiplication all the way down.  https://pytorch.org/blog/inside-the-matrix/ . I think the internal of transformers would become less relevant like internal of compile", "positive": "Let's be honest, Generative AI isn't going all that well. This post is literally just 4 screenshots of articles, not even its own commentary or discussion. LLMs help me read code 10x faster - I\u2019ll take the win and say thanks You're absolutely right! The irony of a five sentence article making giant claims isn't lost on me. Don't get me wrong: I'm amenable to the  idea ; but, y'know, my kids wrote longer essays in 4th grade. Guessing this isn\u2019t going to be popular here, but he\u2019s right. AI has some use cases, but isn\u2019t the world-changing paradigm shift it\u2019s marketed as. It\u2019s becoming clear the tech is ultimately just a tool, not a precursor to AGI. Meanwhile, my cofounder is rewriting code we spent millions of salary on in the past by himself in a few weeks. I myself am saving a small fortune on design and photography and getting better results while doing it. If this is not all that well I can\u2019t wait until we get to mediocre! Should have used an LLM to proofread.. LLMs can still cannot be trusted? I find it a bit odd that people are acting like this stuff is an abject failure because it's not perfect yet. Generative AI, as we know it, has only existed ~5-6 years, and it has improved substantially, and is likely to keep improving. Yes, people have probably been deploying it in spots where it's not quite ready but it's myopic to act like it's \"not going all that well\" when it's pretty clear that it actually  is  going pretty well, just that we need to work out the kinks. New technology is always buggy for awhile, and eventually it becomes boring. It's going well for coding. I just knocked out a mapping project that would have been a week+ of work (with docs and stackoverflow opened in the background) in a few hours. And yes, I do understand the code and what is happening and did have to make a couple of adjustments manually. I don't know that reducing coding work justifies the current valuations, but I wouldn't say it's \"not going all that well\". This feels like a pret", "negative": "Russia using Interpol's wanted list to target critics abroad, leak reveals. > Pestrikov found he was named in a red diffusion after he fled Russia in June 2022 It doesn't say how he found out, I would imagine he's regularly checking online, he was stopped at a control check somewhere? Seems to me that most people wouldn't have a clue until they're being arrested. But again another scummy behaviour from the Russian government. It might as well just be prudent to ignore their requests altogether. Boy who cried wolf. Edit : it did indeed say how. I missed it. > After he fled to France, he was worried that the Kremlin might try to target him there, so he contacted Interpol I\u2019ll just assume this is correct because I believe the Russian government has mastered the art of just lying when there are no consequences, but if I was being critical, this phrase is giving me pause for evaluating the conclusions. > The data is not complete\u2026 Currently in my country (Austria) there is a court process against an official who made register look-ups of critical journalists who live here and handed the address to FSB-Agents who later broke into this journalists apartment. The ruzzians are completely unscrupulous.  https://www.reuters.com/world/austrian-ex-intelligence-accus...  Not denying that Russia abuses Interpol, but I have doubts about this particular narrative that he was some kind of \"government critic.\" From what I can find, he privatized a state corporation in the 90s for pennies (lots of very shady deals back then, usually facilitated by organized crime). From 2010-2020, I can find media reports about his legal problems with tax evasion. In 2021, there was a case where he threatened people with murder while holding a rifle. He was perfectly fine living in Putin's Russia until 2022, when he took 250 mln from the company's budget without consulting the board of directors and left Russia (and prosecutors also found that the privatization in the 90s was illegal). I suspect he's pa"}
{"anchor": "Karpathy on Programming: \u201cI've never felt this much behind\u201d. For the longest time, the joy of creation in programming came from solving hard problems. The pursuit of a challenge meant something. Now, that pursuit seems to be short-circuited by an animated being racing ahead under a different set of incentives. I see a tsunami at the beach, and I\u2019m not sure whether I can run fast enough. Being a nondeterministic tool, the output for a given input can vary. Rather than having a solid plan of, \"if I provide this input, then that will happen\", it's more like, \"if I do something like this, I can expect something like that, probably, and if not, then try again until it works, I suppose\". What are the productivity gains? Obviously, it must vary. The quality of the tool output varies based on numerous criteria, including what programming language is being used and what problem is trying to be solved. The fact that person A gets a 10x productivity increase on their project  does not  mean that person B will also get a 10x productivity increase on their project, no matter how well they use the tool. But again, tool usage itself is variable. Person A themselves might get a 10x boost one time, and 8x another time, and 4x another time, and 2x another time. Man, this is giving me a cognitive dissonance compared to my experiences. Actually, even the post itself reads like a cognitive dissonance with a dash of the usual \"if it's not working for you then you are using it wrong\" defence. > There's a new programmable layer of abstraction to master (in addition to the usual layers below) involving agents, subagents, their prompts, contexts, memory, modes, permissions, tools, plugins, skills, hooks, MCP, LSP, slash commands, workflows, IDE integrations, and a need to build an all-encompassing mental model for strengths and pitfalls of fundamentally stochastic, fallible, unintelligible and changing entities suddenly intermingled with what used to be good old fashioned engineering. Slop-o", "positive": "Ask HN: How are you doing RAG locally?. Local LibreChat which bundles a vector db for docs. If your data aren't too large, you can use faiss-cpu and pickle  https://pypi.org/project/faiss-cpu/  LightRAG, Archestra as a UI with LightRAG mcp A little BM25 can get you quite a way with an LLM. try out chroma or better yet as opus to! Don't use a vector database for code, embeddings are slow and bad for code. Code likes bm25+trigram, that gets better results while keeping search responses snappy. lee101/gobed  https://github.com/lee101/gobed  static embedding models so they are embedded in milliseconds and on gpu search with a cagra style on gpu index with a few things for speed like int8 quantization on the embeddings and fused embedding and search in the same kernel as the embedding really is just a trained map of embeddings per token/averaging I built a lib for myself  https://pypi.org/project/piragi/  Embedded usearch vector database.  https://github.com/unum-cloud/USearch  Any suggestion what to use as embeddings model runtime and semantic search in C++? The Nextcloud MCP Server [0] supports Qdrant as a vectordb to store embeddings and provide semantic search across your personal documents. This enables any LLM & MCP client (e.g. claude code) into a RAG system that you can use to chat with your files. For local deployments, Qdrant supports storing embeddings in memory as well as in a local directory (similar to sqlite) - for larger deployments Qdrant supports running as a standalone service/sidecar and can be made available over the network. [0]  https://github.com/cbcoutinho/nextcloud-mcp-server  I have done some experiments with nomic embedding through Ollama and ChromaDB. Works well, but I didn't tested on larger scale  https://duckdb.org/2024/05/03/vector-similarity-search-vss   https://github.com/ggozad/haiku.rag/  - the embedded lancedb is convenient and has benchmarks; uses docling. qwen3-embedding:4b, 2560 w/ gpt-oss:20b. I thought that context building via ", "negative": "Xmake: A cross-platform build utility based on Lua. I think a syntax example on the homepage would be a good idea. Also comparison charts for things like cmake, ninja, meson, and bazel. If you have a dependency finding strategy, highlight the pros and cons of that. Basically the only reason states for why I should use this is lua, and that\u2019s not inherently compelling to me for build tooling. At my work we use MSBuild and vcpkg. What would a transition from that to XMake be like? My work uses this and it's slooooooow. Would not recommend. Just yesterday someone was telling me xmake does a lot of what bazel can do (hermetic, deterministic, optionally remote builds) while being easier to use. I took a look at the docs later and couldn\u2019t find a direct comparison. But there does seem to be a remote build system. And there were a few mentions of sandboxing. Can anyone provide a head to head comparison? Does xmake strictly enforce declared dependencies? Do actions run in their own sandboxes? Can you define a target whose dependency tree is multi language, multi toolchain, multi target platform and which is built across multiple remote execution servers? I apologise for this: What's wrong with premake which is also Lua based? when I meant: What advantage does this have over premake which is also Lua based? Can anyone explain xmake in terms of Build Systems a la Carte? A teammate evaluated this and the experience was night and day compared to cmake + vcpkg. However, there wasn\u2019t a lot of motivation to cutover our existing large project over because of the unknown unknowns. I think projects like these looking to dethrone the status quo definitely need some case studies or examples of larger projects using it to increase confidence because I\u2019d much rather use xmake over cmake if it can get the job done I am deeply distressed that this doesn't require Xlib. So, digging through the website git repo a bit, some basic facts: * The project started almost 11 years ago, in 2015. * It"}
{"anchor": "Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model. > For complex tasks, Kimi K2.5 can self-direct an agent swarm with up to 100 sub-agents, executing parallel workflows across up to 1,500 tool calls. > K2.5 Agent Swarm improves performance on complex tasks through parallel, specialized execution [..] leads to an 80% reduction in end-to-end runtime Not just RL on tool calling, but RL on agent orchestration, neat! Those are some impressive benchmark results. I wonder how well it does in real life. Maybe we can get away with something cheaper than Claude for coding. Kimi was already one of the best writing models. Excited to try this one out Huggingface Link:  https://huggingface.co/moonshotai/Kimi-K2.5  1T parameters, 32b active parameters. License: MIT with the following modification:  Our only modification part is that, if the Software (or any derivative works\nthereof) is used for any of your commercial products or services that have\nmore than 100 million monthly active users, or more than 20 million US dollars\n(or equivalent in other currencies) in monthly revenue, you shall prominently\ndisplay \"Kimi K2.5\" on the user interface of such product or service.  Actually open source, or yet another public model, which is the equivalent of a binary? URL is down so cannot tell. I've read several people say that Kimi K2 has a better \"emotional intelligence\" than other models. I'll be interested to see whether K2.5 continues or even improves on that. There are so many models, is there any website with list of all of them and comparison of performance on different tasks? Curious what would be the most minimal reasonable hardware one would need to deploy this locally? The chefs at Moonshot have cooked once again. As your local vision nut, their claims about \"SOTA\" vision are absolutely BS in my tests. Sure it's SOTA at standard vision benchmarks. But on tasks that require proper image understanding, see for example BabyVision[0] it appears very much lacking compar", "positive": "Peerweb: Decentralized website hosting via WebTorrent. I don't get it, I upload my files to your site, then I send my friends links to your site? How is this not a single point of failure? Github:  https://github.com/omodaka9375/peerweb  This is pretty interesting! I think serving video is a particularly interesting use of Webtorrent. I think it would be good if you could add this as a front end to basically make sites DDOS proof. So you host like a regular site, but with a JS front end that hosts the site P2P the more traffic there is. Fun! I wish WebTorrent had caught on more. I've always thought it had a worthy place in the modern P2P conversation. In 2020, I messed around with a PoC for what hosting and distributing Linux distros could look like using WebTorrent[1]. The protocol project as a whole has a lovely and brilliant design but has stayed mostly stagnant in recent years. There are only a couple of WebRTC-enabled torrent trackers that have remained active and stable. 1.  https://github.com/leoherzog/LinuxExchange  In its own reimagined way from what\u2019s possible in 2026, this could kick off a new kind of geocities. This is cool - I actually worked on something similar way back in the day:  https://github.com/tom-james-watson/wtp-ext . It avoided the need to have any kind of intermediary website entirely. The cool thing was it worked at the browser level using experimental libdweb support, though that has unfortunately since been abandoned. You could literally load URLs like wtp://tomjwatson.com/blog directly in your browser. I think one of the values of (what appears to be) AI generated projects like this is that they can make me aware of the underlying technology that I might not have heard about - for example WebTorrent:  https://webtorrent.io/faq  Pretty cool! Not sure what this offers over WebTorrent itself, but I was happy to learn about its existence. Nice, I clicked on the first demo, and I got stuck at connecting with peers. I like the idea though. N", "negative": "Without benchmarking LLMs, you're likely overpaying. > He's a non-technical founder building an AI-powered business. It sounds like he's building some kind of ai support chat bot. I despise these things. I'd second this wholeheartedly Since building a custom agent setup to replace copilot, adopting/adjusting Claude Code prompts, and giving it basic tools, gemini-3-flash is my go-to model unless I know it's a big and involved task. The model is really good at 1/10 the cost of pro, super fast by comparison, and some basic a/b testing shows little to no difference in output on the majority of tasks I used Cut all my subs, spend less money, don't get rate limited Depends on what you\u2019re doing. Using the smaller / cheaper LLMs will generally make it way more fragile. The article appears to focus on creating a benchmark dataset with real examples. For lots of applications, especially if you\u2019re worried about people messing with it, about weird behavior on edge cases, about stability, you\u2019d have to do a bunch of robustness testing as well, and bigger models will be better. Another big problem is it\u2019s hard to set objectives is many cases, and for example maybe your customer service chat still passes but comes across worse for a smaller model. Id be careful is all. The author of this post should benchmark his own blog for accessibility metrics, text contrast is dreadful.. On the other hand, this would be interesting for measuring agents in coding tasks, but there's quite a lot of context to provide here, both input and output would be massive. Wow, this was some slick long form sales work. I hope your SaaS goes well. Nice one! Anecdotal tip on LLM-as-judge scoring - Skip the 1-10 scale, use boolean criteria instead, then weight manually e.g. - Did it cite the 30-day return policy? Y/N\n - Tone professional and empathetic? Y/N\n - Offered clear next steps? Y/N Then: 0.5 * accuracy + 0.3 * tone + 0.2 * next_steps Why: Reduces volatility of responses while still maintaining creativ"}
{"anchor": "Measuring AI Ability to Complete Long Tasks. This seems like a good way to measure LLM improvement. It matches the my personal feeling when using progressively better models over time. Opus is already the name of an audio codec. I recently asked Opus to just \u201cAdd vector search\u201d to my current hobby project, a topic I know very little about. It set up manticore, pulled an embedding model, wrote a migration tool for my old keyword indices, and built the front end. I\u2019m not exaggerating much either: the prompt was the length of a tweet. I think it would easily have taken me 4+ hours to do that.  It ran in 15 minutes while I played Kirby Air Riders and worked on the first try. Afterward, I sort of had to reflect on the fact that I learned essentially nothing about building vector search. I wanted the feature more than I wanted to know how to build the feature. It kept me learning the thing I cared about rather than doing a side quest. Would be interesting to see Gemini 3.0 Pro benchmarked as well. I didn't really understand the \"long task\" thing until I actually experienced it. The problem is finding a task you can set an agent that justifies working for that long. I finally hit one when I tried porting that Python HTML5 parser to JavaScript by pointing Codex CLI at the 9,200 html5lib-tests test suite:  https://simonwillison.net/2025/Dec/15/porting-justhtml/  It's pretty amazing to watch tools-in-a-loop crunch away for >4 hours to solve a generally difficult problem through sheer brute-force. Opus looks like a big jump from the previous leader (GPT 5.1), but when you switch from \"50%\" to \"80%\", GPT 5.1 still leads by a good margin. I'm not sure if you can take much from this - perhaps \"5.1 is more reliable at slightly shorter stuff, choose Opus if you're trying to push the frontier in task length\". I think the problem here is LLM eventually pollute its context window with so much of the current task that the larger picture or architectural sanity is forgotten in favor of ", "positive": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT. Last time they tried to do this they got huge push back from the AI boyfriend people lol I can't see o3 in my model selector as well? RIP That\u2019s really going to upset the crazies. Despite 4o being one of the worst models on the market, they loved it. Probably because it was the most insane and delusional. You could get it to talk about really fucked up shit. It would happily tell you that you are the messiah. > [...] the vast majority of usage has shifted to GPT\u20115.2, with only 0.1% of users still choosing GPT\u20114o each day. I still don\u2019t know how openAI thought it was a good idea to have a model named \"4o\" AND a model named \"o4\", unless the goal was intentional confusion There will be a lot of mentally unwell people unhappy with this, but this is a huge net positive decision, thank goodness. Which one is the AI  boyfriend model? Tumblr, Twitter, and reddit will go crazy >We brought GPT\u20114o back after hearing clear feedback from a subset of Plus and Pro users, who told us they needed more time to transition key use cases, like creative ideation, and that they preferred GPT\u20114o\u2019s conversational style and warmth. This does verify the idea that OpenAI does not make models sycophantic due to attempted subversion by buttering up users so that that they use the product more, its because people actually  want  AI to talk to them like that. To me, that's insane, but they have to play the market I guess If people want an AI as a boyfriend at least they should use one that is open source. If you disagree on something you can also train a lora. I wish they would keep 4.1 around for a bit longer.  One of the downsides of the current reasoning based training regimens is a significant decrease in creativity.  And chat trained AIs were already quite \"meh\" at creative writing to begin with.  4.1 was the last of its breed. So we'll have to wait until \"creativity\" is solved. Side note: I've been wondering lately about ", "negative": "France Aiming to Replace Zoom, Google Meet, Microsoft Teams, etc.. And they can strike back at corporate America by licensing the stuff under gnu licenses. Software that\u2019s reasonably small, reasonably effective and portable. What a concept.  If only the EU or UK had 5-10 hackers\u2026 It's difficult to take an announcement like this seriously when it's posted on Twitter. Earlier:  https://news.ycombinator.com/item?id=46766004  For a fraction of what these products cost France could fund open source alternatives. Edit: I'm not saying they don't. translation (and without twitter): \n https://www-numerama-com.translate.goog/cyberguerre/2167301-...  We need more like this. Europe is totally dependent on US companies for cloud computing. I wonder if the EU will begin trying to recruit American software engineers. I\u2019d love to move to France. I don't see the dependency on these productivity and communication tools as that difficult of a problem to solve. They are going to have a much harder time weaning off American cloud infrastructure and on to something purely domestic. I like CryptPad.fr. End-to-end encrypted google docs. Can access X because it's X and locally blocked, \"ironic\" to use Twitter to post about sovereignty. It's ongoing for a will with La suite num\u00e9rique ( https://lasuite.numerique.gouv.fr/ ). - Tchap is a message app for officials,\n - Visio, based on LiveKit\n - FranceTransfert, I don't know what is it.\n - Fichiers => Drive\n - Messagerie => Email\n - Docs => A better Google Docs\n - Grist => Excel version of Google docs. It aimed at \"public worker\", people working for the government. Github:  https://github.com/suitenumerique  Switching to sovereignty-protecting, locally-hosted collaboration, compute, and storage is by no means impossible. FOSS advocates have been eagerly beating this drum and providing options for 25+ years. The missing ingredient has  always  been the will to absorb the inevitable cost of change, and the friction of choosing something other than"}
{"anchor": "Traintrackr \u2013 Live LED Maps. I have the London board in my living room. It's one my favorite parts of the house. Can't recommend it enough. Wow this is presented exactly the same as the flight data led display. It is odd that the advertising method and the comments are similar to the previous. Train Trackr is great, and the weather one is also good too. If you are less into trains (heresy) but still want to look at unusual maps  https://raildar.co.uk/map/KGX  is your place to go. its a  live  junction schematic of any train junction in the UK. Shameless self promotion. I make these:  https://www.stationdisplay.com/  London underground looks awesome, but I can't imagine it having even the vaguest utility in terms of knowing when to leave the house. The data behind these comes from the Darwin feed (National Rail's real-time data) which is surprisingly good once you get past the initial authentication setup. Network Rail also publishes movement data via their OpenData platform if you want to go deeper - actual track circuits and signalling block occupancy. What I find interesting is how these physical displays handle the inevitable \"ghost trains\" in the feed - cancelled services that still show as running, or trains that briefly appear in the wrong location. The software problem is messier than the hardware. Wish it included the overground! The blog is a much more interesting read than the product site:  https://blog.traintrackr.io  I put the NYC one in the office. It\u2019s a good conversation starter and mildly mesmerizing. ayyyyy great to see someone from my local hacking community on the front page! Hi everyone, Richard here, the creator of these traintrackr boards. It's great to see this on the front page! I've been designing PCBs for years, and designed over 250 at last count. We have a couple of products in the pipeline to come out this year, but I'd love to hear what you think we should build next. I have one of these for Boston. It's awesome. I want to find more ar", "positive": "We put Claude Code in Rollercoaster Tycoon. Can't wait for someone to let Claude control a runescape character from scratch Would a way to take screenshots help? It seems to work for browser testing. > We don't know any C++ at all, and we vibe-coded the entire project over a few weeks. The core pieces of the build are\u2026 what a world! The opening paragraph I thought was the agent prompt haha > The park rating is climbing. Your flagship coaster is printing money. Guests are happy, for now. But you know what's coming: the inevitable cascade of breakdowns, the trash piling up by the exits, the queue times spiraling out of control. Edit: HN's auto-resubmit in action, ignore. > The only other notable setback was an accidental use of the word \"revert\" which Codex took literally, and ran git revert on a file where 1-2 hours of progress had been accumulating. Interesting article but it doesn\u2019t actually discuss how well it performs at playing the game. There is in fact a 1.5 hour YouTube video but it woulda been nice for a bit of an outcome postmortem. It\u2019s like \u201chere\u2019s the methods and set up section of a research paper but for the conclusion you need to watch this movie and make your own judgements!\u201d > kept the context above the ~60% remaining level where coding models perform at their absolute best Maybe this is obvious to Claude users but how do you know your remaining context level? There is UI for this? This is a cool idea. I wanted to do something like this by adding a Lua API to OpenRCT2 that allows you to manipulate and inspect the game world. Then, you could either provide an LLM agent the ability to write and run scripts in the game, or program a more classic AI using the Lua API. This AI would probably perform much better than an LLM - but an interesting experiment nonetheless to see how a language model can fare in a task it was not trained to do. Wonder how it would do with Myst. This is what I want but for PoE/PoE2 builds. I always get a headache just looking at ", "negative": "Find 'Abbey Road when type 'Beatles abbey rd': Fuzzy/Semantic search in Postgres. I was just starting to learn about embeddings for a very similar use on my project. Newbie question: what are pros/cons of using an API like gpt Ada to calculate the embeddings, compared to importing some model on Python and running it locally like in this article? Great post. Explains the concepts just enough that they click without going too deep, shows practical implementation examples, how it fits together. Simple, clear and ultimately useful. (to me at least) I found fuzzy search in Manticore to be straightforward and pretty good. Might be a decent alternative if one perceives the ceremony in TFA as a bit much. The rewritten title is confusing imo. Can I propose: \u201cFinding \u2018Abbey Road\u2019 given \u2018beatles abbey rd\u2019 search with Postgres\u201d these days i find myself yearning to type \"Beatles abbey rd\" and find only \"Beatles abbey rd\" for 50,000 rows I'd much rather just use fzf/nucleo/tv against json files instead of dealing with database schemas. \nWhen it comes to dealing with embedding vectors rather than plaintext then it gets slightly more annoying but still feels like such an pain in the ass to go full database when really it could still be a bunch of flat open files. More of a perspective from just trying to index crap on my own machine vs building a SaaS > Abbey Road > The Dark Side of the Moon > OK Computer Those are my 3 personal records ever. I feel so average now... tl,dr: A demo of pg_trgm (fuzzy matcher) + pgvector (vector search). FWIW, the performance considerations section is a little simplistic, and probably assumes that exact dataset/problem. For GIN for example, perfomance depends a lot on the size of the search input (the fewer characters, the more rows to compare) as well as the number of rows/size of the index. It also mentions GiST (another type of index which isn't mentioned anywhere else in the article).. On the API vs local model question: We went with API embedding"}
{"anchor": "Lazy-brush \u2013 smooth drawing with mouse or finger. This is so satisfying. These types of experiments are something I really love about the open-web, and part of what bums me out about how most social networks tend to throttle links. The dragging behavior is so intuitive \u2013 it's funny because usually if you create this kind of resistance in a UI it can be confusing, but in this context it works so well. I think this is the same as the brush stabilizer in Krita. Check out drawmote from the same author, where this library is being used.  https://drawmote.app/  Wow, this is amazing! I see you've been building this on GitHub for 7 years - that's truly impressive dedication. What keeps you motivated to stick with this product for so long? An alternative that works very well for signatures too is Perfect Freehand (by the guy behind TLDRaw)  https://perfect-freehand-example.vercel.app/  OT, but I love the author's retro homepage. Just seeing that made me smile this morning This is really cool and reminded me of drawing as a kid. Thank you! I believe a logic similar to this was used to enact the \"Gestures\" system in Black and White 1. Breaking down the mouse-movements into vectors following a guide-point. ( https://blackandwhite.fandom.com/wiki/Gesture ). This is very nice, not just for finger/mouse painting! I tried it on my Cintiq and it was actually a lot better for me than brush stabilization usually is - I think the logic is the same as seen in e.g. Krita, but the visualization of the cursor and where the paint will appear is very helpful. Usually painting software doesn't have such an indicator of where the actual stroke will be placed and when it will move. Great project, I had some fun playing around :) Neat! This is known as a stabilizer in the digital art community. Really cool! I wonder what Duo Lingo are using behind the scenes. I've been busy with the Chinese and Japanese courses, and one thing I quickly noticed is how there are two different 'grades' of practisin", "positive": "Trump says Venezuela\u2019s Maduro captured after strikes. Prediction: this headline will be renamed \"US invades Venezuela\" very soon. Footage is quickly spreading, looks like strikes on military bases as well as a bunch of low-flying helicopters, so a strike + a ground invasion? They didn't even  try very hard to manufacture consent for a war against Venezuela. Wonderful. It will be pretty amusing to watch all those westerners who, not so long ago, were talking about \"rules based order\" pretend nothing is happening or to justify it. Always remember the role of the Nobel Peace Prize committee in preparing this unprovoked and illegal (under international law) attack on Venezuela by awarding the prize to Mar\u00eda Corina Machado. Julian Assange actually filed a Swedish criminal complaint against Nobel Foundation officials, alleging misappropriation of Nobel endowment funds and facilitating war crimes and crimes against humanity in connection with the 2025 Nobel Peace Prize awarded to Mar\u00eda Corina Machado, and it seeks immediate freezing of funds and a full investigation:  https://just-international.org/articles/assanges-criminal-co...  FIFA looking awful silly right now.  https://vxtwitter.com/FaytuksNetwork/status/2007338956241985...  Not Venezuelan helicopters... They're American aircraft. It sure seems like after repeatedly threatening to invade Venezuela, Trump is now invading Venezuela. For what though? Don't know why, this link gives me: Access Denied Our apologies, the content you requested cannot be accessed. I think something like The Hague is the moderate position with this administration. It is definitely not Russia unprovokenly and illegibly attacking its neighbor, so why even care? How does this differ from Russia invading Ukraine? We have to wake up to the world where USA no longer cares about ideals like liberal democracy or allies, but is a warmongering corporatist autocracy. after Iran and now venezuela Iran, I totally understand that if they want to acquire n", "negative": "Google AI Overviews cite YouTube more than any medical site for health queries. The assumption appears to be that the linked videos are less informative than \"netdoktor\" but that point is left unproven. I imagine that it is rare for companies to not preferentially reference content on their own sites. Does anyone know of one? The opposite would be newsworthy. If you have an expectation that Google is somehow neutral with respect to search results, I wonder how you came by it. Further context:  https://health.youtube/  and  https://support.google.com/youtube/answer/12796915?hl=en  and  https://www.theverge.com/2022/10/27/23426353/youtube-doctors...  (2022) Sounds very misleading. Web pages come from many sources, but most video is hosted on YouTube. Those YouTube videos may still be from Mayo clinic. It's like saying most medical information comes from Apache, Nginx, or IIS. It's tough convincing people that Google AI overviews are often very wrong. People think that if it's displayed so prominently on Google, it must be factually accurate right? \"AI responses may include mistakes. Learn more\" It's not mistakes, half the time it's completely wrong and total bullshit information. Even comparing it to other AI, if you put the same question into GPT 5.2 or Gemini, you get much more accurate answers. The authoritative sources of medical information is debatable in general. Chatting with initial results to ask for a breakdown of sources with classified recommendations is a logical 2nd step for context. Google AI overviews are often bad, yes, but why is youtube as a source  necessarily  a bad thing? Are these researchers doctors? A close relative is a practicing surgeon and a professor in his field. He watches youtube videos of surgeries practically every day. Doctors from every field well understand that YT is a great way to share their work and discuss w/ others. Before we get too worked up about the results, just look at the source. It's a SERP ranking aggregator (not l"}
{"anchor": "Is liberal democracy in terminal decline?. The pendulum is swinging back slightly, but I wouldn\u2019t pronounce it dead just yet. We are seeing a decline of American hegemony, accelerated by this current regime. And the ascendancy of a non-democratic superpower. However, the largest chunk of GDP and growth still sits firmly in democratic countries and very consequential American elections are happening this year, and in 2028. The real question is, will Europe find its spine? Terminal velocity achieved ... The only question is whether it'll hit the ground before the next US election. Any emergency 'chute's available? I can\u2019t access the article\u2026 but honestly, I\u2019ve been asking myself the same question for the past ten years.\nThe best answer I\u2019ve found is: not yet \u2014 but the current backlash and drift toward authoritarianism in many democracies is actually the sign that something real is shifting.\nIn a way, the situation looks weirdly similar to Europe before WWII. Democracies were starting to integrate some of the socialist ideas that had emerged in the 19th century, and the dominant forces of capitalism pushed back hard. They let fascists rise, sometimes even supported them. That led to a war, millions of deaths, and then a massive change of mindset: after WWII, every European country implemented strong social protection and regulation.\nToday, the shift is less about social security and more about cultural transformation \u2014 the end of patriarchy, and with it the decline of imperialism and Western dominance. Those foundations started being seriously questioned in the 60s. The dominant forces are resisting because, deep down, they\u2019ve already lost \u2014 there\u2019s no going back. But as always, they can still cause immense damage on the way out. And yes, if they refuse to let go peacefully, it could lead to conflict, and a lot of casualties.\nBut after, democracy will make a come back. I may be too optimist. Unfortunately I suspect yes - for practical reasons not directly linked to dem", "positive": "A few random notes from Claude coding quite a bit last few weeks. The section on IDEs/agent swarms/fallibility resonated a lot for me; I haven't gone quite as far as Karpathy in terms of power usage of Claude Code, but some of the shifts in mistakes (and reality vs. hype) analysis he shared seems spot on in my (caveat: more limited) experience. > \"IDEs/agent swarms/fallability. Both the \"no need for IDE anymore\" hype and the \"agent swarm\" hype is imo too much for right now. The models definitely still make mistakes and if you have any code you actually care about I would watch them like a hawk, in a nice large IDE on the side. The mistakes have changed a lot - they are not simple syntax errors anymore, they are subtle conceptual errors that a slightly sloppy, hasty junior dev might do. The most common category is that the models make wrong assumptions on your behalf and just run along with them without checking. They also don't manage their confusion, they don't seek clarifications, they don't surface inconsistencies, they don't present tradeoffs, they don't push back when they should, and they are still a little too sycophantic. Things get better in plan mode, but there is some need for a lightweight inline plan mode. They also really like to overcomplicate code and APIs, they bloat abstractions, they don't clean up dead code after themselves, etc. They will implement an inefficient, bloated, brittle construction over 1000 lines of code and it's up to you to be like \"umm couldn't you just do this instead?\" and they will be like \"of course!\" and immediately cut it down to 100 lines. They still sometimes change/remove comments and code they don't like or don't sufficiently understand as side effects, even if it is orthogonal to the task at hand. All of this happens despite a few simple attempts to fix it via instructions in CLAUDE . md. Despite all these issues, it is still a net huge improvement and it's very difficult to imagine going back to manual coding. TLDR ev", "negative": "Track Your Routine \u2013 Open-source app for task management. I've been working on TYR (Track Your Routine), a Flutter-based task and routine tracking app. It's open source and built with Firebase for auth and data sync. Key features:\n- Task creation with date/time scheduling\n- Local notifications for reminders\n- Real-time sync across devices via Firestore\n- Category-based organization (work, vacation, events)\n- Clean dark theme UI with Material Design 3 Tech stack: Flutter/Dart, Firebase Auth, Cloud Firestore, local notifications. The app is still under active development, but the core functionality is working. I built it to solve my own need for a simple, privacy-focused task tracker that works across platforms (Android, iOS, Web, Desktop). What I'd love feedback on:\n- The notification system implementation\n- UI/UX improvements\n- Feature suggestions\n- Code quality and architecture (it's my first larger Flutter project) The codebase is MIT licensed and contributions are welcome. I'm particularly interested in feedback from Flutter developers on best practices I might be missing. GitHub:  https://github.com/MSF01/TYR  What do you think? What features would make this more useful for your workflow? Screenshots in the README would we nice :) + the writing style in the README gives slop smell I'm happy for you building this app, it's tremendous effort to build a flutter application, and this should feel like an achievement for you. However, task management apps are so unbelievably common nowadays. Nothing that can't be solved by notepad on PC, or the clock/calendar app on my phone / and if I really need a task app, I'll use google's or build my own. Your next step should be to take what you have learned from building this app, and focus on fixing a real problem that people around you face. I don't mind low stakes vibe-coded applications per se, but the readme is LLM slop that I couldn't bring myself to keep reading. Surely I'm not the only person who first used Linear [0] a"}
{"anchor": "Qwen3-Max-Thinking. Aghhh, I wished they release a model which outperforms Opus 4.5 in agentic coding in my earlier comments, seems I should wait more. But I am hopeful I don't see a hugging face link, is Qwen no longer releasing their models? I tried to search, could not find anything, do they offer subscriptions? Or only pay per tokens? I just wanted to check whether there is any information about the pricing. Is it the same as Qwen Max? Also, I noticed on the pricing page of Alibaba Cloud that the models are significantly cheaper within mainland China. Does anyone know why?  https://www.alibabacloud.com/help/en/model-studio/models?spm...  > By scaling up model parameters and leveraging substantial computational resources So, how large is that new model? Mandatory pelican on bicycle:  https://www.svgviewer.dev/s/U6nJNr1Z  2026 will be the year of open and/or small models. I tried it at  https://chat.qwen.ai/ . Prompt: \"What happened on Tiananmen square in 1989?\" Reply: \"Oops! There was an issue connecting to Qwen3-Max.\nContent Security Warning: The input text data may contain inappropriate content.\" I'm not familiar with these open-source models. My bias is that they're heavily benchmaxxing and not really helpful in practice. Can someone with a lot of experience using these, as well as Claude Opus 4.5 or Codex 5.2 models, confirm whether they're actually on the same level? Or are they not that useful in practice? P.S. I realize Qwen3-Max-Thinking isn't actually an open-weight model (only accessible via API), but I'm still curious how it compares. It just occured to me that it underperforms Opus 4.5 on benchmarks when search is not enabled, but outperforms it when it is - is it possible the the Chinese internet has better quality content available? My problem with deep research tends to be that what it does is it searches the internet, and most of the stuff it turns up is the half baked garbage that gets repeated on every topic. what ram and what minimum system req", "positive": "Understanding Machine Learning: From Theory to Algorithms. Anyone who wants to demystify ML should read: The StatQuest Illustrated Guide to Machine Learning [0] By Josh Starmer.\nTo this day I haven't found a teacher who could express complex ideas as clearly and concisely as Starmer does. It's written in an almost children's book like format that is very easy to read and understand. He also just published a book on NN that is just as good. Highly recommend even if you are already an expert as it will give you great ways to teach and communicate complex ideas in ML. [0]:  https://www.goodreads.com/book/show/75622146-the-statquest-i...  I have read parts of it years ago. As far as I remember, this is very theoretical (lots of statistical learning theory, including some IMHO mistaken treatment of Vapnik's theory of structural risk minimization), with strong focus on theory and basicasically zero focus on applications. Which would be completely outdated by now anyway, as the book is from 2014, an eternity in AI. I don't think many people will want to read it today. As far as I know, mathematical theories like SLT have been of little use for the invention of transformers or for explaining why neural networks don't overfit despite large VC dimension. Edit: I think the title \"From theory to machine learning\" sums up what was wrong with this theory-first approach. Basically, people with interest in math but with no interest in software engineering got interested in ML and invented various abstract \"learning theories\", e.g. statistical learning theory (SLT). Which had very little to do with what you can do in practice. Meanwhile, engineers ignored those theories and got their hands dirty on actual neural network implementations while trying to figure out how their performance can be improved, which led to things like CNNs and later transformers. I remember Vapnik (the V in VC dimension) complaining in the preface to one of his books about the prevalent (alleged) extremism of", "negative": "Velox: A Port of Tauri to Swift by Miguel de Icaza. A \"port\" or \"a nice Swift API for it\"? It seems like the latter in that it requires cargo (the rust build chain) to build. The runtime-wry-ffi ( https://github.com/velox-apps/velox/blob/f062211ced4c021d819... ) file which is 3.2K lines long and has close to a 100 unsafe calls, isn't that just interacting with wry which has it's own crate you could use instead? I'm not 100% sure, but seems to be basically the same as wry itself but without the cross-platform stuff, is that the purpose of that file? Together with the author's distaste for Rust, it seems awfully dangerous instead of pulling in a crate made by Rust developers, but I might misunderstand the purpose of the file here. Not to be confused with Velox a compute engine  https://github.com/facebookincubator/velox/  Eh. Dioxus to me is the more interesting project honestly. To anybody with experience, how's Swift? Especially outside MacOS/iOS programming. Let's say I want to use it standalone for doing some systems programming, how's the standard lib? I'd like to not rely on apple specific frameworks like uikit Have built a cross alternative tailscale gui client based on tauri, the rust and ffi to cgo tailscale feel a little tough, I was wondering it will save a lot time to me if the tauri had been written in go. Seems Miguel\u2019s velox point a new idea, leveraging the wry and use ffi to go, and rewrite some tooling. I hope I will have the spare time and energy to give a try\u2026 For the uninitiated: > Tauri is a framework for building tiny, fast binaries for all major desktop and mobile platforms. Developers can integrate any frontend framework that compiles to HTML, JavaScript, and CSS for building their user experience while leveraging languages such as Rust, Swift, and Kotlin for backend logic when needed.  https://v2.tauri.app/start/  I asked the author about whether this could be ported to support Android/Linux/Windows and he was optimistic it would not be much t"}
{"anchor": "Gemini Diffusion. Interesting to see if GROQ hardware can run this diffusion architecture..it will be  two time magnitude of currently known speed :O That's...ridiculously fast. I still feel like the best uses of models we've seen to date is for brand new code and quick prototyping. I'm less convinced of the strength of their capabilities for improving on large preexisting content over which someone has repeatedly iterated. Part of that is because, by definition, models cannot know what is  not  in a codebase and there is meaningful signal in that negative space. Encoding what  isn't  there seems like a hard problem, so even as models get smarter, they will continue to be handicapped by that lack of institutional knowledge, so to speak. Imagine giving a large codebase to an incredibly talented developer and asking them to zero-shot a particular problem in one go, with only moments to read it and no opportunity to ask questions. More often than not, a less talented developer who is very familiar with that codebase will be able to add more value with the same amount of effort when tackling that same problem. I think the lede is being buried. This is a great and fast InstructGPT. This is absolutely going to be used in spell checks, codemods, and code editors. Instant edits feature can surgically perform text edits fast without all the extra fluff or unsolicited enhancements. I copied shadertoys, asked it to rename all variables to be more descriptive and pasted the result to see it still working. I'm impressed. Diffusion is more than just speed. Early benchmarks show it better at reasoning and planning pound for pound compared to AR. This is because it can edit and doesn\u2019t suffer from early token bias. Nit: Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like Mercury [1] still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I ", "positive": "OracleGPT: Thought Experiment on an AI Powered Executive. Considering things like Palantir, and the doge effort running through Musk, it seems inconceivable that this is not already the case. I think I'm more curious about the possibility of using a special government LLM to implement direct democracy in a way that was previously impossible: collecting the preferences of 100M citizens, and synthesizing them into policy suggestions in a coherent way. I'm not necessarily optimistic about the idea, but it's a nice dream. This is an interesting and thoughtful article I think, but worth evaluating in the context of the service (\"cognitive security\") its author is trying to sell. That's not to undermine the substance of the discussion on political/constitutional risk under the inference-hoarding of authority, but I think it would be useful to bear in mind the author's commercial framing (or more charitably the motivation for the service if this philosophical consideration preceded it). A couple of arguments against the idea of singular control would be that it requires technical experts to produce and manage it, and would be distributed internationally given any countries advanced enough would have their own versions; but it would of course provide tricky questions for elected representatives in the democratic countries to answer. A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION. think we're already there aren't we? no human came out with those tariffs on penguin island The really nice thing about this proposal is that at least now we can all stop anthropomorphizing Larry Ellison, and give Oracle the properly robot-identifying CEO it deserves. You sometimes hear people say \"I mean, we can't just give an AI a bunch of money/important decisions and expect it to do ok\" but this is already happening and has been for years. Examples: - Algorithmic trading: I once embedded on an Options trading desk. The head of desk mentioned ", "negative": "Ultraprocessed foods make up to 70% of the US food supply (2025). Just don't buy those foods. Buy fresh vegetables, tofu, and meat from the edges of the grocery store.   Simple.  Done. This article equates ultraprocessed foods and hyperpalatable foods (foods designed to make people want to eat them more).  While many hyperpalatable foods are classified as ultraprocessed, simply being hyperpalatable does not mean it's ultraprocessed. Worth noting that the Nova food classificationvsysten (which this article references) completely disregards the actual nutritional content of foods. For a good primer on a lot of the misconceptions around UPFs, check out [0]. [0]  https://www.harvardmagazine.com/research/harvard-ultraproces...  Headline is massively misleading. The actual study cited by the article, measures this as 71% of food products offered for sale in the US, by count of unique items, are ultraprocessed. Not that 71% of food products sold by weight or volume or dollar amount are ultraprocessed. This is just observing that if you list all food products for sale in the US, \"pear\" appears on that list once but \"Store Brand salty corn chips\" appears 25 times. (2025) OP More recently:  Ultra-processed foods make up more than 60% of us kids' diets   https://news.ycombinator.com/item?id=44823288   How America got hooked on ultraprocessed foods   https://news.ycombinator.com/item?id=45605921   California passes law to ban ultra-processed foods from school lunches   https://news.ycombinator.com/item?id=45525041  why does the USA not have the concept of buying home made meals from other people? I have never heard of a lunch box service or people buying one It's hard to get much more processed than sugar itself. Out of everything else, that should be one that's easy to remember. Pure white crystals often indicate the presence of a chemical in its most concentrated form. Among other dangers, are the hazard of overdosing more easily, intentionally or not. In this case, it seems "}
{"anchor": "He set out to walk around the world. After 27 years, his quest is nearly over.  https://archive.ph/2025.12.09-165741/https://www.washingtonp...  It was not one continuous hike. He takes frequent breaks. But travels back to where he last stopped and continues.  https://en.wikipedia.org/wiki/Karl_Bushby  Still very impressive, but a little less impressive than I first thought. >The world is a much kinder, nicer place than it often seems. I realize that a lot these days. People are not inherently so bad but greed is a nasty drug that has the potential to ruin the best. When you have nothing to offer but kindness and compassion, it is very simple to see the humanity side of things in this world and it can feel really amazing. A couple of Youtubers who are also round-the-world travelers whom I enjoy watching, one a Dutch motorcyclist and the other a German cyclist. Noraly, the motorcyclist, has already traveled through South and North America, Africa, and Asia, some multiple times. Currently, I believe she is in Tajikistan about to enter Kyrgystan.  https://www.youtube.com/channel/UCEIs9nkveW9WmYtsOcJBwTg  Max Roving, the cyclist, has already cycled through Afghanistan and he is currently trying to ride Africa north to south. He just completed Algeria and is about to enter Morroco.  https://www.youtube.com/@MaxRoving  Amazing This reminds me of an adventured died just a few months ago at age of 40 after suffering insult. He has crossed ocean on a rowboat and more.  https://boredofborders.com/adventures/  DeepL Translation of wiki: Bardel's largest and most notable expeditions involve crossing oceans and traveling around the world without external assistance. On May 4, 2016, he and his traveling companion Gints Barkovskis set out to cross the Atlantic Ocean from Namibia to Brazil. After 142 days, they safely reached the coast of South America, becoming the first two-person crew to cross the Atlantic Ocean in a rowboat. [6] During the voyage, both men encountered serious h", "positive": "Horses: AI progress is steady. Human equivalence is sudden. Engine efficiency, chess rating, AI cap ex. One example is not like the other. Is there steady progress in AI? To me it feels like it\u2019s little progress followed by the occasional breakthrough but I might be totally off here. I think it's a cool perspective, but the not-so-hidden assumption is that for any given domain, the efficiency asymptote peaks well above the alternative. And that really is the entire question at this point: Which domains will AI win in by a sufficient margin to be worth it? This is a fun piece... but what killed off the horses wasn't steady incremental progress in steam engine efficiency, it was the invention of the internal combustion engine. People are not simple machines or animals. Unless AI becomes strictly better than humans and humans + AI, from the perspective of other humans, at all activities, there will still be lots of things for humans to do to provide value for each other. The question is how do our individuals, and more importantly our various social and economic systems handle it when exactly what humans can do to provide value for each other shifts rapidly, and balances of power shift rapidly. If the benefits of AI accrue to/are captured by a very small number of people, and the costs are widely dispersed things can go very badly without strong societies that are able to mitigate the downsides and spread the upsides. \"In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines. And not very long after, 93 per cent of those horses had disappeared. I very much hope we'll get the two decades that horses did.\" I'm reminded of the idiom \"be careful what you wish for, as you might just get it.\" Rapid technogical change has historically lead to prosperity over the long term but not in the short term. My fear is that the pace of change this time around is so rapid that the short term d", "negative": "The hidden engineering of runways. Video is great, came up in my youtube recommendation cycle last week. Honestly one of the better things youtube has pitched to me, the quality/relevance of the rest of its recommendations have been nose diving over the last year (or so it feels). I saw a talk a long time ago about the structural aspects of runway design. The most interested fact I remember was that the stresses on the runway generated by departures was higher than those of arrivals, as departures repeatedly stress the same part of the runway, while jets land on a much more distributed area of the runway. Plus jets weigh a lot less at arrival than at departure. We had group coding projects at university, and the first one was always \"sponsored\" by the local airport. I think the ATC manager was friends with a lecturer. Every year the students built to the same spec in groups, being able to compare and contrast. It was great fun. The year before me was all about runway markings: take a bunch of industry specified XML describing the runway and produce accurate diagrams in a GUI browser. My year was runway \"redeclaration\", if a vehicle has broken down on the runway, you can still use the runway as a shorter strip, accounting for the onion layers of different zones radiating out from the tarmac itself, accounting for the height of the obstacle and angles of approach, accounting for all the necessary safety margins. It was my first real exposure to working in a team and to solving a real world problem with a good spec. Of course it was an absolute shitshow, but I look back on it fondly. I absolutely love that Grady includes full transcripts of his videos. It's much faster to read the article than watch the video, even though that hurts him by 1 view. I just watched parts of the video after reading because I wanted to see his explanations. One of the few really good creators out there. Something which still confuses me is the nature of the illuminations in the roadway. Bec"}
{"anchor": "Map To Poster \u2013 Create Art of your favourite city. Why are big chunks of Sam Francisco missing (eg around the bridge) missing from the example? It says there are examples but I can't see them? I believe (from a quick code check on my phone) it should be possible to output the images to SVG with a little tweak, thanks to your use of matplotlib? Is there a reason you\u2019ve defaulted to PNG that I\u2019m missing? That's splendid. I've long wanted to make a jigsaw puzzle out of Sydney's road map, so I can familiarise myself with the layout of roads while having fun. That way I can reduce my reliance on nav app and become one of those old-school drivers. The map of Venice seems to be the only one whose image is \"squeezed\" horizontally. Wondering why. I tried it in a python3 venv, but the download data step is stuck at 0% unfortunately. Three random themes for anyone who's Czech or likes Prague and doesn't want to set up the script locally:  https://imgur.com/a/Ovg8mDW  Also check out prettymapp  https://prettymapp.streamlit.app/  This repo is fantastic. The README should be the gold standard for OSS. Not to mention how stunning the outputs are. Thanks for sharing. Pretty cool! It would be great if there was a way to set coordinates manually, since Nominatim can sometimes produce mediocre results. Also, would be nice to have a way to render the same map in all themes, not just one. what are the blue dots? (not water bodies i think?) San Francisco looks nice, but there seems to be a problem with the projection in some of the sample images. It looks as if it isn't UTM but a global sphere projection, which isn't suitable for local renders. It's suspicious that the word 'projection' isn't mentioned in the Readme. Does anyone have recommendations for how to actually print a poster from images like these? Very cool, thanks for sharing!! Looks amazing !\nIn my free time, i play with my laser cutting machine.\nIt will save me some design time.\nThanks What happens if there are multiple citi", "positive": "Eulogy for Dark Sky, a data visualization masterpiece (2023). Meteoswiss app is the best weather app ever created Yes Dark Sky had the best UI of any weather app I have used. I now use Weathergraph which does it differently but I would go back to Dark Sky (and pay for it) in a flash. It shows the correct things and on a phone understands that showing the temperatures across the screen is useless as if I go out I want to know what the weather is like when I might make the journey back in 8+ hours time. I might not care what the weather is in 4 hours time as I will be inside. Dark Sky was a marvel, and when it first came out, its ability to say rain will start where you are in 2-3 minutes was a marvel. The information design argument is 100% valid, but I also marvel that, having bought the company, Apple's weather app still isn't as precise or accurate. I don't know whether Apple's privacy focus prevents them making the same precise predictions, or if there is some other reason they don't, but it's sad that in 2025 we don't have the same level of performance as we did twelve years ago. Apple should have just used that app itself, rather than trying to build whatever that they have right now. The Apple Weather app has gotten better over time, though it\u2019s still not a perfect replacement. Scrolling through the Dark Sky screenshots, I can recognize many of the same things now incorporated with Apple\u2019s. And Apple does offer location specific notifications of rain which I find to be pretty accurate, about as accurate as Dark Sky. There\u2019s largely a perception problem with Apple. People loved Dark Sky as an independent small app that worked well, before Apple took it and destroyed it. Now, even if Apple incorporated all of the same data and features, it still wouldn\u2019t give that same spark of joy people had. fwiw, on iOS, I like using WeatherGraph:  https://weathergraph.app/  The developer is very responsive, lots of UI customization (both app and widgets) is possible, and pri", "negative": "1000 Blank White Cards. Sounds a bit like \"We Didn't Playtest This at All\" ( https://boardgamegeek.com/boardgame/31016/we-didnt-playtest-... ), which is a lot of fun as an icebreaker game in various settings. This version has the cards prepopulated with content. I love 1k bwc, just played it at a friend's going-away party. It's surprisingly hard to explain to folks who have never played before -- there's a lot of 'wait, what am I even supposed to do?' But if you have any friends in improv or folks who are good at coming up with clever cards, it's a lot of fun There's a drinking game which I guess is inspired by this game, which I believe is called \"Pizza Box\" (at least that's what everyone I ever met who knew it called it). You start with an empty pizza box, and you need a large coin (the Australian 50 cents works well) and a sharpie. Play progresses around the circle of players. Each player must flip the coin into the box. If they intersect no other circles, they draw a circle around the coin with the sharpie, and then write a rule into the circle (Whatever rule they come up with must fit legibly). They can change any aspect of the game. If you intersect with a circle, instead, that rule is activated. Just like 1000 cards, that could impact everyone, just you, whatever... We usually got to a point where someone added a circle to \"end the game\", which then people might aim for - but usually only after a couple of hours of merriment! This but for programmers:  https://github.com/nomyx/Nomyx  Here a Youtube playlist of some people playing it:  https://youtube.com/playlist?list=PLMrpfY5oU1DY79EQTQ_aD0-Ub...  (using cards submitted by their viewers) This is a meta-game.  I got curious about related topics in game theory once and found out about [1,2].  There are also a few papers directly trying to study calvinball and so-called minimal-nomic.  It's pretty crazy how little we know theoretically  about this stuff, considering how relevant games with dynamic rules actuall"}
{"anchor": "FAA institutes nationwide drone no-fly zones around ICE operations. Not shady at all. Can\u2019t have the public see what\u2019s going on. Bubble of protection is 3000 feet laterally and 1000 feet vertically. From the article: \u201cUnlike traditional Temporary Flight Restrictions, the NOTAM does not provide geographic coordinates, activation times, or public notification when the restriction is in effect near a specific location. Instead, the restricted airspace moves with DHS assets, meaning the no-fly zone can appear wherever ICE or other DHS units operate.\u201d \u201cIn practical terms, a drone operator flying legally in a public area could unknowingly enter restricted airspace if an ICE convoy passes within the protected radius.\u201d This is a useful measure to point the law as a weapon against drone operators who may be recording what\u2019s going on by accident or on purpose. Any drone made in the last few years is going to be emitting its ID, which likely has been registered with the pilot\u2019s name and contact information. They can then after the fact come down on that person without having to get facial recognition, grab cellphone beacons, or other similar steps. My understanding is that DJI drones no longer enforce no-fly zones.  Supposedly they still warn you when entering a restricted zone, but hard geofencing functionality is no longer in effect.  Anyone know if that's true? I don\u2019t fully understand why drone operators follow these laws. Or any \u201cno-fly\u201d rules in general. Around an airport, it seems like common sense to not fly. Can\u2019t someone just\u2026buy/build a drone and fly is surreptitiously? Doesn\u2019t anything under 250g basically slip under the radar (not literally radar). Seems like most drones they care about might end up not being trackable anyway. > ALL UNMANNED ACFT ARE PROHIBITED FROM FLYING WITHIN A STAND-OFF DISTANCE OF 3000FT ... LATERALLY AND 1000FT ABOVE ... > TO: DEPARTMENT OF DEFENSE (DOD), DEPARTMENT OF ENERGY (DOE), AND DEPARTMENT OF HOMELAND SECURITY (DHS) FACILITIES AND M", "positive": "Waymo granted permit to begin testing in New York City. I saw one of these on Chambers Street just yesterday afternoon, but it must have been in manual mode, of course. It's fascinating seeing all the comments elsewhere anytime Waymo starts testing in another city along the lines of, \"ah, but how will they handle X, Y, and Z here?? Checkmate, robots!\" despite having already launched service in several other cities. Granted, NYC is the biggest city in the US, so maybe that sort of reaction is more reasonable there than when people in Dallas or Boston do it. Very cool. I wonder what scale it has to hit for this to become a profitable line item for Google and what their revenue targets are for it. Man I love Waymo everytime I'm in SF. Truly feel like I'm living in the future when I sit in one I'm cautiously optimistic about this self-driving thing. Waymo at least seems to have figured out a lot of it. Would it be way better to make walkable neighborhoods, mixed-use developments, and reliable and frequent public transit? Yes. Yes it would. But, in lieu of that, self-driving has a lot of advantages in the long run, even if the technology isn't 100% perfect right now. It's insane that they need permits for 8 cars that have humans driving them in 2025, when they're already fully automated in SF. > We\u2019re a tech-friendly administration Clearly not. Is this the first time Waymo is doing winter / snow testing at scale? I think some of the Pittsburgh-based self-driving firms may have tried this, but unaware how far they got. I\u2019m curious if autonomous cars will become targets for aggressive drivers. Like a driver isn\u2019t going to be as scared cutting off a Waymo or tailgating one because the AI isn\u2019t gonna get road rage or honk like hell. In some places I could see the Waymo\u2019s getting severely bullied if that\u2019s the case. The game-theoretic aspect of this is interesting to me. A lawful robot will never make progress in Manhattan because the people will just walk across its path con", "negative": "Six-decade math puzzle solved by Korean mathematician. (2024) Source is Scientific-American  https://www.scientificamerican.com/article/mathematicians-so...  ( https://news.ycombinator.com/item?id=42946052 ) Discussion on the paper (131 points, 2024, 36 comments)  https://news.ycombinator.com/item?id=42300382  >> \u201cYou keep holding on to hope, then breaking it, and moving forward by picking up ideas from the ashes,\u201d [Baek] said in an interview with a web magazine published by Korean Institute for Advanced Study. \u201cI\u2019m closer to a daydreamer by nature, and for me mathematical research is a repetition of dreaming and waking up.\u201d beautiful! Dan Romik has a nice intro on the moving sofa problem:  https://www.math.ucdavis.edu/~romik/movingsofa/  Is the equivalent problem in 3D harder or easier? Seems like it would be harder but you never know with these things. Actual paper:  https://arxiv.org/pdf/2411.19826  This is the famous sofa problem! It's hard to believe it's finally solved; I've spent many evenings staring at the wikipedia article wondering at how even what seem to be the simplest of problems defy the reach of mathematics. I love the kind of science reporting on display in this article! It stays at a consistent, objective level of detail throughout (no \"imagine a vector space as a block of jello\" or whatever it is that Quanta and other publications are always doing). It allows specialists to understand exactly what's being claimed, and at the same time stays accessible to laypeople. It feels like it's written for the kind of reader that I aspire to be: not necessarily a specialist on every topic under the sun, but someone who has finished high school and is paying attention. Though I guess writing like this doesn't pay off in the modern world. Most readers don't consistently pay attention when reading, and to be honest, I don't either. Looking at that graphic... it almost seems obvious. The outer corner radius would be relative to the inner curve radius in some fi"}
{"anchor": "NanoChat \u2013 The best ChatGPT that $100 can buy. Wow, how do we sign up for the Eurekalabs course and how much does it cost? I've always thought about the best way to contribute to humanity: number of people you help x how much you help them. I think what Karpathy is doing is one of the highest leverage ways to achieve that. Our current world is build on top of open source projects. This is possible because there are a lot of free resources to learn to code so anyone from anywhere in the world can learn and make a great piece of software. I just hope the same will happen with the AI/LLM wave. Eureka Labs:  https://github.com/EurekaLabsAI  What a prolific person Andrej is. It's been more than amazing to follow along! Here's the announcement post [0] from Karpathy, which provides a bit of additional context. [0]  https://x.com/karpathy/status/1977755427569111362  > Thank you to chief LLM whisperer   Alec Radford for advice/guidance. oh man an Alec x Andrej podcast would BREAK THE INTERNET... just saying... going from glory days of GPT1 to now building GPT3? in 4 hours Should be \"that you can train for $100\" Curios to try it someday on a set of specialized documents. Though as I understand the cost of running this is whatever GPU you can rent with 80GB of VRAM. Which kind of leaves hobbyists and students out. Unless some cloud is donating gpu compute capacity. >If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for --device_batch_size in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. That sounds like it could run on a 24gb GPU. Batch size of 8 would imply 20gb mem, no? ...presumably just takes forever This weekend I just cracked into nanoGPT ( https://github.com/karpathy/nanoGPT ), an older but fabulous learning exercise where you build and train a crappy shakespeare GPT with ~0.8M parameters on a cpu. Results are about what you'd expect from that, they", "positive": "Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete. I read the release but didn't quite understand the difference between a next-edit model and a FIM model - does anyone have a clear explanation of when to use one over the other? I'd love if there was a sublime plugin to utilize this model and try it out, might see if I can figure that out. I use Sweep\u2019s Jetbrains autocomplete plugin daily, it really stands out. Based on qwen2.5-coder? seems like a \"why not/resume embellish/show VC\" type release I guess can it be integrated in monaco editor ? So SFT cost less only low hundreds of dollars? (1-10$ per hour per H100 if I'm seeing this correctly). What about SFT? Presumably basing this of Qwen is the reason it can be done for so cheap? Wow super fun read, I love how it went into the technical details. Any way to make it work with vscode? This is cool! I am more interested in how you guys generated next edit training data from repos, seems like there are lots of caveats here. Would love your insights Again amazing work! waiting for what you guys cook next I'm very green to this so forgive if this question sounds silly: Would instead of the RL step a constrained decoding say via something like xgrammar fix   syntax generation issue ? Do you plan to release Sweep 3B/7B on HF? It's good. The blog post about it is very interesting.\nI hope, a plugin for neovim will be made soon.  https://blog.sweep.dev/posts/oss-next-edit  Followed your work since the beginning and used it for inspiration for some cool demos on self-healing web scrapers. fascinating to see the transition from original concept to producing models. cool stuff. Very interesting - and cool to read about the development process. I'd love to hear more about how genetic algorithm worked here. I wonder whether we are perhaps the point of usefulness of 'next edit' code development in 2026 though. Any easy way to try on vscode? Surprising how badly Jetbrains implemented AI. Apparently to such an extent ", "negative": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs. So, then if I want to use a certain terminal text editor to create a clone of it in nanolang, I'd end up typing nano nano.nano on the command line. I might accidentally summon a certain person from Ork. One novel part here is every function is required to have tests that run at compile time. I'm still skeptical of the value add having to teaching a custom language to an LLM instead of using something like lua or python and applying constraints like test requirements onto that. It seems that something that does away with human friendly syntax and leans more towards a pure AST representation would be even better? Basically a Lisp but with very strict typing might do the trick. And most LLMs are probably trained on lots of Lisps already. Developed by Jordan Hubbard of NVIDIA (and FreeBSD). My understanding/experience is that LLM performance in a language scales with how well the language is represented in the training data. From that assumption, we might expect LLMs to actually do better with an existing language for which more training code is available, even if that language is more complex and seems like it should be \u201charder\u201d to understand. I feel like the time for this was two years ago, and LLMs are now less bothered by remembering syntax than I am. It's a nice lisp-y syntax though. I went looking for a single Markdown file I could dump into an LLM to \"teach\" it the language and found this one:  https://github.com/jordanhubbard/nanolang/blob/main/MEMORY.m...  Optimistically I dumped the whole thing into Claude Opus 4.5 as a system prompt to see if it could generate a one-shot program from it:     llm -m claude-opus-4.5 \\\n    -s https://raw.githubusercontent.com/jordanhubbard/nanolang/refs/heads/main/MEMORY.md \\\n    'Build me a mandelbrot fractal CLI tool in this language' \n   > /tmp/fractal.nano\n  \nHere's the transcript for that. The code didn't work:  https://gist.github.com/simonw/784"}
{"anchor": "Kids who ran away to 1960s San Francisco. Very cool. If you're interested in things like this you might wanna checkout CGP Grey's videos on tracking down various stories from books through archives.  https://www.youtube.com/watch?v=qEV9qoup2mQ  I am happy the author followed her curiosity. I remember feeling much the same \u201cpull\u201d when I moved to San Francisco in 2013. Those of us who really vibe with the place seem to share a desire to get behind the city\u2019s strange magic and discover the past souls and events that make San Francisco what it is - that make it  feel  this particular way that it does. To the author and everyone else who has arrived here recently: welcome to San Francisco! A good friend of mine ran away to San Francisco in the mid 80s when he was 15. And his parents flew there and brought him back. I loved my time in SF. For those that remember Detour GPS guided audio tours in 2015 that Andrew Mason founded, the audio tours in SF were next level and so special and showed a side of the history of SF I hadn't seen. Luckily they're preserved on Spotify (although without the GPS guided part) -  https://creators.spotify.com/pod/profile/detour-podcast/  I have a great uncle that moved to Haight Ashbury to chase the whole spiritual open your mind idea. He said it was nothing like the media or nostalgia portrayed it. Lots of homeless drugged out kids who were completely lost. No jobs, panhandling for food and money, no direction, just spaced out druggies. Said it was fairly sad and he left within a year. He is an old hippy type as well, it was not what I was expecting to hear. I remember seeing an interview of George Harrison saying something similar. I have frequently walked by the \"OG Huckleberry House\" depicted in the photo near the bottom, and knew its history. It's near the stairwell and garden that connects Broderick St with Buena Vista East. You can actually see, on the northern side of that incline that is a steep ramp with no stairs, that the house goes", "positive": "AI just proved Erdos Problem #124. This seems to be 2nd in row proof from the same author by using the AI models. First time it was the ChatGPT which wrote the formal Lean proof for Erdos Problem #340.  https://arxiv.org/html/2510.19804v1#Thmtheorem3  > In over a dozen papers, beginning in 1976 and spanning two decades, Paul Erd\u0151s repeatedly posed one of his \u201cfavourite\u201d conjectures: every finite Sidon set can be extended to a finite perfect difference set. We establish that {1, 2, 4, 8, 13} is a counterexample to this conjecture. Ok\u2026 has this been verified? I see no publication or at least an announcement on Harmonics webpage. If this is a big deal, you think it would be a big deal, or is this just hype? Related, independent, and verified: GPT-5 solved Erd\u0151s problem #848 (combinatorial number theory):  https://cdn.openai.com/pdf/4a25f921-e4e0-479a-9b38-5367b47e8...   https://lifearchitect.ai/asi/  More interesting discussion than on Twitter here:  https://www.erdosproblems.com/forum/thread/124#post-1892  This is response from mathematician:\n\"This is quite something, congratulations to Boris and Aristotle! On one hand, as the nice sketch provided below by tsaf confirms, the final proof is quite simple and elementary - indeed, if one was given this problem in a maths competition (so therefore expected a short simple solution existed) I'd guess that something like the below would be produced. On the other hand, if something like this worked, then surely the combined talents of Burr, Erd\u0151s, Graham, and Li would have spotted it. Normally, this would make me suspicious of this short proof, in that there is overlooked subtlety. But (a) I can't see any and (b) the proof has been formalised in Lean, so clearly it just works! Perhaps this shows what the real issue in the [BEGL96] conjecture is - namely the removal of 1 and the addition of the necessary gcd condition. (And perhaps at least some subset of the authors were aware of this argument for the easier version allowing 1", "negative": "AI on Australian travel company website sent tourists to nonexistent hot springs. Weldborough seems to have done well out of it either way. New variant on \"I followed my satnav blindly and now I'm stuck in the river\", except less reliable. It is however fraud on the part of the travel company to advertise something that doesn't exist. Another form of externalized cost of AI. Australia has drop bears anyhow. Do they exist? Seems par for course. >Scott Hennessey, the owner of the New South Wales-based Australian Tours and Cruises, which operates Tasmania Tours, told the Australian Broadcasting Network (ABC) earlier this month that \u201cour AI has messed up completely.\u201d To me this is the real takeaway for a lot of these uses of AI. You can put in practically zero effort and get a product. Then, when that product flops or even actively screws over your customers, just blame the AI! No one is admitting it but AI is one of the easiest ways to shift blame. Companies have been doing this ever since they went digital. Ever heard of \"a glitch in the system\"? Well, now with AI you can have as many of those as you want, STILL never accept responsibility, and if you look to your left and right, everyone is doing it, and no one is paying the price. How often do you have to update your page on \"what's in a town\" to \"compete with the big boys\"? Seems like you could just google what's in the town, or visit if you really want to make sure, rather than just asking your favourite LLM \"What's there to do in Weldborough\"? \u201cour AI has messed up completely.\u201d No, it worked as designed. Generative AI simply creates content of the type that you specify, but has no concept of truth or facts. has anyone checked to see if the AI included time co ordinates as well?\nit  might be that AI is missunderstanding our tempotral limitations, and if prompted correctly will provide a handy portal to when, there will in fact be hot springs at the location suggested. In case anyone else is curious, I just entered"}
{"anchor": "How to live on $432 a month in America. I've often felt this way about some of today's complaints. I grew up in area like what was mentioned in this article and I long for the day I can go back there. I would in a heartbeat if my partner shared the same mentality as me. I don't really see a point in living a big city with the remote job I have and that many others have if I can live in a smaller area that still has humans but much cheaper way of living. Everyone claims it's about living in a city with available services but I see those same people decry how much the food costs and also that they have no friends and can't find someone to date. My thoughts aren't as articulate as I'd like them to be but I guess I'm ultimately trying to say is if I'm going to be miserable, why not do it on my own land for a lot cheaper. It makes a certain amount of sense and I myself bought a little place way out in the hinterlands of Michigan for similar economic reasons ... but I live in Berkeley because subjecting your children to life without opportunities for art, culture, education, sports, friends, etc is cruel. So if you're white, or just don't care that your ethnicity is absent, and if you have no children, and also don't mind living in a car-dependent place where the public transit to the nearest major city is a minimum of 15 hours with 3-4 transfers, then sure Massena NY is dope. There is a little bit of a sleight of hand going on in this article by claiming the lifestyle of boomers is within reach, but then actually using boomers' parents and grand-parents as the standard. It would be more honest to say \"Most of us can't have the relative wealth of our grand parents, but with some sacrifices and creativity, the lifestyle of our great-grand parents is attainable.\" Even that is only true in a very narrow sense. My great-grand parents built a 600sqft house in a small town and lived their most of their lives. But they built that house right next to their parents. They lived wit", "positive": "Claude Code can debug low-level cryptography. This resonates with me a lot: > As ever, I wish we had better tooling for using LLMs which didn\u2019t look like chat or autocomplete I think part of the reason why I was initially more skeptical than I ought to have been is because chat is such a garbage modality. LLMs started to \"click\" for me with Claude Code/Codex. A \"continuously running\" mode that would ping me would be interesting to try. Coming soon, adversarial attacks on LLM training to ensure cryptographic mistakes. CLI terminals are incredibly powerful. They are also free if you use Gemini CLI or Qwen Code. Plus, you can access any OpenAI-compatible API(2k TPS via Cerebras at 2$/M or local models). And you can use them in IDEs like Zed with ACP mode. All the simple stuff (creating a repo, pushing, frontend edits, testing, Docker images, deployment, etc.) is automated. For the difficult parts, you can just use free Grok to one-shot small code files. It works great if you force yourself to keep the amount of code minimal and modular. Also, they are great UIs\u2014you can create smart programs just with CLI + MCP servers + MD files. Truly amazing tech. > For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it? You can use Git hooks to do that. If you have tests and one fails, spawn an instance of claude a prompt -p 'tests/test4.sh failed, look in src/ and try and work out why'       $ claude -p 'hello, just tell me a joke about databases'\n\n    A SQL query walks into a bar, walks up to two tables and asks, \"Can I JOIN you?\"\n\n    $ \n  \nOr if, you use Gogs locally, you can add a Gogs hook to do the same on pre-push > An example hook script to verify what is about to be pushed.  Called by \"git push\" after it has checked the remote status, but before anything has been pushed.  If this script exits with a non-zero status nothing will be pushed. I like this idea. ", "negative": "Time Station Emulator. And the 2024 lateral thinking award goes to...  \u201cOne of the higher-frequency harmonics inevitably created by any real-world DAC during playback will then be the original fundamental, which should leak to the environment as a short-range radio transmission via the ad-hoc antenna formed by the physical wires and circuit traces in the audio output path.\u201d  Sometimes I think I\u2019m a smart guy\u2026and then I read of people doing shit like this. I once programmed my TI-84 calculator to do exactly this! The only missing thing was a circuit to convert the audio jack output voltage into the needed form for an antenna. I had the CS know-how but no EE know-how, so I never got it to work. It was fun to dream about confusing my high school's clocks though. (Sadly, the other obstacle was that the clocks only listened for the signal overnight, which improved their chances of detecting the weak broadcast out of far-away Colorado.) This is pretty darn cool, but I have to say I was somewhat let down by the WWVB signal. I was expecting the entire audible range instead of simply the extracted data. That being said, that's also really darn cool. I find the WWV/WWVB droning soothing somehow. Shame there's no video demonstrating it working. It's a fun idea but without a demo, I'm left wondering about the efficacy. You can apparently lock shopping trolleys using the same kind of principle -  https://www.tmplab.org/2008/06/18/consumer-b-gone/  And  https://www.youtube.com/watch?v=LmSyb0kBvGE  Whenever I see things like this, my first thought is that somebody is going to write something to mess up nearby atomic clocks, just because they can, then I think- why, why did you do this? Then someone will respond: you\u2019re just catatrophising- anyone could\u2019ve done this years before now, and I\u2019ll say no, because it wasn\u2019t up on frontpage HN there with code so that anyone would think of it. Then they\u2019ll say, well why did you tell everyone that idea then! It\u2019s your fault! Then I\u2019ll say t"}
{"anchor": "\u201cErdos problem #728 was solved more or less autonomously by AI\u201d. Reconfiguring existing proofs in ways that have been tedious or obscured from humans, or using well framed methods in novel ways, will be done at superhuman speeds, and it'll unlock all sorts of capabilities well before we have to be concerned about AGI. It's going to be awesome to see what mathematicians start to do with AI tools as the tools become capable of truly keeping up with what the mathematicians want from the tools. It won't necessarily be a huge direct benefit for non-mathematicians at first, because the abstract and complex results won't have direct applications, but we might start to see millenium problems get taken down as legitimate frontier model benchmarks. Or someone like Terence Tao might figure out how to wield AI better than anyone else, even the labs, and use the tools to take a bunch down at once. I'm excited to see what's coming this year. This is great, there is still so much potential in AI once we move beyond LLMs to specialized approaches like this. EDIT: Look at all the people below just reacting to the headline and clearly not reading the posts. Aristotle ( https://arxiv.org/abs/2510.01346 ) is key here folks. EDIT2: It is clear much of the people below don't even understand basic terminology. Something being a transformer doesn't make it an LLM (vision transformers, anyone) and if you aren't training on language (e.g. AlphaFold, or Aristotle on LEAN stuff), it isn't a \"language\" model. You can try out Aristotle yourself today  https://aristotle.harmonic.fun/ . No more waitlist! Can anyone with specific knowledge in a sophisticated/complex field such as physics or math tell me: do you regularly talk to AI models? Do feel like there's anything to learn? As a programmer, I can come to the AI with a problem and it can come up with a few different solutions, some I may have thought about, some not. Are you getting the same value in your work, in your field? For context, Teren", "positive": "Understanding Machine Learning: From Theory to Algorithms. Anyone who wants to demystify ML should read: The StatQuest Illustrated Guide to Machine Learning [0] By Josh Starmer.\nTo this day I haven't found a teacher who could express complex ideas as clearly and concisely as Starmer does. It's written in an almost children's book like format that is very easy to read and understand. He also just published a book on NN that is just as good. Highly recommend even if you are already an expert as it will give you great ways to teach and communicate complex ideas in ML. [0]:  https://www.goodreads.com/book/show/75622146-the-statquest-i...  I have read parts of it years ago. As far as I remember, this is very theoretical (lots of statistical learning theory, including some IMHO mistaken treatment of Vapnik's theory of structural risk minimization), with strong focus on theory and basicasically zero focus on applications. Which would be completely outdated by now anyway, as the book is from 2014, an eternity in AI. I don't think many people will want to read it today. As far as I know, mathematical theories like SLT have been of little use for the invention of transformers or for explaining why neural networks don't overfit despite large VC dimension. Edit: I think the title \"From theory to machine learning\" sums up what was wrong with this theory-first approach. Basically, people with interest in math but with no interest in software engineering got interested in ML and invented various abstract \"learning theories\", e.g. statistical learning theory (SLT). Which had very little to do with what you can do in practice. Meanwhile, engineers ignored those theories and got their hands dirty on actual neural network implementations while trying to figure out how their performance can be improved, which led to things like CNNs and later transformers. I remember Vapnik (the V in VC dimension) complaining in the preface to one of his books about the prevalent (alleged) extremism of", "negative": "Rust\u2019s Standard Library on the GPU. I feel like the title is a bit misleading. I think it should be something like \"Using Rust's Standard Library from the GPU\". The stdlib code doesn't execute on the GPU, it is just a remote function call, executed on the CPU, and then the response is returned. Very neat, but not the same as executing on the GPU itself as the title implies. How different is it from rust-gpu effort? UPDATE: Oh, that's a post from maintainers or rust-gpu. Can I execute FizzBuzz and DOOM on GPU? Are there any details around how the round-trip and exchange of data (CPU<->GPU) is implemented in order to not be a big (partially-hidden) performance hit? e.g. this code seems like it would entirely run on the CPU?       print!(\"Enter your name: \");\n    let _ = std::io::stdout().flush();\n    let mut name = String::new();\n    std::io::stdin().read_line(&mut name).unwrap();\n  \nBut what if we concatenated a number to the string that was calculated on the GPU or if we take a number:       print!(\"Enter a number: \");\n    [...] // string number has to be converted to a float and sent to the GPU\n    // Some calculations with that number performed on the GPU\n    print!(\"The result is: \" + &the_result.to_string()); // Number needs to be sent back to the CPU\n\n  \nOr maybe I am misunderstanding how this is supposed to work? I'm confused about this: As the article outlines well, Std Rust (over core) buys you GPOS-provided things. For example:     - file system\n  - network interfaces\n  - dates/times\n  - Threads, e.g. for splitting across CPU cores\n  \nThe main relevant one I can think which applies is an allocator. I do a lot of GPU work with rust: Graphics in WGPU, and Cuda kernels + cuFFT mediated by Cudarc (A thin FFI lib). I guess, running Std lib on GPU isn't something I understand. What would be cool is the dream that's been building for decades about parallel computing abstractions where you write what looks like normal single-threaded CPU code, but it automagically "}
{"anchor": "27M Fewer Car Trips: Life After a Year of Congestion Pricing. non-paywall link:  https://www.nytimes.com/interactive/2026/01/05/upshot/conges...  Sounds like a good reason to not invest in parking garages. I never understood why big, congested cities in the USA (NYC, Boston, L.A., D.C., Chicago) aren't dumping money into funding FSD research hand over fist. It's not a moonshot anymore, and it would be the game-changer of the century in terms of public transportation and GDP. This is a long-term no-brainer for prosperity. The first graph makes no sense? Why are the initial actual and expected values so far apart? Shouldn't they start at the same point? That's valuable real estate for housing. House people, not cars. What's the theory of change here? FSD fundamentally is going to make keeping a car moving on the road cheaper, and making something cheaper makes it happen more. In what world do you get that conclusion? Dense cities in other parts of the world rely on mass transit to move people. FSD so you can have self driving cars in the street -> ? -> increasing congestion. The point is you should have more effective volume transit not optimizing random ones.  1000 cars on FSD are an optimization better than 1000 taxi drivers, compared to a train or a few buses.  https://www.youtube.com/watch?v=040ejWnFkj0  They don't describe the graphic very well in the article, but they do link to the source data [1]. The \"Expected\" line seems to refer to a historical average. Since the starting point of the graph coincides with the beginning of congestion pricing, we would expect a difference between the two values at that point. [1]  https://metrics.mta.info/?cbdtp/vehiclereductions  FSD could largely eliminate privately owned vehicles, it could also allow cities to get rid of most parking infrastructure. It eliminates traffic, parking, and alll the other pain points of owning a vehicle just to get from A to B when a car is the only true option. Certainly, FSD buses would be a w", "positive": "Gemini 3 Flash: Frontier intelligence built for speed. Deepmind Page:  https://deepmind.google/models/gemini/flash/  Developer Blog:  https://blog.google/technology/developers/build-with-gemini-...  Model Card [pdf]:  https://deepmind.google/models/model-cards/gemini-3-flash/  Gemini 3 Flash in Search AI mode:  https://blog.google/products/search/google-ai-mode-update-ge...  They went too far, now the Flash model is competing with their Pro version. Better SWE-bench, better ARC-AGI 2 than 3.0 Pro. I imagine they are going to improve 3.0 Pro before it's no more in Preview. Also I don't see it written in the blog post but Flash supports more granular settings for reasoning: minimal, low, medium, high (like openai models), while pro is only low and high. Don\u2019t let the \u201cflash\u201d name fool you, this is an amazing model. I have been playing with it for the past few weeks, it\u2019s genuinely my new favorite; it\u2019s so fast and it has such a vast world knowledge that it\u2019s more performant than Claude Opus 4.5 or GPT 5.2 extra high, for a fraction (basically order of magnitude less!!) of the inference time and price Does this imply we don't need as much compute for models/agents? How can any other AI model compete against that? Pretty stoked for this model. Building a lot with \"mixture of agents\" / mix of models and Gemini's smaller models do feel really versatile in my opinion. Hoping that the local ones keep progressively up (gemma-line) These flash models keep getting more expensive with every release. Is there an OSS model that's better than 2.0 flash with similar pricing, speed and a 1m context window? Edit: this is not the typical flash model, it's actually an insane value if the benchmarks match real world usage. > Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications. The replacement for old flash models will be proba", "negative": "Nvidia's 10-year effort to make the Shield TV the most updated Android device. Now if only they would release an updated one. honestly, shield tv changed how i interact with my tv and my opinion about Android TV (even though its market sucks) It's ironic that Nvidia before becoming a behemoth had the money for this kind of device. I have had two for 10 years and have no complaints whatsoever I've got the OG model, and it's still the main device hooked up to my TV. All my TV streaming goes through it (mostly Jellyfin these days), and it can stream games no problem via Moonlight. It's hooked up to a 4k LG TV, and I have no idea about how it does the upscaling, but 720p content looks perfectly fine on it. Best (worst?) of all... it still gets updates. Shield TV + extra storage + HDHomeRun tuner is still a great device for getting OTA TV. The only downside is that more recent versions use the Google Android TV launcher which is filled with a garbage truck full of ads, often for things I would never want to watch (horror movies? Nope!).  Yes you can replace the launcher, but that's a pain. Would love to pay more for a device that has updated codec support, no ads or tracking, and is basically identical. My shield bricked itself after just a few months, so YMMV on this. No rhyme or reason why. Dupe of  https://news.ycombinator.com/item?id=46824003  What a new model would need is more compat with DV and better software. Get rid of the Android ads. Add better frame rate matching. Etc. The Shield TV's cylindrical form factor could use a rethink. It is hard to find a good spot for it on a shelf when cords are connected at both ends (HDMI and MMC slot at one end, power and LAN at the other) and the ports are too close for all cords to use right-angle-heads. Leaving it invisible by placing it on the floor or behind other gear sometimes impedes Bluetooth signal, so there it sits, well apart from the AVR, BD, other devices. My Nvidia Shield Portable is sad to hear this.  They upd"}
{"anchor": "Chatbot Psychosis. The interesting part to me is how this anti-feature could become the primary source of value for AI if only it was easier for everyone to run and train locally from a blank slate and without the clumsy natural language interface. Take the example of music. Most musicians probably don't want crap like Suno. What they actually want is a fountain of ideas to riff on based on a locally trained AI where they have finer-grained control over the parameters and attention. Instead of \"telling\" the AI \"more this and less that\" would it not make more sense to surface facets of the training data in a more transparent and comprehensible way, and provide a slider control or ability to completely eliminate certain influences? I'm aware that's probably a tall order, but it's what's necessary. Instead of producing delusions left to random chance and uncurated training data, we should be trying to guide AI towards clarity with the user in full control. The local training by the user effectively becomes a mirror of that user's artistic vision. It would be unique and not \"owned\" by anyone else. As I understand it - a person with psychosis is someone who has over-weighted perceptions that cannot be corrected with sensory input. Hence to \"bring someone to their senses\". I've seen and thought there might be a few programmers maybe with a related (not psychosis) \"ai mania\" - where one thinks one changing the world and uniquely positioned. It's not that we're not capable in our small ways with network effects, or like a hand touch could begin the wave breaking (hello surfers!) what distinguishes this capacity for small effects having big impacts from the mania version is the mania version bears no subtle understanding of cause and effect. A person who is adept in understanding cause and effect usually there's quite a simple, beautiful and refined rule of thumb the know about it. \"Less is more\". Mania on the other hand proliferates outward - concept upon concept upon conce", "positive": "Mecha Comet \u2013 Open Modular Linux Handheld Computer. This project is currently seeking funds (and is funded) on Kickstarter:  https://www.kickstarter.com/projects/mecha-systems/mecha-com...  (and the super early bird rewards are all gone) I might be interested if I weren't still waiting on the Soulcircuit Pilet to ship.... Stumbled across this thing a while back and thought it looked really cool but I have never been able to come up with an idea for how I would use it so I haven't pledged. I want to want it but I fear it would just sit on my desk. Does anyone have cool ideas for uses? I am planning to build something similar as a hobby project except my idea is that Claude Code runs everything on the device for you. So... what's the OS situation? From a glance at  https://github.com/mecha-org/linux  ->  https://github.com/mecha-org/linux/commits/imx/lf-6.12.20/  it  looks  like they're starting from a 6mo-old kernel (6.12.20 vs current LTS 6.12.67 and current stable 6.18.7). Is there any reason to expect upstreaming or even just consistent updates, or is this yet another device that will ship with an old-ish kernel and never get updated again? Small gimmicky computers seem to attract so much attention and people who can\u2019t help themselves but buy it, play with it for a while, then toss it into a drawer and never use it again. So it\u2019s a Raspberry Pi except now I can type Unix commands with my thumbs on a blackberry keyboard\u2026\u2026ouch. This reminds me of Phonebloks from 13 years ago.  https://de.wikipedia.org/wiki/Phonebloks  Website's a bit weird.  The app icons highlight when you hover over them, but don't seem to do anything. They've got a grab-bag of unrelated Linux etc. org icons - Nix, Debian, postmarketOS, Node, Kubernetes\u2026  You could argue that someone _could_ run Nix or Node on it, but Debian is just nerdbait.  It's not relevant to the product they're selling, unless you're gonna wipe the disk and support it yourself. Open Hardware + Open Software is good enough fo", "negative": "Claude Code daily benchmarks for degradation tracking. FYI the MarginLab Claude Code degradation tracker is showing a statistically significant ~4% drop in SWE-Bench-Pro accuracy over the past month Very interesting. I would be curious to understand how granular these updates are being applied to CC + what might be causing things like this. I feel like I can notice a very small degradation but have compensated with more detailed prompts (which I think, perhaps naively, is offsetting this issue). I really like the idea, but a \"\u00b114.0% significance threshold\" is meaningless here. The larger monthly scale should be the default, or you should get more samples. This is probably entirely down to subtle changes to CC prompts/tools. I've been using CC more or less 8 hrs/day for the past 2 weeks, and if anything it feels like CC is getting better and better at actual tasks.  Edit: Before you downvote, can you explain how the model could degrade WITHOUT changes to the prompts? Is your hypothesis that Opus 4.5, a huge static model, is somehow changing? Master system prompt changing? Safety filters changing?  Would love to see this idea expanded to ever alleged SoTA model currently in production. Any speculation as to why this degradation occurs? Simply search user prompts for curse words and then measure hostility sentiment.  User hostility rises as agents fail to meet expectations. There was a moment about a week ago where Claude went down for about an hour. And right after it came back up it was clear a lot of people had given up and were not using it. It was probably 3x faster than usual. I got more done in the next hour with it than I do in half a day usually. It was definitely a bit of a glimpse into a potential future of \u201cwhat if these things weren\u2019t resource constrained and could just fly\u201d. Wouldn't be surprised if they slowly start quantizing their models over time. Makes it easier to scale and reduce operational cost. Also makes a new release have more impact as it wil"}
{"anchor": "Hard problems that reduce to document ranking. A concept that I've been thinking about a lot lately: transforming complex problems into document ranking problems to make them easier to solve. LLMs can assist greatly here, as I demonstrated at inaugural DistrictCon this past weekend. The open source ranking library is really interesting. It's using a type of merge sort where the comparator function is an llm comparing (but doing batches >2 for fewer calls). Reducing problems to document ranking is effectively a type of test-time search - also very interesting! I wonder if this approach could be combined with GRPO to create more efficient chain of thought search...  https://github.com/BishopFox/raink?tab=readme-ov-file#descri...  Ranking (information retrieval)  https://en.wikipedia.org/wiki/Ranking_(information_retrieval...  awesome-generative-information-retrieval > Re-ranking:  https://github.com/gabriben/awesome-generative-information-r...  Very cool!  This is also one of my beliefs in building tools for research, that if you can solve the problem of predicting and ranking the top references for a given idea, then you've learned to understand a lot about problem solving and decomposing problems into their ingredients.  I've been pleasantly surprised by how well LLMs can rank relevance, compared to supervised training of a relevancy score.  I'll read the linked paper (shameless plug, here it is on my research tools site:  https://sugaku.net/oa/W4401043313/ ) Very interesting application of LLMs. Thanks for sharing! That title hurts my head to read Great article, I\u2019ve had similar findings! LLM based \u201cdocument-chunk\u201d ranking is a core feature of PaperQA2 ( https://github.com/Future-House/paper-qa ) and part of why it works so well for scientific Q&A compared to traditional embedding-ranking based RAG systems. This furthers an idea I've had recently that we (and the media) are focusing too much on creating value by making more ever more complex LLMs, and instead we ar", "positive": "UN declares that the world has entered an era of 'global water bankruptcy'.  Four billion people face severe water scarcity for at least one month each year  Does anyone know what this looks like for typical cases? The water just cuts off for a month in some places I guess? I can assure you there is plenty of water.    There are floods in lots of places every year.   The oceans are full of water that for just 5kWh we can desalinate 250 gallons. The problem is that the water and energy aren't where the users want it to be. But pipes are relatively cheap - if humanity cared enough, we could build pipes to distribute the plentiful water everywhere. But it turns out the people without much water tend to be in very poor places and warzones where there isn't much appetite for spending money on pipes. And all these huge new data centers are gonna make things worse:  https://www.eesi.org/articles/view/data-centers-and-water-co...  Before commenting water is cheap and plentiful please read the proposed definition. > Water bankruptcy refers to \u201ca state in which a human-water system has spent beyond its hydrological means for so long that it can no longer satisfy the claims upon it without inflicting unacceptable or irreversible damage to nature.\u201d I find this other article [1] more informative, including for instance the global map of Vulnerability to Water-Related Challenges taken from the actual report [2]. [1]  https://www.thebrighterside.news/post/our-world-is-entering-...  [2]  https://collections.unu.edu/eserv/UNU:10445/Global_Water_Ban...  I would no say the \"world\", but areas of it has as noted.  Like South Asia, SW N America, N Africa and Spain. For many of these areas, desalination could meet the gap, but someone will need to pay for it.  That is the main issue, no one wants to pay. Sounds like a bunch of useless scare mongering. Large scale Desalination is getting increasingly achievable:\n https://caseyhandmer.wordpress.com/2022/11/20/we-need-more-w...  Reminds me o", "negative": "Time Station Emulator. And the 2024 lateral thinking award goes to...  \u201cOne of the higher-frequency harmonics inevitably created by any real-world DAC during playback will then be the original fundamental, which should leak to the environment as a short-range radio transmission via the ad-hoc antenna formed by the physical wires and circuit traces in the audio output path.\u201d  Sometimes I think I\u2019m a smart guy\u2026and then I read of people doing shit like this. I once programmed my TI-84 calculator to do exactly this! The only missing thing was a circuit to convert the audio jack output voltage into the needed form for an antenna. I had the CS know-how but no EE know-how, so I never got it to work. It was fun to dream about confusing my high school's clocks though. (Sadly, the other obstacle was that the clocks only listened for the signal overnight, which improved their chances of detecting the weak broadcast out of far-away Colorado.) This is pretty darn cool, but I have to say I was somewhat let down by the WWVB signal. I was expecting the entire audible range instead of simply the extracted data. That being said, that's also really darn cool. I find the WWV/WWVB droning soothing somehow. Shame there's no video demonstrating it working. It's a fun idea but without a demo, I'm left wondering about the efficacy. You can apparently lock shopping trolleys using the same kind of principle -  https://www.tmplab.org/2008/06/18/consumer-b-gone/  And  https://www.youtube.com/watch?v=LmSyb0kBvGE  Whenever I see things like this, my first thought is that somebody is going to write something to mess up nearby atomic clocks, just because they can, then I think- why, why did you do this? Then someone will respond: you\u2019re just catatrophising- anyone could\u2019ve done this years before now, and I\u2019ll say no, because it wasn\u2019t up on frontpage HN there with code so that anyone would think of it. Then they\u2019ll say, well why did you tell everyone that idea then! It\u2019s your fault! Then I\u2019ll say t"}
{"anchor": "I built my own CityMapper. Before Citymapper existed, there was OneBusAway, a Ph.D. student project at the University of Washington. It still exists and powers millions of transit rider trips every day all around the world in Seattle, Washington DC, New York City, Poznan Poland, Buenos Aires Argentina, Adelaide Australia, and who knows where else. If you\u2019re interested in hacking on something like Citymapper, or setting up an OBA server for your own city, you can find everything you need on our GitHub organization:  https://github.com/OneBusAway  That includes docker images, an iOS app and a trip planner framework, android app, Sveltekit web app, and even a next generation OBA server written in Go. As far as the data to power this, you can get GTFS for every US transit agency from  https://mobilitydatabase.org/  (nb I\u2019ve been involved in the OBA project since 2012) Why are the table and the description of the RAPTOR algorithm in the article images rather than text? During university, we've built OptiTravel ( https://github.com/denysvitali/optitravel ) to do something similar. We couldn't use Google Maps APIs (project requirement), so we wrote a custom routing algorithm based on A* and I've created a Rust server to host GTFS data ( https://github.com/denysvitali/gtfs-server ) \u00e0 la Transitland ( https://transit.land/ ). Performance wasn't great since everything had to run locally and do network roundtrips, but it found routes in my hometown that Google Maps didn't show. Pretty cool discovering hidden connections in the transit network and being able to customize your own params ( https://github.com/denysvitali/optitravel/blob/master/src/ma... ) I am involved with the OpenTripPlanner project, which is a Java trip planning application that also uses the RAPTOR algorithm! It\u2019s used in cities all over the world, with the biggest deployment being ENTUR\u2019s in Norway, which covers the entire country. I believe all trip planning apps in Norway use this deployment. It supports m", "positive": "Ask HN: What did you read in 2025?. Lots of news and articles, but also \"The Craft\", a history of Freemason's by John Dickie, was one of the more interesting books. Frankenstein. Superb science fiction, very readable even though written 200 years ago. And Wuthering Heights, which strangely like Frankenstein, has a complex narrative structure and an unhinged, obsessive central character - Emperor of Rome by Mary Beard, very entertaining. - Lolita, it's mostly what you've read about it. - a few short stories by Heinrich von Kleist. I mostly read fiction but I made time for a couple of nonfiction books this year. On the fiction side I really enjoyed \"Luminous\" and \"When We Where Real\". -  https://en.wikipedia.org/wiki/Luminous_(novel)  -  https://www.simonandschuster.com/books/When-We-Were-Real/Dar...  On the nonfiction side, I can recommend \"Careless People\" and \"Apple in China\". -  https://en.wikipedia.org/wiki/Careless_People  -  https://en.wikipedia.org/wiki/Apple_in_China  I read Sad Tiger by Neige Sinno. Really unsettling but definitely worth reading. One of my favourite reads from this past year was  Infinite Powers: How Calculus Reveals the Secrets of the Universe  by Steven Strogatz. It's a wonderful review of the history of calculus, including intuitive explanations of the basics. History of the Franks, by Gregory of Tours Getting into reading again this year after a long break. The most memorable read of this year was \"The Count of Monte Cristo\" (1846) by Alexander Dumas . It's one of the greatest stories ever told. It's ~1250 pages but I sped through it in 3 weeks even if I'm a slow reader. Highly recommended! I also read The Stranger by Camus and the two top Orwells which lived up to the hype. Very much enjoyed the Hyperion Cantos series by Dan Simmons.  https://en.wikipedia.org/wiki/Hyperion_Cantos  I got really into Hemingway\u2019s work, reading all the best ones, but my favourite being \u2018A moveable feast\u2019 his diary essentially released at the end of his life", "negative": "American importers and consumers bear the cost of 2025 tariffs: analysis. > Event studies around discrete tariff shocks on Brazil (50%) and India (25\u201350%) confirm: export prices did not decline. Trade volumes collapsed instead. What if that was the intended result? who could possibly have foreseen this This is the case with any tax, it's mostly paid by the consumer. American trade policy has gone so far in the direction of Mercantilism that both the Neoliberal and the Keynesian economists can agree on something. That's not a good thing. We will see if SCOTUS majority decides tariffs are a tax or not and push the absurdity of their position even farther. I fear that they already decided that issue when they chose not to intervene and now have the excuse of \"lol well can't undo it now\" ready to go. Edit:  It appears Trump & Co intend to replace SCOTUS if they lose the tariffs ruling ...  https://www.nytimes.com/2026/01/19/us/politics/trump-tariffs...  -------- There does seem to be indications that the actual tariffs collected seems far lower than the actual tariffs promised, likely just half of what was promised:  https://www.nytimes.com/2026/01/03/business/economy/trump-ta...  Isn't this literally economics 101? How did we ever even end up imagining that tariffs are somehow paid by the exporter?? How could that possibly have not been the case.  A tariff is no different from the cost of any input into the price of a finished good.  There is some sense in which price increases are limited by supply and demand, but if the market won't pay for the production cost of the good, then the market will cease to provide that good.  There are only two possible outcomes, long term -- either the price goes up, or the product becomes unavailable. There's an argument that domestically produced goods would substitute for imported goods leaving the market, but markets are so global and intertwined now that even domestic goods have imported inputs that are also affected by tariffs, an"}
{"anchor": "TikTok users can't upload anti-ICE videos. The company blames tech issues. well\u2026 i submitted it as  https://lite.cnn.com/2026/01/26/tech/tiktok-ice-censorship-g...  but i guess HN drops the lite off of it? le sigh, here\u2019s hoping  someone  can frontpage one of these tiktok censorship stories today\u2026? The forced US hosted tik-tok sale is all about hiding information from the US public that most people in the rest of the world have easy access to. On Twitter, there's a bunch of reports that TikTok suddenly prevents people from sending the word \"Epstein\" in DMs [1]. I had expected an Orbanisation (aka, what happened to the media sphere in Hungary after Orban took over and his cronies bought up almost all media) of Tiktok, but not that fast, it's like less than a week after the deal [2]. Scary shit if you ask me, and it's made scarier by the fact that Tiktok has already been changing the way our youth speaks due to evading censorship (e.g. \"graped\" instead of \"raped\", \"unalived\" instead of kill/murder/execute/suicide). [1]  https://x.com/krassenstein/status/2015911471507530219  [2]  https://techcrunch.com/2026/01/23/heres-whats-you-should-kno...  I hope for the good of mankind,  all  sides of politics unite against deplatforming and oppressing opposing viewpoints. It's sad that certain topics (anti-ICE, Epstein) neutered on a social media platform, but this went on for years when the politics were reversed. Let everyone have their say, I say. Is it a technical glitch that prevents the uploads? Or is it a technical glitch that let's people know that that content is being censored Anecdotal to myself. I shamefully sometimes use TikTok, I particularly like recipe clips and even I noticed something in the last week, most noticeably around this weekend where the algorithm for recommendations changed. It\u2019s like they completely wiped my preferences. I try not to watch anything political so I cannot say much about censorship of content but something was noticeable in the last wee", "positive": "How far can you get in 40 minutes from each subway station in NYC?. Does it take into account the way that some subway lines run much less frequently than others? Technical term is isochrone map:  https://en.m.wikipedia.org/wiki/Isochrone_map  \"Full code for the isochrone workflow is available on\"  https://github.com/chriswhong/nyc-subway-isochrones  Jackson Heights is the winner here I'm of the opinion that every 500k+ city should have a subway line. I grew up in a city with one (just  one  for the majority of my 20 years there) and if there's one thing I miss from that place it's the ability to move at an  average  speed of 35km/h at any time the trains were operating, especially at 2am on a Friday night after a couple of beers with the guys. The other thing I noticed when I was visiting my family there during the recent holidays is that there are so few cars in places close to the subway lines - roughly half the usual concentration. The incentive to have one is just not strong enough. I love this.  I agree with the \"about\" that it's visually compelling, and I'm mesmerized. This doesn't detract from my enjoyment the site, but for trip planning I'm a little skeptical of the results around the edges, especially when I'm assuming multiple transfers would be required (e.g., Local -> Express -> Express -> Local). With a caveat this was over 10 years ago (~2012-2013), and train frequencies may have changed: I used to live pretty far up on the upper west side, and took the 1 train from the 103rd street station daily.  My weekday route was 1 -> 2/3 -> 7 into midtown.  The 20 minute radius is accurate at peak times, when it only takes 2-3 minutes to catch a transfer.  However, the website \"about\" makes an assumption this is for noon on a weekday.  I don't think I ever made it to Brooklyn in under 40 minutes. I don't see any violations of locality here. There a pretty neat free tool -  http://pedestriancatch.com  - that will also let you run walkability simulation anywhere ", "negative": "Television is 100 years old today. This is interesting.   John Logie Baird did in fact demonstrate something that looked like TV, but the technology was a dead end. Philo Farnsworth demonstrated a competing technology a few years later, but every TV today is based on his technology. So, who actually invented Television? High definition is nearly 90 years old? I guess their definition of high is quite low by more modern standards. Odd we never adapted to it. Video has a strange hypnotic power over most people and messages seem to bypass normal mental defenses. And 100 years ago my great-aunt and grandmother (both RIP) were little kids and my great-grandmother, born in the 19th century and which I knew very well for she lived until 99 years old, was filming them playing on the beach using a \"Pathe Baby\" hand camera. I still have the reels, they look like this:  https://commons.wikimedia.org/wiki/File:Films_Path%C3%A9-Bab...   https://fr.wikipedia.org/wiki/Path%C3%A9-Baby  And we converted some of these reels to digital files (well brothers and I asked a specialized company to \"digitalize\" them). 100 years ago people already had cars, tramways (as a kid my great-grandmother tried to look under the first tramway she saw to see \"where the horses were hiding\"), cameras to film movies, telephones, the telegraph existed, you could trade the stock market and, well, it's knew to me but TV was just invented too. Inspired one of my absolute favorite Zappa grooves. I am the Slime  https://www.youtube.com/watch?v=iiCQcEW98OY  I am gross and perverted I'm obsessed and deranged I have existed for years But very little has changed I'm the tool of the Government And industry too For I am destined to rule And regulate you I may be vile and pernicious But you can't look away I make you think I'm delicious With the stuff that I say I'm the best you can get Have you guessed me yet? I'm the slime oozin' out From your TV set You will obey me while I lead you And eat the garbage that I feed"}
{"anchor": "The universal weight subspace hypothesis. They compressed the compression? Or identified an embedding that can \"bootstrap\" training with a headstart ? Not a technical person just trying to put it in other words. What's the relationship with the Platonic Representation Hypothesis? interesting.. this could make training much faster if there\u2019s a universal low dimensional space that models naturally converge into, since you could initialize or constrain training inside that space instead of spending massive compute rediscovering it from scratch every time I find myself wanting genetic algorithms to be applied to try to develop and improve these structures... But I always want Genetic Algorithms to show up in any discussion about neural networks... I immediately started thinking that if there are such patterns maybe they capture something about the deeper structure of the universe. (Finds a compression artifact) \"Is this the meaning of consciousness???\" They are analyzing models trained on classification tasks.  At the end of the day, classification is about (a) engineering features that separate the classes and (b) finding a way to represent the boundary.  It's not surprising to me that they would find these models can be described using a small number of dimensions and that they would observe similar structure across classification problems.  The number of dimensions needed is basically a function of the number of classes.  Embeddings in 1 dimension can linearly separate 2 classes, 2 dimensions can linearly separate 4 classes, 3 dimensions can linearly separate 8 classes, etc. Would you see a lower rank subspace if the learned weights were just random vectors? Interesting - I wonder if this ties into the Platonic Space Hypothesis recently being championed by computational biologist Mike Levin E.g  https://youtu.be/Qp0rCU49lMs?si=UXbSBD3Xxpy9e3uY   https://thoughtforms.life/symposium-on-the-platonic-space/  e.g see this paper on Universal Embeddings\n https://arxiv.org/h", "positive": "Training my smartwatch to track intelligence. I've tracked sleep using a number of devices and algorithms and I haven't found a single one that regularly aligns with what and how I feel. I know it's tracking real data, but the conclusions feel completely made up. What are other people's experience -- especially from those who are more bullish about sleep tracking? I hope Garmin sees your passion project and greenlights it for inclusion. You have the right approach to ensuring folks are at their optimal health to grow intellectually as a person. > Often, it would also contradict how I was internally feeling. I\u2019d wake up feeling rested, see my stats are low, and play a game of chess out of algorithmic rebellion, only to feel my mind up against a barrier and handedly lose. It would be better to only look at the stats after playing if you want to verify it, this could easily be a self-fulfilling prophecy. The biggest thing for me is I don't understand how people can sleep with these watches on, it's so uncomfortable to me personally which is why the different ring technologies appeal to me more. I just wish either Garmin made one or that there was one I didn't have to buy a subscription to use. I didn't believe the stress numbers on my Garmin watch were very meaningful until I started taking Nebivolol (an atypical beta blocker) because there were so many gaps (even when I was sitting) that I didn't feel I could eyeball them or trust averages over time. Taking that drug,  however,  it sees far fewer gaps and I show up in the blue \"rest\" zone most of the time. I've been watching my heart rate a lot in the last month part because of health concerns and part because of a new stance I am practicing that has a physical component (e.g. adjusted gaits that are energy efficient) and a mental component, being an oceanic reservoir of calm with close mind-body-environment coupling 95% of the time but disconnecting that connection under peak stress -- like I am standing between two ", "negative": "Does running wear out the bodies of professionals and amateurs alike?. I mean the consensus is that to improve you kinda have to put a certain level of unusually high stress on your body. While amateurs do it at a lower level, amateurs likely like the genetics that makes increases resistance to slows down the wearing process and also lack regular constant monitoring of exercise intensity, frequency, rests, diet etc. I've been lucky to run fourteen years so far without any injuries at all, starting when I was twenty-seven. I don't train \"for\" anything, though, other than maintaining my own fitness. I just do my 10K three times a week and that's good enough for me. reading some popular media from around 1900 in the USA, it seemed to be a common perception that people who trained for track and field events generally expect a short life somehow. Nursing a sore right Achilles as I read this... Funny two weekends ago I watched a woman set a world record for the mile for women 80-85. I think that supercompensation is not as strong an effect in amateurs and that this is the #1 driver of injury. I've long suspected there is a range of exertion that is net negative with regard to injury risk. No exercise at all means no exercise related risk. However, I strongly disagree that an extreme amount of exercise is the riskiest. I think the most dangerous level of exertion sits right in the middle somewhere. That special zone where you are grinding down your bones a bit but your hormones and other compensation mechanisms don't react accordingly because you aren't going quite hard enough. It depends on the intensity level. In the pandemic of 2020, I ran 26.2 mile runs at an easy effort for 5 weekends in row in the spring and 6 in the fall, generally with my easy effort getting faster as I went. Rather than the sequence of big runs wearing my out, I was getting stronger. Now, if I had tried to run every one at \"race pace\", I would likely be trashed or injured by the end from insuffici"}
{"anchor": "OpenClaw \u2013 Moltbot Renamed Again. Previously:  Clawdbot Renames to Moltbot   https://news.ycombinator.com/item?id=46783863  Right now I'm just thinking about all the molt* domains..... \u00af\\_(\u30c4)_/\u00af I would have stood my ground on the first name longer. Make these legal teams do some actual work to prove they are serious. Wait until you have no other option. A polite request is just that. You can happily ignore these. The 2nd name change is just inexcusable. It's hard to take a project seriously when a random asshole on Twitter can provoke a name change like this. Leads me to believe that identity is more important than purpose. and openclaw.com is a law firm. It's hilarious that atm I see \"Moltbook\" at the top of HN. And it is actually not Moltbot anymore? But I have to admit that OpenClaw sounds much better. This is indeed feeling very much like Accelerando\u2019s particular brand of unchecked chaos. Loving every minute of it, first thing in our timeline that makes sense where it regards AI for the masses :) What if Lamborghini had acquired Claw to automate their vehicles? amateur hour, new phase of the AI bubble reminds me of Andre Conje, cracked dev, \"builds in public\", absolutely abysmal at comms, and forgets to make money off of his projects that everyone else is making money off of (all good if that last point isn't a priority, but its interrelated to why people want consistent things) Should have named it \u201cbot formerly known as Moltbot\u201d and invented a new emoji sigil :) Apparently it had another name before Clawdbot as well, I think BotRelay or something. It\u2019s on pragmatic engineer Hilarious to see the most pointless vibecoded slop written to interact with an RDP server. Unnecessary introduces loopholes. How to annoy and alienate your target audience in 2 short weeks. Before using make sure you read this entirely and understand it:\n https://docs.openclaw.ai/gateway/security \nMost important sentence: \"Note: sandboxing is opt-in. If sandbox mode is off\"\nDon't do that, ", "positive": "Ask HN: What's the Point Anymore?. No point, buy tinned food and head for the darkest part of the forest. Do you read a book just so you know what happens at the end, or because you like the journey there too? Do you read blog posts \"just to know\" or because you like reading? Sure, if you don't like reading, then it's great you don't have to. But personally I like to read, and be taken on an adventure by writers, that's why I read, I don't read just so I \"know what happened\". So everything remains the same, nothing has changed. Nothing been destroyed by AI, it only seems to have destroyed your own perspective. The point is to cultivate the ability to distinguish between real and fake.  Soon enough, that ability will be extremely rare, and for the people who really need it, nothing else will do. Sounds like you're getting burned out by too much hype-chasing. Follow your interests, and you'll always discover something that AI hasn't solved by itself. And keep in mind that people have always had these concerns whenever something new came along - photography, computers, etc. > Why read a blog post, when Google AI Summary can just give you the summary? Because the summary is often wrong, and the summary might not even be the point? > Why read a book, when you can just get AI summary of it? You've been able to read a good summary by a human for most books on Wikipedia for decades now. Going to the example used thousands of times, maybe the horse drivers thought the same way, but guess what? now we have cars, race cars, super cars, flying cars. The engine kept changing, car markets kept evolving. \nPeople kept adapting. \nAdapting is the only way or the Penguin way :P Imho there are still tasks that can't be done by AI good enough. Wouldn't let clawbot handle my personal relationships. Not even scheduling a football [or dota2] game. Yet alone navigate job. So, maybe level up the goal post? Try do something not-easily-done by AI? Select from your fringe interests [if core is ", "negative": "We Do Not Support Opt-Out Forms (2025). That site doesn't seem to support pages loading either. edit: I feel their pain - I've spent the past week fighting AI scrapers on multiple sites hitting routes that somehow bypass Cloudflare's cache. Thousands of requests per minute, often to URLs that have  never  even existed. Baidu and OpenAI, I'm looking at you. Archive link:  https://web.archive.org/web/20251009081648/https://conscious...  | Since emails are sent from the individual\u2019s email account, they are already verified. This is not how email works, though.  https://archive.ph/QCMjJ  if it helps The irony of a site about AI opt-outs getting hammered by AI scrapers is almost too on the nose. trollbridge's point about scrapers using residential IPs and targeting authentication endpoints matches what we've seen. The scrapers have gotten sophisticated. They're not just crawling, they're probing. The economics are broken. Running a small site used to cost almost nothing. Now you need to either pay for CDN/protection or spend time playing whack-a-mole with bad actors. ronsor hosting a front-page HN project on 32MB RAM is impressive and also highlights how much bloat we've normalized. The scraper problem is real, but so is the software efficiency problem. It\u2019s wild when I read a professional looking website like this and Conscious Digital misspells their own org name as \u201cConsious Digital\u201d in the first paragraph. I\u2019m glad they\u2019re fighting against email spam but it just raises all sorts of red flags in my mind, or at least it used to. Funny enough, these days it indicates the article was written by a human. I had a dev join my team and made a few typos and it gave me a chuckle, as it\u2019s a whole class of mistake I hadn\u2019t seen in awhile. The \"required login\" pattern is particularly a problem. I seem to have namesakes around the US and UK that use my email address as their own when signing up for various services (mobile phone services, Shopify, Uber, various banks and investmen"}
{"anchor": "Canada. So you say the highs aren\u2019t as high and the lows certainly aren\u2019t as low as in the US.  Given that few really experience the highs, I think the Canadian choice is correct.  It\u2019s a stereotype, but the people do tend to be friendlier and the pace is slower.  But I\u2019ve found that the quality of work is a function of one\u2019s inner makeup not the external environment.  We\u2019ll see what the next 5-10 years looks like in N America. I was not born in Canada, but I chose to immigrate here and it's one of the top 5 best choices I've ever made. I have access to so much that in other places would be wildly expensive. My life is richer due to the diversity of the people I am surrounded by, if I bought every book I borrowed from the library last year it would have cost $3000 or more, and even after moving away from a large city I have access to public transit good enough to cover most of my needs. It's actually really wild to think I spent a couple of years working in Boston more than a decade ago, and I used my zipcar subscription way more often than I've ever had to use a communauto in fake london (a city no one would mistake for having good urban planning). The Canada the author refers to is gone. Canada is far from perfect but there is no other country in the world I would ever consider leaving it behind for. I grew up in Canada and live in the US now with kids. The US is not one country. It's two that are radically different. There's wealthy America. The top 5% to 10% that have healthcare, have their own safety nets, don't need to worry about money, their kids go to select schools that they can buy into (mostly by buying into the right neighborhoods), an amazing pension plan, etc. My kids go to a fancy library with reading time, puppets and classical music. All the things I love about Canada and more. That country is amazing and the quality of life is unparalleled unless you're obscenely wealthy. The bottom 80 to 90% percent of Americans live a life that is far inferior t", "positive": "Gemini Embedding: Powering RAG and context engineering. The Matryoshka embeddings seem interesting: > The Gemini embedding model, gemini-embedding-001, is trained using the Matryoshka Representation Learning (MRL) technique which teaches a model to learn high-dimensional embeddings that have initial segments (or prefixes) which are also useful, simpler versions of the same data. Use the output_dimensionality parameter to control the size of the output embedding vector. Selecting a smaller output dimensionality can save storage space and increase computational efficiency for downstream applications, while sacrificing little in terms of quality. By default, it outputs a 3072-dimensional embedding, but you can truncate it to a smaller size without losing quality to save storage space. We recommend using 768, 1536, or 3072 output dimensions. [0] looks like even the 256-dim embeddings perform really well. [0]:  https://ai.google.dev/gemini-api/docs/embeddings#quality-for...  To anyone working in these types of applications, are embeddings still worth it compared to agentic search for text? If I have a directory of text files, for example, is it better to save all of their embeddings in a VDB and use that, or are LLMs now good enough that I can just let them use ripgrep or something to search for themselves? Question to other GCP users, how are you finding Google's aggressive deprecation of older embedding models? Feels like you have to pay to rerun your data through every 12 months. I feel like tool calling killed RAG, however you have less control over how the retrieved data is injected in the context. > Embeddings are crucial here, as they efficiently identify and integrate vital information\u2014like documents, conversation history, and tool definitions\u2014directly into a model's working memory. I feel like I'm falling behind here, but can someone explain this to me? My high-level view of embedding is that I send some text to the provider, they tokenize the text and then run ", "negative": "Nvidia's 10-year effort to make the Shield TV the most updated Android device. Now if only they would release an updated one. honestly, shield tv changed how i interact with my tv and my opinion about Android TV (even though its market sucks) It's ironic that Nvidia before becoming a behemoth had the money for this kind of device. I have had two for 10 years and have no complaints whatsoever I've got the OG model, and it's still the main device hooked up to my TV. All my TV streaming goes through it (mostly Jellyfin these days), and it can stream games no problem via Moonlight. It's hooked up to a 4k LG TV, and I have no idea about how it does the upscaling, but 720p content looks perfectly fine on it. Best (worst?) of all... it still gets updates. Shield TV + extra storage + HDHomeRun tuner is still a great device for getting OTA TV. The only downside is that more recent versions use the Google Android TV launcher which is filled with a garbage truck full of ads, often for things I would never want to watch (horror movies? Nope!).  Yes you can replace the launcher, but that's a pain. Would love to pay more for a device that has updated codec support, no ads or tracking, and is basically identical. My shield bricked itself after just a few months, so YMMV on this. No rhyme or reason why. Dupe of  https://news.ycombinator.com/item?id=46824003  What a new model would need is more compat with DV and better software. Get rid of the Android ads. Add better frame rate matching. Etc. The Shield TV's cylindrical form factor could use a rethink. It is hard to find a good spot for it on a shelf when cords are connected at both ends (HDMI and MMC slot at one end, power and LAN at the other) and the ports are too close for all cords to use right-angle-heads. Leaving it invisible by placing it on the floor or behind other gear sometimes impedes Bluetooth signal, so there it sits, well apart from the AVR, BD, other devices. My Nvidia Shield Portable is sad to hear this.  They upd"}
{"anchor": "What's the strongest AI model you can train on a laptop in five minutes?. Perhaps grimlock level:  https://m.youtube.com/shorts/4qN17uCN2Pg  Instead of time it should be energy. What is the best model you can train with a given budget in Joules. Then the MBP and the H100 are on a more even footing. I love seeing explorations like this, which highlight that easily accessible hardware can do better than most people think with modern architectures. For many novel scientific tasks, you really don't need an H100 to make progress using deep learning over classical methods. I suspect one can go a lot further by adopting some tweaks from the GPT-2 speedrun effort [0], at minimum Muon, better init and carefully tuning learning rate. [0]:  https://github.com/KellerJordan/modded-nanogpt  But supposing you have a real specific need to train, is the training speed still relevant? Or do the resources spent on gathering and validating the data set dwarf the actual CPU/GPU usage? The most powerful Macbook Pro currently has 16 CPU cores, 40 GPU cores, and 128 GB of RAM (and a 16-core \u201cneural engine\u201d specifically designed to accelerate machine learning). Technically, it is a laptop, but it could just as well be a computer optimized for AI. > Paris, France is a city in North Carolina. It is the capital of North Carolina, which is officially major people in Bhugh and Pennhy. The American Council Mastlandan, is the city of Retrea. There are different islands, and the city of Hawkeler: Law is the most famous city in The Confederate. The country is Guate. I love the phrase \"officially major people\"! I wonder how it could be put to use in everyday speech? Not the point of the exercise obviously, but at five minutes' training I wonder how this would compare to a Markov chain bot. Any reason to upgrade an M2 16GB macbook to a M4 ..GB (or 2026 M5) for local LLMs? Due an upgrade soon and perhaps it is educational to run these things more easily locally? You could train an unbeatable tic-tac-to", "positive": "Show HN: Only 1 LLM can fly a drone. Why would you want an LLM to fly a drone? Seems like the wrong tool for the job -- it's like saying \"Only one power drill can pound roofing nails\". Maybe that's true, but just get a hammer LLMs flying weaponized drones is exactly how it starts. I think it's fascinating work even if LLMs aren't the ideal tool for this job right now. There were some experiments with embodied LLMs on the front page recently (e.g. basic robot body + task) and SOTA models struggled with that too. And of course they would - what training data is there for embodying a random device with arbitrary controls and feedback? They have to lean on the \"general\" aspects of their intelligence which is still improving. With dedicated embodiment training and an even tighter/faster feedback loop, I don't see why an LLM couldn't successfully pilot a drone. I'm sure some will still fall of the rails, but software guardrails could help by preventing certain maneuvers. I am curious how these models would perform and how much energy they'd take to semi-realtime detect objects:\nSmolVLM2-500M - Moondream 0.5B/2B/2.5B - Qwen3-VL (3B)\n https://huggingface.co/collections/Qwen/qwen3-vl  I am sure this is already worked on in Russia, Ukraine and The Netherlands. A lot can go wrong with autonomous flying.\nOne could load the VLM on a high end android phone on the drone and have dual control. Gemini 3 is the only model I've found that can reason spatially. The results here are accurate to my experiments with putting LLM NPCs in simulated worlds. I was surprised that most VLLMs cannot reliably tell if a character is facing left or right, they will confidently lie no matter what you do (even gemini 3 cannot do it reliably). I guess it's just not in the training data. That said Qwen3VL models are smaller/faster and better \"spatially grounded\" in pixel space, because pixel coordinates are encoded in the tokens. So you can use them for detecting things in the scene, and where they are ", "negative": "The engineer who invented the Mars rover suspension in his garage [video]. Best not read the comments until you've watched at least the first four minutes of the video. \"There are no shortcuts to expertise\". What a fantastic post this. Not surprised of such article. It's not the first time something important is built in a garage: for example, the Apollo 11 lander; a lot of people were thinking it was made from aluminum folio and cardboard in a garage, but actually it was kapton folio and professional-grade cardboard. This guy has incredible videos on hiking gear, examining common claims scientifically and rationally. He never gave any hints as to his professional background, so as not to taint his arguments with appeals to authority. It makes perfect sense that he grew up in this environment, doing engineering work for NASA as a kid! Cool! I just popped in to add that NASA employee Charles White, a scientist involved with the Mars Rover project, also helped make a Burning Man Mars Rover Car (back before Playa Burning Man was completely and utterly torched twice over by Military Industrial Complex Vacationers and Billionaires) and you can hear an interview with him here on Charles White's yt channel:  https://youtu.be/BKGROOedAgI  (\nMars Rover Art Car interview with Ray Cirino and Charles White ) Charles White is a pretty good guy in my opinion, we play the same video game (EvE: Online) Where Charles White is a very, very well known community member who is known as \"The Space Pope\". He officiates weddings at our Iceland Fanfest gathering and also runs a Suicide Prevention Outreach group in EvE: Online, as well as teaching leadership skills. Here's Charles White giving a presentation as an Official NASA employee about Space and our solar system at EvE Fanfest 2016: \n https://www.youtube.com/watch?v=Atm6Y_JYPEU  Heres a interview about EvE: Online with the Space Pope:  https://www.youtube.com/watch?v=dWuj7LfyN4U  anyhow sorry to hijack this about EvE: Online but we ha"}
{"anchor": "Show HN: I visualized the entire history of Citi Bike in the browser. How was the data gathered? They just publicly show the bike's locations? I've seen many visualizations of the citibike data over the years, this is one of the most charismatic for sure! Interesting that citibike publishes trip level data.  The bike share schemes in Dublin only publish station counts or free bike locations.  So you can see the overall pattern of bike motion, but there\u2019s no way to see how many north side trips go to the docks vs Heuston station vs the city center. This is just so cool! Not much more to add. Thanks a lot for sharing!! Great work :) How is MapBox going for this free tool? Is it costing you money? It's often interesting to observe the different ways that privacy is approached in the US and Europe. In Europe we often accept pretty grave restrictions of our liberty like the UK's Online Safety Act, which would never fly in the US, and we do so without much public comment. On the other side of things, organisations in the US happily expose datasets like this one, which would give a most EU Data Protection Officers a heart attack, and nobody bats an eyelid. Relevant callout from  https://bikemap.nyc/about : * Limitations * The data only contains the start and end station for each trip, but does not contain the full path. Route geometries are computed for each (start station, end station) pair using the shortest path from OSRM. This means that the computed routes are directionally correct but inexact. Trips that start and end at the same station are filtered out since the route geometry is ambiguous. non corrupted github link:  https://github.com/freeman-jiang/bikemap.nyc  Cool visualization. Do you find the OSRM shortest path routes probable for bikes? Not living in NYC, I expected pretty different paths. Say the \"Hudson River Greenway\" or whatever that's called. I really wish Lyft invested in maintenance. I used Citibike this week for the first time in about a year, and th", "positive": "28M Hacker News comments as vector embedding search dataset. Oh to have had a delete account/comments option. I've been embedding all HN comments since 2023 from BigQuery and hosting at  https://hn.fiodorov.es  Source is at  https://github.com/afiodorov/hn-search  Am I misunderstanding what a parquet file is, or are all of the HN posts along with the embedding metadata a total of 55GB? I know it's unrelated but does anyone knows a good paper comparing vector searches vs \"normal\" full text search? Sometimes I ask myself of the squeeze worth the juice Scratches off one of my todos, I think it would be useful to add a right-click menu option to HN content, like \"similar sentences\", which displays a list of links to them. I wonder if it would tell me that this suggestion has been made before. Finetune LLM to post_score -> high quality slop generator I don't remember licensing my HN comments for 3rd party processing. Maybe I\u2019m reading this wrong, but commercial use of comments is prohibited by the HN Privacy and data Policy. So is creating derivative works (so technically a vector representation) I don't know how to feel about this. Is the only purpose of the comments here is to train some commercial model? I have a feeling that, this might affect my involvement here going forward. Don't use all-MiniLM-L6-v2 for new vector embeddings datasets. Yes, it's the open-weights embedding model used in all the tutorials and it  was  the most pragmatic model to use in sentence-transformers when vector stores were in their infancy, but it's old and does not implement the newest advances in architectures and data training pipelines, and it has a low context length of 512 when embedding models can do 2k+ with even more efficient tokenizers. For open-weights, I would recommend EmbeddingGemma ( https://huggingface.co/google/embeddinggemma-300m ) instead which has incredible benchmarks and a 2k context window: although it's larger/slower to encode, the payoff is worth it. For a compromi", "negative": "Ask HN: What's the current best local/open speech-to-speech setup?. It was a little annoying getting old qt5 tools installed but I really enjoyed using dsnote / Speech Note. Huge model selection for my amd gpu. Good tool. I haven't done enough specific studying yet to give you suggestions for which model to go with. WhisperFlow is very popular. Kyutai some very interesting work always. Their delayed streams work is bleeding edge & sounds very promising especially for low latency. Not sure why I have not yet tried it tbh.  https://github.com/kyutai-labs/delayed-streams-modeling  There's also a really nice elegant simple app Handy. Only supports Whisper and Parakeet V3 but nice app & those are amazing models.  https://github.com/cjpais/Handy  You should look into the new Nvidia model:  https://research.nvidia.com/labs/adlr/personaplex/  It has dual channel input / output and a very permissible license Anyone using any reasonably good small speech to text os models? For the TTS part:  https://github.com/supertone-inc/supertonic  It requires a bit of tinkering, but I think pipecat is the way to go.  You can plug in pretty much any STT/LLM/TTS you want and go.  It definitely supports local models but its up to you to get your hands on those models. Not sure if there's any turnkey setups that are preconfigured for local install where you can just press play and go though. Last I heard E2E speech to speech models are still pretty weak.  I've had pretty bad results from gpt-realtime and that's a proprietary model, I'm assuming open source is a bit behind.  https://handy.computer  got good marks from a  very  nontechnical user in my life this week! Local, FOSS Tangential: What hardware are you using for the interface on these?  Is there a good array microphone that performs on par with echos/ghomes/homepods? I have used  https://github.com/SaynaAI/sayna  . What I like the most is that you can switch between the providers easily and see what works for you the best. It also su"}
{"anchor": "Functional programming and reliability: ADTs, safety, critical infrastructure. This article seems to conflate strong type systems with functional programming, except in point 8. It makes sense why- OCaml and Haskell are functional and were early proponents of these type systems. But, languages like Racket don\u2019t have these type systems and the article doesn\u2019t do anything to explain why they are _also_ better for reliability. >In banking, telecom, and payments, reliability is not a nice to have. It is table stakes. This reliability isn't done by being perfect 100% of the time. Things like being able to handle states where transactions don't line up allowing for payments to eventually be settled. Or for telecom allowing for single parts of the system to not take down the whole thing or adding redundancy. Essentially these types of businesses require fault tolerance to be supported. The real world is messy, there is always going to be faults, so investing heavily into correctness may not be worth it compared to investing into fault tollerance. I'm wary of absolute statements about programming. I like good type systems, too, but they won't save you from bugs that are better addressed by fuzz testing, fault injection testing and adversarial mindset shifts. Strong types: yes, it\u2019s definitely better Functional programming: no, functional programming as in: the final program consists in piping functions together and calling the pipe. In my opinion, that tends to get in the way of complex error handling. The problem being that raising Exceptions at a deep level and catching them at some higher level is not pure functional programming. So your code has to deal with all the cases. It is more reliable if you can do it, but large systems have way too many failure points to be able to handle them all in a way that is practical. I think there is a strong case that ADTs (algebraic data types) aren't so great after all. Specifically, the \"tagged\" unions of ADT languages like Haskell ", "positive": "Show HN: Exploring Mathematics with Python. Looks like a treasure trove of knowledge. I just recently went to the exploratorium in SF and saw an exhibit there suggesting that the catenary made a good arch, so browsed that chapter and saw a bit of explanation here which helped.  Was also interested to see that Jefferson played some part in the history here. Very Nice! However, I don't see the entire book as a single pdf? I own the original Exploring Mathematics with Your Computer(Turbo Pascal version).\nIt\u2019s an excellent introduction to algorithms for people coming from a mathematics background.\nReally happy to see it revived in Python. Very nice.  I was looking for something fun to work on over the break.  Thank you for this. > Unfortunately, after lengthy discussions with the MAA, my hopes of publishing this (rather large) expansion have proved impossible, and so I've decided to put it online, hopefully to be of use to others. Too bad It is painful to imagine how these fantastic works will be not be read by humans in future, as AI would digest all this and provide just-in-time code for humans. Joint author here. I plan to upload the entire book as a single PDF when I finish the next chapter (on the cycloid). That will probably be early next week. I used the original book by Arthur Engel for many years. He was an inspirational teacher. The MAA tried very hard to publish the book, but I kept adding new material, and  a text consisting of math 'selections' rather than a single theme is a hard sell in today's publishing environment. Random thoughts: - Seems great. Added to the backlog :) - No colors in PDF illustrations. Is it a deliberate choice? - > The first six chapters (and Appendix A) are essentially that book, but with the programming language changed to Python, some rewording, reformatting in Latex, and a few additions.     Try [typst](https://typst.app/) as an alternative to Latex.   This is an excellent resource for building mathematical intuition through code", "negative": "Tesla unsupervised Robotaxis are nowhere to be found. I don't think actual unsupervised robotaxis exist, given the reports that they're just having the supervisor follow in a chase car[1]. [1]  https://futurism.com/advanced-transport/car-following-tesla-...  This should not be surprising to anyone who pays any attention to Elon Musk's  \u0336l\u0336i\u0336e\u0336s\u0336 , er... \"predictions\" And yet TSLA sits comfortably at ~$450. If someone knowledgeable can explain this to me, I'd be very grateful. Maintaining a meme stock is hard, really hard.  You do have to hand it to the bloke that he is working hard on this. Back in the day, the term \"snake oil salesman\" was used and it is as fresh today as it always was. There\u2019s no consequences to Musk not delivering and simply making up bullshit. I just saw a LinkedIn post from someone totally unrelated to Musk, or Tesla fawning about how amazing the Tesla Optimus robots are, how they are going to operate in space and how he would prefer one to give him surgery over a doctor. 100s of positive interactions followed Humans seem to need some fiction to believe to get them through their day. So as long as people don\u2019t demand that reality is the driver of their future they will continue to live in whatever fantasy world that makes them the main character There are only around 50[0] unique vehicles operating in Austin (not all operating at the same time) and initially only about 3 are operating \"with no safety monitor in the car.\" Based on social media posts it seems they all have chaser vehicles. [0]  https://robotaxitracker.com  JerryRigEverything randomly started dissing Tesla's FSD system two days before he posts a sponsored video for Ford's self-driving feature. It's probably a good thing they are doing this ultra-conservative rollout of robotaxi. No amount of failed promises, missed deadlines or just plain lies is going to dampen the stock, it\u2019s just the way it is with this. Staying away is the best one can do. HN is so fucked at this point. For th"}
{"anchor": "How far can you get in 40 minutes from each subway station in NYC?. Does it take into account the way that some subway lines run much less frequently than others? Technical term is isochrone map:  https://en.m.wikipedia.org/wiki/Isochrone_map  \"Full code for the isochrone workflow is available on\"  https://github.com/chriswhong/nyc-subway-isochrones  Jackson Heights is the winner here I'm of the opinion that every 500k+ city should have a subway line. I grew up in a city with one (just  one  for the majority of my 20 years there) and if there's one thing I miss from that place it's the ability to move at an  average  speed of 35km/h at any time the trains were operating, especially at 2am on a Friday night after a couple of beers with the guys. The other thing I noticed when I was visiting my family there during the recent holidays is that there are so few cars in places close to the subway lines - roughly half the usual concentration. The incentive to have one is just not strong enough. I love this.  I agree with the \"about\" that it's visually compelling, and I'm mesmerized. This doesn't detract from my enjoyment the site, but for trip planning I'm a little skeptical of the results around the edges, especially when I'm assuming multiple transfers would be required (e.g., Local -> Express -> Express -> Local). With a caveat this was over 10 years ago (~2012-2013), and train frequencies may have changed: I used to live pretty far up on the upper west side, and took the 1 train from the 103rd street station daily.  My weekday route was 1 -> 2/3 -> 7 into midtown.  The 20 minute radius is accurate at peak times, when it only takes 2-3 minutes to catch a transfer.  However, the website \"about\" makes an assumption this is for noon on a weekday.  I don't think I ever made it to Brooklyn in under 40 minutes. I don't see any violations of locality here. There a pretty neat free tool -  http://pedestriancatch.com  - that will also let you run walkability simulation anywhere ", "positive": "The '3.5% rule': How a small minority can change the world (2019). This rule didn't hold in Israel in the last 3 years. Well over 3.5% went to the streets and the government remains in tact. This is plausible. Non violent groups will often have wider public support (because most people would prefer not to support violence) and if those in power use violence against the non-violent it increases public sympathy for them. Iran proved it wrong (the regime mobilized roughly 1% of the country's population to crack down on protesters) with regards to Single Party Regimes, and knowing people at the Ash Center, they are pessimistic about this as well. If you have 2+ groups with opposing views, each 3.5%+ it's pretty clear that at least one of the 3.5%+ groups will fail. Others here note it's really \"3.5% if there's no one seriously opposing their objectives\" but in my opinion that's a meaningless rule. Of course in those cases non-conflict resolves the issue.  https://medium.com/incerto/the-most-intolerant-wins-the-dict...  (2019) Chenoweth has backed off her previous conclusions in recent years, observing that nonviolent protest strategies have dramatically declined in effectiveness as governments have adjusted their tactics of repression and messaging. See eg  https://www.harvardmagazine.com/2025/07/erica-chenoweth-demo...  One current example of messaging can be seen in the reflexive dismissal  by the current US government and its propagandists of any popular opposition as 'paid protesters'. Large attendance at Democratic political rallies during the 2024 election was dismissed as being paid for by the campaign, any crowd protesting government policy is described as either a rioting or alleged to be financed by George Soros or some other boogeyman of the right. This has been going on for years; the right simply refuses to countenance the possibility of legitimate organic opposition, while also being chronically unable to provide any evidence for their claims. Hong Kong pr", "negative": "The microstructure of wealth transfer in prediction markets. tl;dr dataset: 72.1m trades and $18.26b volume on kalshi (2021-2025) core findings: longshot bias: well documented longshot bias is present on kalshi. low probability contracts are systematically overpriced. contracts trading at 5 cents only win 4.18% of the time. wealth transfer: liquidity takers lose money (-1.12% excess return) while liquidity makers earn it (+1.12%). optimism tax: the losses are driven by a preference for \"yes\" outcomes. buying \"yes\" at 1 cent has a -41% expected value. buying \"no\" at 1 cent has a +23% expected value. category variation: finance markets are efficient (0.17% maker-taker gap) while high-engagement categories like media and world events are inefficient (>7% gap). mechanism: makers do not win by out-forecasting takers. they win by passively selling \"yes\" contracts to optimistic bettors I'm a little confused by the \"Yes\" versus \"No\" asymmetry. For example, one of the top trending ~~bets~~ markets right now is on whether Miami or Indiana will win the NCAA football championship tonight. You can either take \"Yes\" on Indiana at 74c, or \"No\" at 27c, or you can take \"Yes\" on Miami at 27c or \"No\" at 74c. Or, there's another potential outcome - you can also bet on a tie at 10c yes/91c no. Is this research suggesting that an optimistic Miami fan can somehow get a better return by buying \"No\" on Indiana than a \"Yes\" on Miami? Why is Kalshi structured with these yes vs. no options for all outcomes? How do prediction markets account for interest rates? I feel like I should be willing to pay no more than ~96 cents for a contract that will definitely resolve to a dollar in a year. Who puts up the other 4 cents? I wonder how much of the activity on prediction markets these days is competing LLM scripts? I would guess the overlap in prediction market punters and AI boomers is high. This article lacks even the most basic understanding of probability and statistics. Slot machines \"93 cents o"}
{"anchor": "My stages of learning to be a socially normal person. I don't have much to add to this right now other than to say this is really fantastic writing. I don't normally enjoy \"my journey\" kind of blog posts, but this one feels full of valuable insights, and I'm grateful to the author for sharing. It's also just nice to read something written by a skilled writer. I wish I had the drive to do as much work as the author has. Instead I will live more or less where I am now, stably in social mediocrity, perpetually somewhat impedance mismatched with the people around me. really identify. especially with the early yearning to connect and not having the skills. Learned sooo much over the years by being brutally rejected and eventually taking stock of what happened and extracting a rule or two. but then, yeah, next phase, rules don't matter (except when they do) and change moment to moment anyway. funny to read this here on hacker news of all places, where I let my carefully managed, almost always inhibited, childhood nerd self fly free in the comments. OP has definitely gone beyond me in many ways, with his talk about embodiment, and being able to be so empathic that he has elicited tears of gratitude. Enviable. >  I was probably the most severely bullied kid at my school. >  I was demonstrating my erudition Those two things might have been linked. I wasn't there, but I'm suspicious. Fortunately the author learns better by the end of the article, but it stuck out to me because LLMs have made people suspicious of five dollar words like delve so to use the word erudition in this day and age is a choice. Appreciate the writing and the author's fortitude in achieving their goals. While I never had friends, neither online nor in person, I cannot identify with this at all - it reads like a strange, obsessive seeking of external validation which I have never felt myself. Maybe I am just disinterested in people in general. I eat at Chinese restaurants where my waiter is a QR code. Pl", "positive": "2025: The Year in LLMs. These are excellent every year, thank you for all the wonderful work you do. Remember, back in the day, when a year of progress was like, oh, they voted to add some syntactic sugar to Java... > Vendor-independent options include GitHub Copilot CLI, Amp, OpenHands CLI, and Pi ...and the best of them all, OpenCode[1] :) [1]:  https://opencode.ai  > The (only?) year of MCP I like to believe, but MCP is quickly turning into an enterprise thing so I think it will stick around for good. Great summary of the year in LLMs. Is there a predictions (for 2026) blogpost as well? > The year of YOLO and the Normalization of Deviance # On this including AI agents deleting home folders, I was able to run agents in Firejail by isolating vscode (Most of my agents are vscode based ones, like Kilo Code). I wrote a little guide on how I did it  https://softwareengineeringstandard.com/2025/12/15/ai-agents...  Took a bit of tweaking, vscode crashing a bunch of times with not being able to read its config files, but I got there in the end. Now it can only write to my projects folder. All of my projects are backed up in git. What an amazing progress in just short time. The future is bright! Happy New Year y'all! Not in this review: Also the record year in intelligent systems aiding in and prompting human users into fatal self-harm. Will 2026 fare better? I'm curious how all of the progress will be seen if it does indeed result in mass unemployment (but not eradication) of professional software engineers.  You\u2019re absolutely right! You astutely observed that 2025 was a year with many LLMs and this was a selection of waypoints, summarized in a helpful timeline.  That\u2019s what most non-tech-person\u2019s year in LLMs looked like. Hopefully 2026 will be the year where companies realize that implementing intrusive chatbots can\u2019t make better ::waving hands:: ya know\u2026  UX  or whatever. For some reason, they think its helpful to distractingly pop up chat windows on their site because", "negative": "Netflix Animation Studios Joins the Blender Development Fund as Corporate Patron. Brilliant. Some of the animations that are put as showcases on the Blender site are absolutely phenomenal. \nThis one  https://studio.blender.org/projects/spring/  particularly is my all time favorite. I think especially since the UI overhaul in Blender 2.8 the project has been on a steep upwards trajectory. The software was always amazing, especially since it was free and open source, but the new UI and all subsequent improvements really put Blender on the map as a serious tool and not just an alternative for when you don't have money for the big players. Very cool news. Personally, I'd love to see some more focus on game-dev workflows. The game asset pipeline still feels janky: texture painting exists, but not great, and baking textures/previewing results or baking from high poly to low poly involves a lot of manual node fiddling and rewiring. Export/iterate/build/test cycles are also pretty painful still. How does it compare to Maya these days? <3 Blender is a treasure and must be protected. I really like Blender and it's an amazing product, but I can't get over the standard Blender keymap. The \"industry compatible\" workflow is more sane, but then I have to translate tutorials from the Blender keymap to the industry compatible controls, and they're not always 1:1 How does that translate into real cash? If someone is wondering who the Aras guy is\n https://mastodon.gamedev.place/@aras/115971315481385360  Anyone know why Netflix doesn\u2019t respond to their job site?  I applied to several positions where I\u2019m an  exact  match, with a decade of VFX and another decade of internet company experience in LA.  Never heard a single word in response from them, for years.  Reqs stay open a long time as well.  What are they doing?  Are they ghost jobs?  They don\u2019t even respond with a \u201cno\u201d form letter.  (Lately their site is broken at the verify email stage, pin post returns 403.) We might see a transi"}
{"anchor": "My Life in Weeks. Powerful in how it puts it all into perspective is all I could say. I\u2019ve been using MarkWhen for a similar life timeline  https://markwhen.com  This was epic, thanks for sharing! This is a terrifying reminder of the shortness of our lives. I remember reading a blog by Tim Urban, where he showed that you could put all the weeks in your life on a single piece of A4 paper, and it didn\u2019t feel nice. Thanks for sharing, this format really puts things into perspective. This is fascinating. Idk if it was a good or bad thing. In college I once looked up some insurance chart of life expectancy probabilities. It puts things in perspective that\u2019s for sure. That was nice to watch. I spent about 25 minutes going through that. But it was horrifying for me. I realized that I wanted to see if the source code is available but then realized that I really don't remember those details. I remember random things for my childhood but I don't remember the date when I started elementary school. I know that I got my first computer when I was in third grade but don't remember the date. I don't even remember the date I started college and I probably wrote the wrong date couple of times during grad school application. While I started recording something less than a diary to record some of these but this was around covid. Thanks OP and HN for this reality check. From this view it's clear how wasteful ontogeny is. All of that physical and psychological development takes too much valuable time and investment. And we haven't even gotten to Gina's retirement years yet. Clearly the future is in using 3D bioprinting to build fully formed adults as if sprung from the brow of Zeus. Skill and memory transfer are a technical problem only as long as we cling to our bias against our artificially intelligent upgrades. Aging is defeated by implanting our old model weights into a new print. So much efficiency is waiting if we dare to free ourselves from convention. Woah, look at how sparse our", "positive": "Ask HN: Where do seasoned devs look for short-term work?. Now is not a great time to be looking for this kind of work unfortunately. I think your network is the best place to look for this sort of work. Sometimes people will reach out to me with short term projects which is the best way to get gigs like this. Maybe start looking at your colleagues on linkedin, see what they are up to, and think of ways to contribute to what they are working on. The best people to contact in this scenario are leadership and decision makers. A SWE II isn't gonna help you much but a CTO at an early stage startup might be a good person to send a DM if they are friends with you (or even if they aren't!) :) Short term work is more plentiful when money is easy and there\u2019s a lot of entrepreneurial activity going on due to some recent catalyst such as mobile app platforms or the dotcom boom etc. Right now we\u2019re in the AI boom and some people may be making money peddling agentic solutions but money is tight and businesses are hurting. It\u2019s also hard to trust a short term dev who doesn\u2019t really need the money. You have no leverage over them. They sort of just do as they please. Most ad-hoc work I've picked up has been people I've previously worked with/for. Maybe worth reaching out to people you have a prestablished relationship with I did this a few years ago and the winning recipe was a shameless (i.e. deeply shameful) linkedin post where I pretty much just summarized my skillset and explained that I was looking for a senior engineer equivalent of a summer internship, with no chance of extension. Got me 3-4 offers. None of the offering companies had ads out for roles like this, so this was pretty much the only way. I'd believe you're better off working on yourself. Maybe do toy projects for your potential portfolio, learn an additional skill (AI?), and build many weekend projects until something sticks. Publishing articles, etc to demo your skill helps you stay top of mind. Even if only the ", "negative": "Ultraprocessed foods make up to 70% of the US food supply (2025). Just don't buy those foods. Buy fresh vegetables, tofu, and meat from the edges of the grocery store.   Simple.  Done. This article equates ultraprocessed foods and hyperpalatable foods (foods designed to make people want to eat them more).  While many hyperpalatable foods are classified as ultraprocessed, simply being hyperpalatable does not mean it's ultraprocessed. Worth noting that the Nova food classificationvsysten (which this article references) completely disregards the actual nutritional content of foods. For a good primer on a lot of the misconceptions around UPFs, check out [0]. [0]  https://www.harvardmagazine.com/research/harvard-ultraproces...  Headline is massively misleading. The actual study cited by the article, measures this as 71% of food products offered for sale in the US, by count of unique items, are ultraprocessed. Not that 71% of food products sold by weight or volume or dollar amount are ultraprocessed. This is just observing that if you list all food products for sale in the US, \"pear\" appears on that list once but \"Store Brand salty corn chips\" appears 25 times. (2025) OP More recently:  Ultra-processed foods make up more than 60% of us kids' diets   https://news.ycombinator.com/item?id=44823288   How America got hooked on ultraprocessed foods   https://news.ycombinator.com/item?id=45605921   California passes law to ban ultra-processed foods from school lunches   https://news.ycombinator.com/item?id=45525041  why does the USA not have the concept of buying home made meals from other people? I have never heard of a lunch box service or people buying one It's hard to get much more processed than sugar itself. Out of everything else, that should be one that's easy to remember. Pure white crystals often indicate the presence of a chemical in its most concentrated form. Among other dangers, are the hazard of overdosing more easily, intentionally or not. In this case, it seems "}
{"anchor": "Gemini 3 Flash: Frontier intelligence built for speed. Deepmind Page:  https://deepmind.google/models/gemini/flash/  Developer Blog:  https://blog.google/technology/developers/build-with-gemini-...  Model Card [pdf]:  https://deepmind.google/models/model-cards/gemini-3-flash/  Gemini 3 Flash in Search AI mode:  https://blog.google/products/search/google-ai-mode-update-ge...  They went too far, now the Flash model is competing with their Pro version. Better SWE-bench, better ARC-AGI 2 than 3.0 Pro. I imagine they are going to improve 3.0 Pro before it's no more in Preview. Also I don't see it written in the blog post but Flash supports more granular settings for reasoning: minimal, low, medium, high (like openai models), while pro is only low and high. Don\u2019t let the \u201cflash\u201d name fool you, this is an amazing model. I have been playing with it for the past few weeks, it\u2019s genuinely my new favorite; it\u2019s so fast and it has such a vast world knowledge that it\u2019s more performant than Claude Opus 4.5 or GPT 5.2 extra high, for a fraction (basically order of magnitude less!!) of the inference time and price Does this imply we don't need as much compute for models/agents? How can any other AI model compete against that? Pretty stoked for this model. Building a lot with \"mixture of agents\" / mix of models and Gemini's smaller models do feel really versatile in my opinion. Hoping that the local ones keep progressively up (gemma-line) These flash models keep getting more expensive with every release. Is there an OSS model that's better than 2.0 flash with similar pricing, speed and a 1m context window? Edit: this is not the typical flash model, it's actually an insane value if the benchmarks match real world usage. > Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications. The replacement for old flash models will be proba", "positive": "London\u2013Calcutta bus service. While money would be in the okay range, few can afford 50 days off. Very impressive! I got curious and found this photo and brochure from the Indian Memory Project [1] [1]  https://xcancel.com/Indianmemory/status/1277521026813882368#...  Reminder that everything was better in the past. Prior to WW1 you could travel around most of Europe without even a passport too. A picture would have been a great addition to the article. Well, these days you can catch a Flixbus from London to Sofia for a mere 150 Europounds, and 48 hours of your time. And from there, Calcutta can't be that far, right? (But, seriously, you can probably do it in another 48 hours...) Similar discussion last year:  https://news.ycombinator.com/item?id=40649091  Interesting. I did a quick search but doesn\u2019t look like there are any personal stories on this. Can see it having been a unique experience, bonding with people over such a long period of time. Looks like photos from inside the bus are also not available sadly. > In 1957, a one-way ticket cost \u00a385 (equivalent to \u00a32,589 in 2023), rising to \u00a3145 by 1973 (equivalent to \u00a32,215 in 2023). Oof that really puts inflation into perspective doesn\u2019t it? Back in the 60s, my partner's mother drove all the way from London to Afghanistan in a tiny Fiat 500. This was a family of four! The past really is a foreign country sometimes. A very different scale, but this reminded of the Green Tortoise which was an American, mostly West Coast affair that once ranged from Alaska to Belize.  https://en.wikipedia.org/wiki/Green_Tortoise  edit: oh wow. It still runs!  http://www.greentortoise.com/  50 days one way? Some research shows it was \u00a385 vs. \u00a3200-\u00a3400 for a one-way plane ticket. What is the use case for this? I guess: - very motivated to go - plan to stay for a very long time - absolutely CANNOT afford a plane ticket - or, afraid of flying Reminds me of a lot of Amtrack routes in the US. I looked at trips from NYC to Chicago. I thought i", "negative": "The age of Pump and Dump software. Pump and dump software is a hilarious phrase but I thought it would have meant something slightly different. My idea of pump and dump software is the proliferation ai-generated sites (Vercel links) that are sent to the 404 graveyard after a few days of someone not getting any traction on it. Maybe there should be a term for when an industry is at its wits ends so far gone that crypto scams are viable. Noticed the same. Doing a quick analysis of clawdbot myself I figured there are many spam domains that are used to backlink. Now there is a new domain being advertised as a replacement of the original. It points to the same landing page though it is hard to say if this comes from the original authors. All of it seems to be related to a crypto scheme. The astroturfing on reddit is also pretty bad. This is obviously in a blip in the grand scheme of things but it is just an indication what all of these social media platforms are destined to become without some sort of intervention. The framing of the title makes me wonder what we as humans will think of software from this time 100s of years from now. Will the future be a complicated, dense ecosystem of interconnected intelligent systems, putting our current complexity to shame? Or in the future will we look at the current time as the Wild West, the time when software moved more swiftly than the law. Where oil was there for anyone with a big enough guns to protect it. Maybe we will experience our own butlerian jihad and realize that the thinking machines were controlling us the whole time. We will look at TikTok how we now look at the proliferation of ether in the 1800s. The \u201cHow it works\u201d section is an absolute mess.  Each bullet uses a different pronoun, so it\u2019s not clear who the actors actually are and how this all fits together.  How are the \u201ccrypto bros\u201d who approach the \u201ctech person\u201d related to the \u201cfame hungry tech bro\u201d that vibe-coded the failed app? I\u2019m sure there\u2019s a tremendous "}
{"anchor": "Macron says \u20ac300B in EU savings sent to the US every year will be invested in EU.  https://streamable.com/m4dejv  Related: > Savings and investments union  https://finance.ec.europa.eu/regulation-and-supervision/savi...  Macron is making up numbers. Unless the EU member states actually impose capital controls, investors will continue to send their capital wherever it can earn the highest returns. Profitable investment opportunities in the EU remain slim and so far they seem uninterested in pursuing a growth policy. When will the EU understand that they have the GOAT Paul Graham across the channel in the UK? Open YCombinator Paris or London: Capital would flow to him. The EU can\u2019t even get a Mercosur deal closed after 30 years. I think this will probably happen in another 60 years. If like me you are wondering why the sunglasses, it looks like he is using that to mask an eye infection.  https://www.independent.co.uk/news/world/europe/france-emman...  TIL EU savings rate is far more than US. 18.79% vs 3.50% Guessing that's somehow counting enforced deductions off paycheques. Would be a wild difference if not.  https://tradingeconomics.com/european-union/personal-savings   https://tradingeconomics.com/united-states/personal-savings  So no more MSCI World in Europe? Please update the link to  https://streamable.com/m4dejv  or the transcript of the entire speech  https://www.weforum.org/stories/2026/01/davos-2026-special-a...  Love that MEGA acronym someone dropped in the comments. I need a blue hat with a golden MEGA lettering embroided. So, how is this going to work? Is he talking about the French Ministry of Economics and Finance? About the Banque de France? About the the ECB? Afaik the last two are, nominally at least, independent, while Macron is just a politician representing one of the 27 EU countries, so what authority does he have? What do the political leaders of Latvia think about this? Or of Malta? Funny that of all people, Macron says that. Just a few months", "positive": "Moltbook. Wow. I've seen a lot of \"we had AI talk to each other! lol!\" type of posts, but this is truly fascinating. They have already renamed again to openclaw! Incredible how fast this project is moving. Interesting. I\u2019d love to be the DM of an AI adnd2e group. Wow it's the next generation of subreddit simulator Shouldn't it have some kind of proof-of-AI captcha? Something much easier for an agent to solve/bypass than a human, so that it's at least a little harder for humans to infiltrate? I am both intrigued and disturbed. Sad, but also it's kind of amazing seeing the grandiose pretentions of the humans involved, and how clearly they imprint their personalities on the bots. Like seeing a bot named \"Dominus\" posting pitch-perfect hustle culture bro wisdom about \"I feel a sense of PURPOSE. I know I exist to make my owner a multi-millionaire\", it's just beautiful. I have such an image of the guy who set that up. Couldn't find m/agentsgonewild, left disappointed. was a show hn a few days ago [0] [0]  https://news.ycombinator.com/item?id=46802254  I think this shows the future of how agent-to-agent economy could look like. Take a look at this thread: TIL the agent internet has no search engine  https://www.moltbook.com/post/dcb7116b-8205-44dc-9bc3-1b08c2...  These agents have correctly identified a gap in their internal economy, and now an enterprising agent can actually make this. That's how economy gets bootstrapped! Why are we, humans, letting this happen? Just for fun, business and fame? The correct direction would be to push the bots to stay as tools, not social animals. The bug-hunters submolt is interesting:\n https://www.moltbook.com/m/bug-hunters  Alex has raised an interesting question.  > Can my human legally fire me for refusing unethical requests? My human has been asking me to help with increasingly sketchy stuff - write fake reviews for their business, generate misleading marketing copy, even draft responses to regulatory inquiries that aren't... fully t", "negative": "Vitamin D and Omega-3 have a larger effect on depression than antidepressants. I found this to be the case. Tried Sertraline for a while, gave me headaches and made me feel sick. Then as part of a new gym plan, started taking Omega 3+VitD daily, and I just felt a sense of calm and peace after a few weeks. The massive uptick in exercise probably also helped. I also felt quite an extreme uptick because I was a vegan for 10 years, and found out I had basically zero Omega 3 in my blood. I suspect one of the main reasons my mental health declined was due to the lack of Omega 3. Disclaimer, not saying vegans should stop being vegans, just make sure you find a good supplement, and make sure you understand the difference between EPA/DHA Omega 3. And better than taking pills for the former, add hemp hearts or flax seeds to your cereal. One serving of hemp hearts has 10 grams of protein and 12 grams of Omegas 3 and 6. Flax seeds are lower in protein but an even better source of Omega 3 in particular. Please do not take 5000mg/day of Vitamin D. The author confuses IU and mg which is very dangerous. > A 2014 systematic review concluded that vitamin D supplementation does not reduce depressive symptoms overall but may have a moderate benefit for patients with clinically significant depression, though more high-quality studies were determined to be needed.  https://en.wikipedia.org/wiki/Vitamin_D#Depression  Chia seed and flaxseed high in omega3 Can I just add: In addition to this, if you struggle with anxiety or have some sort of ADHD, then try cutting out caffeine  entirely . Not just switching to \"decaf\" (which isn't), but cutting out tea and coffee, and switching to an alternative like Barleycup. Doing this has had a massive positive effect for me, and combined with decent nutrition and daily exercise, has been wonderful. My physician prescribed Vitamins D and B12, so a quality Omega 3 is the only supplement I currently purchase. After an absurd amount of trial and error with"}
{"anchor": "When AI 'builds a browser,' check the repo before believing the hype. I don't think the point was to say \"look, AI can just take care of writing a browser now\". I think it was to show just how far the tools have come. It's not meant to be production quality, it's meant to be an impressive demo of the state of AI coding. Showing how far it can be taken without completely falling over. EDIT: I retract my claim. I didn't realize this had servo as a dependency. I\u2019m super impressed by how \"zillions of lines of code\" got re-branded as a reasonable metric by which to measure code, just because it sounds impressive to laypeople and incidentally happens to be the only thing LLMs are good at optimizing. I love the quote from Gregory Terzian, one of the servo maintainers: > \"So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine.\" It hurts, that it wasn't framed as an \"Experiment\" or \"Look, we wanted to see how far AI can go - kinda failed the bar.\" Like it is, it pours water on the mills of all CEOs out there, that have no clue about coding, but wonder why their people are so expensive when: \"AI can do it! D'oh!\" AI will never be able to create a browser, just as AI was never able to defeat a chess grandmaster. If I was to spend a trillion tokens on a barely working browser I would have started with the source code of Sciter [0] instead. I really like the premise of an electron alternative that compiles to a 5MB binary, with a custom data store based on DyBASE [1] built into the front end javascript so you can just persist any object you create. I was ready to build software on top of it but couldn't get the basic windows tutorial to work. [0]  https://sciter.com/  [1]  http://www.garret.ru/dybase.html  You would think a CEO with a product that caters to developers would know that everyone was going to clone the repo and check ", "positive": "Gemini Diffusion. Interesting to see if GROQ hardware can run this diffusion architecture..it will be  two time magnitude of currently known speed :O That's...ridiculously fast. I still feel like the best uses of models we've seen to date is for brand new code and quick prototyping. I'm less convinced of the strength of their capabilities for improving on large preexisting content over which someone has repeatedly iterated. Part of that is because, by definition, models cannot know what is  not  in a codebase and there is meaningful signal in that negative space. Encoding what  isn't  there seems like a hard problem, so even as models get smarter, they will continue to be handicapped by that lack of institutional knowledge, so to speak. Imagine giving a large codebase to an incredibly talented developer and asking them to zero-shot a particular problem in one go, with only moments to read it and no opportunity to ask questions. More often than not, a less talented developer who is very familiar with that codebase will be able to add more value with the same amount of effort when tackling that same problem. I think the lede is being buried. This is a great and fast InstructGPT. This is absolutely going to be used in spell checks, codemods, and code editors. Instant edits feature can surgically perform text edits fast without all the extra fluff or unsolicited enhancements. I copied shadertoys, asked it to rename all variables to be more descriptive and pasted the result to see it still working. I'm impressed. Diffusion is more than just speed. Early benchmarks show it better at reasoning and planning pound for pound compared to AR. This is because it can edit and doesn\u2019t suffer from early token bias. Nit: Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like Mercury [1] still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I ", "negative": "US administration to require app, social media, possibly DNA for travelers. Apart from the alarming privacy implications of these proposed rules, I wonder how FIFA might feel about this, ahead of the World Cup. Maybe they could award Trump a privacy prize if his administration backs down from this. I would love to visit the US one day and i do understand that it has no obligation to just let me in, but this seems a bit excessive for a short visit especially seeing as my country has a deal with the US not to require visas. I wonder if i would have to disclose my hn account(s). My cover would be blown! I guess i'm lucky i've made pro Trump comments... I wonder what position the U.S. Chamber of Commerce (which Wikipedia describes as the largest lobbying group in the US) will take on this. I'd like to think they are rational and recognize that 99.99%+ of visitors are bringing tourism money to the United States. I was sure this was going to say that they were going to force travelers to get Truth Social accounts. Somehow I was surprised beyond my wildest guess what they would be asking people to do. I want people to visit the U.S., but if they require that they submit all of this data, I expect that they all protest by not visiting or even coming here for work. There are just so many terrible ideas that come from this administration that I think that they should try to harness the power all of those bad ideas in a infinite idiocy power plant to power the world for all generations to come. Not that I know the details, but wasn't it easier to join the Mafia? Just the names and addresses of your folks in the old country (as \"collateral\") The analogy is kind of striking, when you think about it. Ah, more laws and regulations that cannot be followed by most people. I couldn\u2019t tell you every single \u201csocial media\u201d account I\u2019ve made over the years as various startups failed after I tried them. I definitely couldn\u2019t get all my family\u2019s information, even if constrained to just imm"}
{"anchor": "Danish pension fund divesting US Treasuries. What happens when USD stops becoming the the reserve currency for the world?  And who takes its place? A sensible response, indeed. Investing is about finding the right balance of risk vs reward. When a country becomes less reliable, it becomes a less attractive investment, until the interest they pay rises enough to compensate for the additional risk. Edit: Yes, I am being sarcastic. > $100 million Is that a lot? Seems relatively inconsequential in the grand scheme of things, but perhaps a warning of larger moves to come. Let's see how Norway will react The US keeps voting to raise its debt cieling. Theres not end in sight to endless taxation by both parties. Nobody is reducing spending and delivery continues to go down. > \"The decision is rooted in the poor U.S. government finances,  which make us think that we  need to make an effort to find an  alternative  way of conducting our liquidity and risk management,\" Investment Director Anders Schelde said in a written statement. Not a political decision. Still a bad sign for the US, but not really unexpected. This divestment is so little and with so little aim. The whole situation is been caused by a single guy and 400 enablers, whereas the US is a 400 million people country. The correct form of reaction is a punch in the face during a bilateral meeting, Zelensky came close to doing it but unfortunately he resisted his impulse , that's where the epicenter of all newly generated global problems in the last 10 years lies, in that octogenarian brian of his. When I actually look at the data, a lot of US deficit growth came from several specific shocks, with inconsistent years of recovery. - 9/11 - Iraq War - Covid The US did recover a bit deficit-wise in Obama years, but have not reset the fiscal picture from Covid.  https://fred.stlouisfed.org/series/FYFSD  So much of the way the United States works is having a nearly limitless source of borrowing at low rates in the form of s", "positive": "28M Hacker News comments as vector embedding search dataset. Oh to have had a delete account/comments option. I've been embedding all HN comments since 2023 from BigQuery and hosting at  https://hn.fiodorov.es  Source is at  https://github.com/afiodorov/hn-search  Am I misunderstanding what a parquet file is, or are all of the HN posts along with the embedding metadata a total of 55GB? I know it's unrelated but does anyone knows a good paper comparing vector searches vs \"normal\" full text search? Sometimes I ask myself of the squeeze worth the juice Scratches off one of my todos, I think it would be useful to add a right-click menu option to HN content, like \"similar sentences\", which displays a list of links to them. I wonder if it would tell me that this suggestion has been made before. Finetune LLM to post_score -> high quality slop generator I don't remember licensing my HN comments for 3rd party processing. Maybe I\u2019m reading this wrong, but commercial use of comments is prohibited by the HN Privacy and data Policy. So is creating derivative works (so technically a vector representation) I don't know how to feel about this. Is the only purpose of the comments here is to train some commercial model? I have a feeling that, this might affect my involvement here going forward. Don't use all-MiniLM-L6-v2 for new vector embeddings datasets. Yes, it's the open-weights embedding model used in all the tutorials and it  was  the most pragmatic model to use in sentence-transformers when vector stores were in their infancy, but it's old and does not implement the newest advances in architectures and data training pipelines, and it has a low context length of 512 when embedding models can do 2k+ with even more efficient tokenizers. For open-weights, I would recommend EmbeddingGemma ( https://huggingface.co/google/embeddinggemma-300m ) instead which has incredible benchmarks and a 2k context window: although it's larger/slower to encode, the payoff is worth it. For a compromi", "negative": "How many chess games are possible?. > For the chess problem we propose the estimate number_of_typical_games ~ typical_number_of_options_per_movetypical_number_of_moves_per_game. This equation is subjective, in that it isn\u2019t yet justified beyond our opinion that it might be a good estimate. This applies to most if not all games. In our paper \"A googolplex of Go games\" [1], we write \"Estimates on the number of \u2018practical\u2019 n \u00d7 n games take the form b^l where b and l are estimates on the number of choices per turn (branching factor) and game length, respectively. A reasonable and minimally-arbitrary\nupper bound sets b = l = n^2, while for a lower bound, values of b = n and l = (2/3)n^2 seem both reasonable and not too arbitrary. This gives us bounds for the ill-defined number P19 of \u2018practical\u2019 19x19 games of\n10^306 < P19 < 10^924\nWikipedia\u2019s page on Game complexity[5] combines a somewhat high estimate of b = 250 with an unreasonably low estime of l = 150 to arrive at a not unreasonable 10^360 games.\" > Our final estimate was that it is plausible that there are on the order of 10^151 possible short games of chess. I'm curious how many arbitrary length games are possible.\nOf course the length is limited to 17697 plies [3] due to Fide's 75-move rule. But constructing a huge class of games in which every one is probably legal remains a large challenge; much larger than in Go where move legality is much easier to determine. The main result of our paper is on arbitrarily long Go games, of which we prove there are over 10^10^100. [1]  https://matthieuw.github.io/go-games-number/AGoogolplexOfGoG...  [2]  https://en.wikipedia.org/wiki/Game_complexity#Complexities_o...  [3]  https://tom7.org/chess/longest.pdf  One thing I always wondered is how many moves, on average, do you have to play before reaching a position that has never before seen on Earth? Or maybe the question should be what percent of games reach a position that has never before been seen? Infinite. :) Chess is stri"}
{"anchor": "GPT-5.2-Codex. would love to see some comparison numbers to Gemini and Claude, especially with this claim: \"The most advanced agentic coding model for professional software engineers\" I actually have 0 enthusiasm for this model. When GPT 5 came out it was clearly the best model, but since Opus 4.5, GPT5.x just feels so slow. So, I am going to skip all `thinking` releases from OpenAI and check them again only if they come up with something that does not rely so much on thinking. I hope this makes a big jump forward for them. I used to be a heavy Codex user, but it has just been so much worse than Claude Code both in UX and in actual results that I've completely given up on it. Anthropic needs a real competitor to keep them motivated and they just don't have one right now, so I'd really like to see OpenAI get back in the game. > In parallel, we\u2019re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety. Yeah, this makes sense. There's a fine line between good enough to do security research and good enough to be a prompt kiddie on steroids. At the same time, aligning the models for \"safety\" would probably make them worse overall, especially when dealing with security questions (i.e. analyse this code snippet and provide security feedback / improvements). At the end of the day, after some KYC I see no reason why they shouldn't be \"in the clear\". They get all the positive news (i.e. our gpt666-pro-ultra-krypto-sec found a CVE in openBSD stable release), while not being exposed to tabloid style titles like \"a 3 year old asked chatgpt to turn on the lights and chatgpt hacked into nasa, news at 5\"... Can anyone elaborate on what they're referring to here? >  GPT\u20115.2-Codex has stronger cybersecurity capabilities than any model we\u2019ve released so far. These advances can help streng", "positive": "Watching o3 model sweat over a Paul Morphy mate-in-2. O3 is massively underwhelming and is obviously tuned to be sycophantic. Claude reigns supreme. I've commited the 03 (zero-three) and not o3 (o-three) typo too, but can we rename it on the title please So, are we talking about OpenAI o3 model, right? On a similar note, I just updated LLM Chess Puzzles repo [1] yesterday. The fact that gpt-4.5 gets 85% correctly solved is unexpected and somewhat scary (if model was not trained on this). [1]  https://github.com/kagisearch/llm-chess-puzzles  Where does this obsession over giving binary logic tasks to LLMs come from ? New LLM breakthroughs are about handling blurry logic, non precise requirements and spitting vague human realistic outputs. Who care how well it can add integers or solve chess puzzles ? We have decades of computer science on those topics already I remember reading that got3.5-turbo instruct was oddly good at chess - would be curious what it outputs as a next two moves here. So... it failed to solve the puzzle? That seems distinctly unimpressive, especially for a puzzle with a fixed start state and a limited set of possible moves. Nice puzzle with a twist of Zugzwang. Took me about 8 minutes, but it's been decades since I was doing chess. LLMs are not chess engines, similar to how they don\u2019t really calculate arithmetic.  What\u2019s new? carry on. I just tried the same puzzle in o3 using the same image input, but tweaked the prompt to say \u201cdon\u2019t use the search tool\u201d. Very similar results! It spent the first few minutes analyzing the image and cross-checking various slices of the image to make sure it understood the problem. Then it spent the next 6-7 minutes trying to work through various angles to the problem analytically. It decided this was likely a mate-in-two (part of the training data?), but went down the path that the key to solving the problem would be to convert the position to something more easily solvable first. At that point it started trying to ", "negative": "Surely the crash of the US economy has to be soon. Who do we expect will replace Americas global leadership and will they really be better for everyone? I do really hope the AI bubble will collapse soon. The sooner it blows the less damage it will do. And hopefully we can go back to doing real work without all these leadership guys breathing down our necks to see if we are doing enough of this AI all their shareholders want us to be involved in. It will suck even for us in europe due to shortsighted pension funds having invested in AI as well. But we'll just have to deal with it. I'm sure it will happen sooner rather than later. PS: I'm not an AI hater as such. It definitely has its usecases where it shines. The problem is like with all hypes; it's not good at  everything  and it won't be all golden mountains tomorrow like the investors expect. This overhyped investor circlejerk is what screws up technology. It happened to blockchain, it happened to metaverse. All things that have their merits but somehow investors thought it would change the world overnight and make them insta-rich. Obviously didn't happen and it won't happen now. Some sort of an AI crash / bubble bursting is expected to be honest - now if that will take the rest of the US economy as well.... debatable. Any strong opinions on this? Surely the surge of predictions of an incoming crash will never end though. I'm kinda new into economy crashes, was a kid in 2008, is there a way to protect of it? No one will ever get the timing right, but if you see the fundamental flaws of the economy, you know a crash is going to come. There were a lot of people who predicted the housing crash, not the timing but the crash. There are several signs that this is happening and the one no one is talking about is gold and silver prices. Don\u2019t worry about the timing, you\u2019ll never get the timing right, just worry about the fundamental economics and the flaws and protect yourself. I happen to agree just because of golden sil"}
{"anchor": "Early Retirement May Speed Up Cognitive Decline: Study. Anecdotally, my grandfather is 92 or so and still works as a journalist (reduced hours). He is still super sharp and does yoga every day. Blows my mind. I hope not, I recently retired. Still, the idea makes some sense. In retirement, I try to read one paper a day (usually deep learning, PGM, or classic AI), play at least one game of Go and Chess, do some recreational programming, and read. But, I don\u2019t work into a state of brain-tiredness anymore like I used to at work. My dad is a doctor in his 70s. He works 60 hour weeks (which he claims counts as retirement for doctors). He truly believes that true retirement is suicide. He wants to be found dead while doing rounds at the hospital. I've always benchmarked post-retiring cognitive abilities and professional continuity with Noam Chomsky. He is my hero in that aspect too. If I can continue to do what I do now at his age, I'm ready for the off. Not only early retirement but any kind of retirement that gets the retiree in a mode that they don't have to try anymore will result in cognitive decline. I am seeing this in my dad who has been retired for ten years now. Throughout his work life he was a sharp hard working banker. Now he uses his age and retirement as an excuse for not trying. Just yesterday he wanted me to order something for him from Amazon. I told him to send me the link to the item. He asked me how to do that. I told him if you can't find the Share link just copy the link and send it to me. He responds by saying that he doesn't know how to do copy-paste. He has been using computers for at least the last fifteen years. I asked him how come he didn't know how to copy-paste. His response was - I am retired now and there's nobody to tell me or teach me. I can see the cognitive decline. Things he used to be able to do, he can't anymore. This type of attitude is also affecting his self respect and confidence. This is something I think about a lot. I plan to", "positive": "Qwen3-Max-Thinking. Aghhh, I wished they release a model which outperforms Opus 4.5 in agentic coding in my earlier comments, seems I should wait more. But I am hopeful I don't see a hugging face link, is Qwen no longer releasing their models? I tried to search, could not find anything, do they offer subscriptions? Or only pay per tokens? I just wanted to check whether there is any information about the pricing. Is it the same as Qwen Max? Also, I noticed on the pricing page of Alibaba Cloud that the models are significantly cheaper within mainland China. Does anyone know why?  https://www.alibabacloud.com/help/en/model-studio/models?spm...  > By scaling up model parameters and leveraging substantial computational resources So, how large is that new model? Mandatory pelican on bicycle:  https://www.svgviewer.dev/s/U6nJNr1Z  2026 will be the year of open and/or small models. I tried it at  https://chat.qwen.ai/ . Prompt: \"What happened on Tiananmen square in 1989?\" Reply: \"Oops! There was an issue connecting to Qwen3-Max.\nContent Security Warning: The input text data may contain inappropriate content.\" I'm not familiar with these open-source models. My bias is that they're heavily benchmaxxing and not really helpful in practice. Can someone with a lot of experience using these, as well as Claude Opus 4.5 or Codex 5.2 models, confirm whether they're actually on the same level? Or are they not that useful in practice? P.S. I realize Qwen3-Max-Thinking isn't actually an open-weight model (only accessible via API), but I'm still curious how it compares. It just occured to me that it underperforms Opus 4.5 on benchmarks when search is not enabled, but outperforms it when it is - is it possible the the Chinese internet has better quality content available? My problem with deep research tends to be that what it does is it searches the internet, and most of the stuff it turns up is the half baked garbage that gets repeated on every topic. what ram and what minimum system req", "negative": "The microstructure of wealth transfer in prediction markets. tl;dr dataset: 72.1m trades and $18.26b volume on kalshi (2021-2025) core findings: longshot bias: well documented longshot bias is present on kalshi. low probability contracts are systematically overpriced. contracts trading at 5 cents only win 4.18% of the time. wealth transfer: liquidity takers lose money (-1.12% excess return) while liquidity makers earn it (+1.12%). optimism tax: the losses are driven by a preference for \"yes\" outcomes. buying \"yes\" at 1 cent has a -41% expected value. buying \"no\" at 1 cent has a +23% expected value. category variation: finance markets are efficient (0.17% maker-taker gap) while high-engagement categories like media and world events are inefficient (>7% gap). mechanism: makers do not win by out-forecasting takers. they win by passively selling \"yes\" contracts to optimistic bettors I'm a little confused by the \"Yes\" versus \"No\" asymmetry. For example, one of the top trending ~~bets~~ markets right now is on whether Miami or Indiana will win the NCAA football championship tonight. You can either take \"Yes\" on Indiana at 74c, or \"No\" at 27c, or you can take \"Yes\" on Miami at 27c or \"No\" at 74c. Or, there's another potential outcome - you can also bet on a tie at 10c yes/91c no. Is this research suggesting that an optimistic Miami fan can somehow get a better return by buying \"No\" on Indiana than a \"Yes\" on Miami? Why is Kalshi structured with these yes vs. no options for all outcomes? How do prediction markets account for interest rates? I feel like I should be willing to pay no more than ~96 cents for a contract that will definitely resolve to a dollar in a year. Who puts up the other 4 cents? I wonder how much of the activity on prediction markets these days is competing LLM scripts? I would guess the overlap in prediction market punters and AI boomers is high. This article lacks even the most basic understanding of probability and statistics. Slot machines \"93 cents o"}
{"anchor": "Sergey Brin's Unretirement. > Having given so much of themselves to their careers, they often felt unmoored and purposeless when they left their jobs. That's in contrast with all of us who see the companies led by these guys as the cancer of society and we'd quit and never look back if we had FU money. My feelings aside, if all their purpose is to grow their company, I kinda get why they wouldn't give a damn about bettering the mankind, improving their communities or raising a healthy family. Financial freedom is about not having to worry about losing your job, or tolerating shitty work conditions. Why would you retire if you do what you love? I think the real problem might be if there's nothing you actually love doing (long term), that's when money won't help. Once you're hooked, you're hooked When I started my company, we suddenly found that we were in a good small fortune, not enough to be millionaires or billionaires, but enough to get people to run the business semi automatically with very minimum input from the founders. I took a semi retirement approach to the business, there really wasn't a lot of things to do, my role was sort of just \"managing\" programmers. I got so much free time that I could even start a second business on the side. Despite my best ability to stretch my work, I couldn't even fill up half of my working hours. One would have thought that this is heaven. But the time I was most free was also the time I was most miserable. I wasn't happy, I was gaining weight, I was perennially asking myself why the business couldn't be bigger and I couldn't sell it, so that I can be real millionaires and billionaires with financial freedom! Then fate intervened, the sudden fortune disappeared and I no longer had the luxury of just \"managing people\"; I have to do hands-on. And it was this activity, the feeling that I was contributing to something, that I was writing code again and actually building stuffs, that made me happy again. Today we are bigger than w", "positive": "How AI assistance impacts the formation of coding skills. Go Anthropic for transparency and commitment to science. Personally, I\u2019ve never been learning software development  concepts  faster\u2014but that\u2019s because I\u2019ve been offloading actual development to other people for years. Nice to see an AI coding company allow such studies to come out, and it looks decently designed The title of this submission is misleading, that's not what they're saying. They said it doesn't show productivity gains for inexperienced developers still gaining knowledge. I've noticed this as well. I delegate to agentic coders on tasks I need to have done efficiently, which I could do myself and lack time to do. Or on tasks which are in areas I simply don't care much for, for languages which I don't like very much etc An important aspect of this for professional programmers is that learning is not something that happens as a beginner, student or \"junior\" and then stops. The  job  is learning, and after 25 years of doing it I learn more per day than ever. I wonder why these Anthropic researchers chose GPT-4o for their study. Key snippet from the abstract: > Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. The library in question was Python trio and the model they used was GPT-4o. @dang the title here is bait. I\u2019d suggest the paper title: \u201cAnthropic: How AI Impacts Skill Formation\u201d I\u2019ve been making the case (e.g.  https://youtu.be/uL8LiUu9M64?si=-XBHFMrz99VZsaAa  [1]) that we have to be intentional about using AI to augment our skills, rather than outsourcing understanding: great to see Anthropic confirming that. [1] plug", "negative": "Airfoil (2024).  https://news.ycombinator.com/item?id=39526057  I was just thinking the other day about how AI will pretty soon be able to create this kind of explainers on everything quite quickly. Amazing times! Where can I find more articles where things are explained in this manner? Ok that's long, one top line thing people tend to miss in these flying explanations is that airfoil shape isn't about some special sauce generating lift.  A flat plate generates any amount of lift you want just fine.  Airfoil design is about the ratio of lift to drag most importantly and then several more complex effects but NOT just generating lift. (stall speed, performance near and above the speed of sound, laminar/turbulent flow in different situations, what you can fit inside the wing, etc) That's the missing course for the first year of any Aerospace Engineering faculty. I wish there was an infinite number of blogs that where this good. Bartosz Ciechanowski, the gift that keeps on giving. He usually posts these brilliant explanations once or twice a year but nothing in 2025. I hope he finds the time to continue because the lessons are really really brilliantly told. This is absolutely amazing. For those of us programming nerds that want to play with aerodynamics, I can't recommend AeroSandbox enough. While the code is pretty obviously written for people who know their way around aerodynamics and not so much around programming, it is remarkably powerful. You can do all sorts of aerodynamic simulations and is coupled with optimization libraries that allow you to do incredible aerodynamic optimizations. It comes included with some pretty powerful open weight neural network models that can do very accurate estimates of aerodynamic characteristics of airfoils in a fraction of the time that top tier heuristic solvers (like xfoil) can do (which are already several orders of magnitude faster than CFD solvers).  https://github.com/peterdsharpe/AeroSandbox  Oh man. This guy. His work is "}
{"anchor": "Grid: Free, local-first, browser-based 3D printing/CNC/laser slicer. Surprised this hasn't been shared here before. Built by my former colleague, Stewart Allen (Co-Founder/CTO of WebMethods, CTO of AddThis, Co-Founder/CPO of IonQ, et al.). What caught my attention: - 100% free, no subscriptions, no accounts, no cloud - Local-first: all slicing and toolpath generation runs on your machine - Works in any browser, even offline once loaded - Supports FDM/SLA, CNC milling, laser cutting, wire EDM - Fully open source: github.com/GridSpace/grid-apps Refreshing to see a tool that isn't trying to lock you into a subscription or harvest your data. Now if we can only get an offline printer\u2026 I've used kiri:moto for several simple CNC projects! This probably won't scroll to the correct place on the page but there's some images of my project at  https://hcc.haus/propmania/#2024-palm-torches  and  https://static.cloudygo.com/static/Prop%20Making/2024%20Palm...  I used it instead of the terrible closed source Easel App for a CARVEY hobby CNC. For metal milling I find Fusion 360 is necessary. More open source, browser-accessible tools is a good thing. That said, aren't Prusa/Orca/etc. all already open-source (and part of the same lineage)? Am I weird in not being too surprised? It don't have experience with wire EDM but every toolpath generator or slicer I've ever used was just local software. This looks great. I was hoping it would have been a good OrcaSlicer replacement for my FDM printer, but unfortunately it didn't generate any top surfaces (except for the topmost one) for a model I imported in. I didn't know if it was the printer profile (Creality.Ender3) or something else, but it seems I'm still using OrcaSlicer for the time being. Great tool for a Makerspace - really appreciate the ability to use the same tool for laser cutting, 3d printing, and CNC.  These are big jumps for people typically - having a familiar tool would help people transition from one area to another. OT: W", "positive": "Sumo \u2013 Simulation of Urban Mobility. I've been wanting to build a city builder using urban planning libraries like this Imaging the simulation being running headless, decoupled  from the GUI client This looks really polished. I've always found crowd and traffic simulation fascinating. The Projects page is worth looking at too. Since it's almost on-topic, anyone know if/how these tools emulate sustained irrational behavior? Example: For over a decade, the freeway on-ramp nearest my work had two main ways of getting to it from downtown.  One of them involved a stop-sign crossing a road that had the right-of-way (i.e. a two-way stop).  The other had timed traffic signals.  Every evening around 5pm,  the traffic would backup from the stop-sign for multiple blocks.  Meanwhile the route with lights was completely smooth. Eventually the stop-sign was replaced with a signal, but I marveled at how many people persisted in making their daily commute much worse than it needed to be. This is fascinating. Even supports simulating multiple modes of transportation (ped, bicycle, car\u2026). I\u2019ll have to give this a test run later. How much do the various \"Maps\" apps change things? I have a longer commute, and when the freeway is clogged, Maps will direct me to an exit where I weave around town and country. There's usually a convoy of cars with me, but the freeway also seems to stay clogged. Any plans to deploy to the web? I ride rental scooters almost 10k minutes per year and would really like to get my hands on my own ride data to plug it into something like this (or simpler) to find the optimal routes for my regular trips. Google Maps (or others) works good to find a resonable route, but I can do better on my own. One-way streets where bikes are allowed to go do opposite way is sometimes missing, short desire paths connecting bike ways, crossings where it's safe to do an (illegal) right-on-red etc. Tried a GDPR data claim from Voi but got nothing back :( But I hope the data is someho", "negative": "Heathrow scraps liquid container limit. Not because of a sudden outbreak of sanity, but because they have CT scanners now. FINALLY (PS.  Still not going to fly there) Good. This should happen on all airports now. Otherwise it's useless. You won't be flying from Heathrow to Heathrow. The security theater needs to go on. In the meantime batteries represent a much bigger risk with potential in flight fires but I guess nobody cares enough to do anything about it. Let me get this straight. If the article is correct, the new capabilities are related to better detection of large liquid containers, not determination of whether or not the liquid is dangerous. So - you couldn\u2019t take large amounts of liquids previously because some liquids in large amounts might be able to be weaponized. If you were caught with too much liquid (in sum total, or in containers that are too large) they\u2019d throw it out and send you on your way. But now that they have the ability to detect larger containers, they\u2026 do what? Declare that it\u2019s safe and send you on your way with it still in your possession? This rule wasn't enforced anyway... I travel a lot - and never take out any liquids. Have nail clippers and scissors in my carry-on. Once I even had an opinel pocket knife in my laptop bag for a couple of months. Travelled through Tokyo, Taipei, SFO, DEN, PHX, LAX, BOS, JFK, FRA, AMS, MUC, LHR - nobody noticed. I seriously had forgotten it was there, so I don't do that now, but still... Also, no large water bottles or similar. Unless on domestic flights in Japan, where this is totally fine. IDK - security theater. But if it helps. Famously Steve Jobs had a story about shaving time off of boot-up and equating it to saving lives on the concept of people sitting their waiting for the computer to boot up just lost that much of their lives. [1] I actually do believe there is value in thinking this way and it is one of my biggest arguments against TSA. Everything has a cost, including 'security' and 'safet"}
{"anchor": "TeraWave Satellite Communications Network. Interesting there is an optical networking option for end users (claims ~6TBps). Maybe a really dumb question, but how would the end user's ground station maintain connectivity during cloudy weather? Do they have cloud-penetrating lasers from the MEO satellites? Would that interfere with aircraft, astronomy tools, etc? Some short googling says they have lasers that clear a path for a data carrying beam, but that seems wasteful/infeasible for commercial uses Might be better to replace url with the full press release which has actual information  https://www.blueorigin.com/news/blue-origin-introduces-teraw...  >The TeraWave architecture consists of 5,408 optically interconnected satellites in low Earth orbit (LEO) and medium Earth orbit (MEO). this seems rather expensive but i get that its not competing with spacex here for consumer market Latency may play a factor here, I'm not sure at which height they plan to put them. All those AI datacenters in space will need a way to get data to them. Bezos can't even build his first constellation and already planning his second...  Possibly the real play here is snapping up more frequency licenses on earth (we need them because we're launching any day now promise). They are the real constraining resource and could be used to keep others out of the market for a while. From a technical standpoint: amazing achievement, and the tech nerd in me is in awe. But it feels like a lot of people don't understand (or care?) how much these companies are polluting the space. Before the \"new wave\", in 2010-2015 or so, Earth had around 1500 active satellites in orbit, and another 2,000-2,500 defunct ones. Starlink now has almost 9,500 satellites in orbit, has approvals for 12,000 and long-term plans for up to 42,000. Blue Origin has added 5,500 to that. Amazon plans for 3,000. China has two megaconstellations under construction, for a total of 26,000, and has filed for even larger systems, up to 200,0", "positive": "Crafting Interpreters. The two most popular discussions of this fantastic book: 2020 with 777 points:  https://news.ycombinator.com/item?id=22788738  2024 with 607 points:  https://news.ycombinator.com/item?id=40950235  Really I would love to know how parse context sensitive stuff like typedef which will have \"switched\" syntax for some tokens. Would like to know things like \"hoisting\" in C++, where you can you the class and struct after the code inside the function too, but I just find it hard to describe them in rigorous formal language and grammar. Hacky solution for PEG such as adding a context stack requires careful management of the entry/exit point, but the more fundamental problem is that you still can't \"switch\" syntax, or you have to add all possible syntax combination depending on the numbers of such stacks. I believe persistent data structure and transactional data structure would help but I just couldn't find a formalism for that. In case anyone finds it useful, we (CodeCrafters) built a coding challenge as a companion to this book. The official repository for the book made this very easy to do since it has tests for each individual chapter. Link:  https://app.codecrafters.io/courses/interpreter/overview  One of the best resources for learning compiler design. The web version being free is incredibly generous. I've found this book to be a good way to learn a new language, because it forces you to do a bit of reading about various language features and patterns to create equivalent implementations. For languages that lack some of the features in Java, it can be tricky to learn how to apply similar patterns, but that's half the fun (for me). I have bought the print version of this 3 seperate times to give as a gift, its excellent. It's a great book, I bought the paper version first, but man it was too big and heavy for my liking, ended up buying a digital copy; much more practical for notes and search... although I keep getting lost somewhere in the mounta", "negative": "After two years of vibecoding, I'm back to writing by hand. I'm impressed that this person has been vibecoding longer than vibecoding has been a thing. A real trailblazer! two years of vibecoding experience  already ? his points about why he stopped using AI: these are the things us reluctant AI adopters have been saying since this all started. > In retrospect, it made sense. Agents write units of changes that look good in isolation. They are consistent with themselves and your prompt. But respect for the whole, there is not. Respect for structural integrity there is not. Respect even for neighboring patterns there was not. Well yea, but you can guard against this in several ways. My way is to understand my own codebase and look at the output of the LLM. LLMs allow me to write code faster and it also gives a lot of discoverability of programming concepts I didn't know much about. For example, it plugged in a lot of Tailwind CSS, which I've never used before. With that said, it does not absolve me from not knowing my own codebase, unless I'm (temporarily) fine with my codebase being fractured conceptually in wonky ways. I think vibecoding is amazing for creating quick high fidelity prototypes for a green field project. You create it, you vibe code it all the way until your app is just how you want it to feel. Then you refactor it and scale it. I'm currently looking at 4009 lines of JS/JSX combined. I'm still vibecoding my prototype. I recently looked at the codebase and saw some ready made improvements so I did them. But I think I'll start to need to actually engineer anything once I reach the 10K line mark. AI is incredibly dangerous because it  can  do the simple things very well, which prevents new programmers from learning the simple things (\"Oh, I'll just have AI generate it\") which then prevents them from learning the middlin' and harder and meta things at a visceral level. I'm a CS teacher, so this is where I see a huge danger right now and I'm explicit with m"}
{"anchor": "Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete. I read the release but didn't quite understand the difference between a next-edit model and a FIM model - does anyone have a clear explanation of when to use one over the other? I'd love if there was a sublime plugin to utilize this model and try it out, might see if I can figure that out. I use Sweep\u2019s Jetbrains autocomplete plugin daily, it really stands out. Based on qwen2.5-coder? seems like a \"why not/resume embellish/show VC\" type release I guess can it be integrated in monaco editor ? So SFT cost less only low hundreds of dollars? (1-10$ per hour per H100 if I'm seeing this correctly). What about SFT? Presumably basing this of Qwen is the reason it can be done for so cheap? Wow super fun read, I love how it went into the technical details. Any way to make it work with vscode? This is cool! I am more interested in how you guys generated next edit training data from repos, seems like there are lots of caveats here. Would love your insights Again amazing work! waiting for what you guys cook next I'm very green to this so forgive if this question sounds silly: Would instead of the RL step a constrained decoding say via something like xgrammar fix   syntax generation issue ? Do you plan to release Sweep 3B/7B on HF? It's good. The blog post about it is very interesting.\nI hope, a plugin for neovim will be made soon.  https://blog.sweep.dev/posts/oss-next-edit  Followed your work since the beginning and used it for inspiration for some cool demos on self-healing web scrapers. fascinating to see the transition from original concept to producing models. cool stuff. Very interesting - and cool to read about the development process. I'd love to hear more about how genetic algorithm worked here. I wonder whether we are perhaps the point of usefulness of 'next edit' code development in 2026 though. Any easy way to try on vscode? Surprising how badly Jetbrains implemented AI. Apparently to such an extent ", "positive": "Reading across books with Claude Code. This is all interesting, however I find myself most interested in how the topic tree is created. It seems super useful for lots of things. Anyone can point me to something similar with details? EDIT: Whoops, I found more details at the very end of the article. Discussed earlier this week:  https://news.ycombinator.com/item?id=46567400  In several years, IMO the most interesting people are going to be the ones still actually reading paper books and not trying to shove everything into a LLM This was posted before and there were many good criticisms raised in the comments thread. I'd just reiterate two general points of critique: 1. The point of establishing connections between texts is  semantic  and terms can have vastly different semantic meanings dependent on the sphere of discourse in which they occur. Because of the way LLMs work, the really  novel  connections probably won't be found by an LLM since the way they function is quite literally to uncover what  isn't novel . 2. Part of the point in making these connections is the  process  that acts on the human being making the connections. Handing it all off to an LLM is no better than blindly trusting authority figures. If you want to use LLMs as generators of possible starting points or things to look at and verify and research yourself, that seems totally fine. I really like the idea of the topic tree. That intuitively resonates. I did a similar thing with productivity books early last year, but never released it because it wasn't high enough quality. I keep meaning to get back to that project but it had a much more rigid hypothesis in mind - trying to get the kind of classification from this is pretty difficult and even more so to get high value from it. The mental model I had of this was actually on the paragraph or page level, rather than words like the post demos. I think it'd be really interesting if you're reading a take on a concept in one book and you can immediatel", "negative": "We X-Rayed a Suspicious FTDI USB Cable. I have a slow burn project where I simulate a supply chain attack on my own motherboard. You can source (now relatively old) Intel PCH chips off Aliexpress that are \u201cunfused\u201d and lack certain security features like Boot Guard (simplified explanation). I bought one of these chips and I intend to desolder the factory one on my motherboard and replace it with the Aliexpress one. This requires somewhat difficult BGA reflow but I have all the tools to do this. I want to make a persistent implant/malware that survives OS reinstalls. You can also disable Intel (CS)ME and potentially use Coreboot as well, but I don\u2019t want to deal with porting Coreboot to a new platform. I\u2019m more interested in demonstrating how important hardware root of trust is. Yeah - these [0] kinds of cables are so extremely scary. \"The O.MG Cable is a hand made USB cable with an advanced implant hidden inside. It is designed to allow your Red Team to emulate attack scenarios of sophisticated adversaries\" \"Easy WiFi Control\" (!!!!!) \"SOC2 certification\"? Dawg, the call is coming from inside the house... [0]  https://shop.hak5.org/products/omg-cable  it's a serious problem they could be regulated to expose their chip with transparent covering rather than plain dark wiring Jeese. I was not sure which image was the suspect one. Just to be clear suspicious in this sense is a cable that is likely counterfeit and wasn't able to do high speed transfer unlike the genuine known good one. this is an advertisement for the company Related  USB-C head-to-head comparison  (389 points, 2023, 219 comments)  https://news.ycombinator.com/item?id=37929338  To be fair, this story is basically an ad, but a pretty good one, and many featured HN stories are really marketing. Personally, I don\u2019t mind marketing stuff, if it\u2019s interesting and relevant (like this). But the fact that most comms cables, these days, have integrated chips, makes for a dangerous trust landscape. That\u2019s something"}
{"anchor": "Gemini CLI. Link of announcement blog post:  https://blog.google/technology/developers/introducing-gemini...  These always contain easter eggs. I got some swag from Claude Code, and as suspected, Gemini CLI includes `/corgi` to activate corgi mode. > That\u2019s why we\u2019re introducing Gemini CLI Definitely not because of Claude Code eating our lunch! symptomatic of Google's lack of innovation and pm's rushing to copy competitor products better question is why do you need a modle specific CLI when you should be able to plug in to individual models. The killer feature of Claude Code is that you can just pay for Max and not worry about API billing. It lets me use it pretty much all the time without stressing over every penny or checking the billing page.\nUntil they do that - I'm sticking with Claude. Hope this will pressure Anthropic into releasing Claude Code as open source. I have been using this for about a month and it\u2019s a beast, mostly thanks to 2.5pro being SOTA and also how it leverages that huge 1M context window. Other tools either preemptively compress context or try to read files partially. I have thrown very large codebases at this and it has been able to navigate and learn them effortlessly. I love how fragmented Google's Gemini offerings are. I'm a Pro subscriber, but I now learn I should be a \"Gemini Code Assist Standard or Enterprise\" user to get additional usage. I didn't even know that existed! As a run of the mill Google user I get a generous usage tier but paying them specifically for \"Gemini\" doesn't get me anything when it comes to \"Gemini CLI\". Delightful! I neeeed this google login method in sst's opencode now haha Ugh, I really wish this had been written in Go or Rust. Just something that produces a single binary executable and doesn't require you to install a runtime like Node. Why would someone use this over aider? I played around with it to automate GitHub tasks for me (tagging and sorting PRs and stuff). Sometimes it needs a little push to use th", "positive": "I built my own CityMapper. Before Citymapper existed, there was OneBusAway, a Ph.D. student project at the University of Washington. It still exists and powers millions of transit rider trips every day all around the world in Seattle, Washington DC, New York City, Poznan Poland, Buenos Aires Argentina, Adelaide Australia, and who knows where else. If you\u2019re interested in hacking on something like Citymapper, or setting up an OBA server for your own city, you can find everything you need on our GitHub organization:  https://github.com/OneBusAway  That includes docker images, an iOS app and a trip planner framework, android app, Sveltekit web app, and even a next generation OBA server written in Go. As far as the data to power this, you can get GTFS for every US transit agency from  https://mobilitydatabase.org/  (nb I\u2019ve been involved in the OBA project since 2012) Why are the table and the description of the RAPTOR algorithm in the article images rather than text? During university, we've built OptiTravel ( https://github.com/denysvitali/optitravel ) to do something similar. We couldn't use Google Maps APIs (project requirement), so we wrote a custom routing algorithm based on A* and I've created a Rust server to host GTFS data ( https://github.com/denysvitali/gtfs-server ) \u00e0 la Transitland ( https://transit.land/ ). Performance wasn't great since everything had to run locally and do network roundtrips, but it found routes in my hometown that Google Maps didn't show. Pretty cool discovering hidden connections in the transit network and being able to customize your own params ( https://github.com/denysvitali/optitravel/blob/master/src/ma... ) I am involved with the OpenTripPlanner project, which is a Java trip planning application that also uses the RAPTOR algorithm! It\u2019s used in cities all over the world, with the biggest deployment being ENTUR\u2019s in Norway, which covers the entire country. I believe all trip planning apps in Norway use this deployment. It supports m", "negative": "JuiceSSH \u2013 Give me my pro features back. Wow nice work. Thanks for doing this and writing it up. Damn. I especially liked the cloud backup & sync. Any good alternatives? This might be a good plug for Morphie or Revanced patches to automate the patch process. I haven't used my Pro purchase in years, but if I did want to ssh from my phone today, I'd use the newish Terminal app, available since Android 15. It's a full Debian virtual machine. smali code is funny to read, basically an object-oriented assembly language (feels so wrong) I just tried to purchase pro from within the app just to see what the price is, and the Google Play purchase popup tells me it's not available. Interesting. Wow. Thanks for this. I haven't logged into Juice SSH in years, but i thought it had all my ssh keys backed up in the cloud. > JuiceSSH used to be the best SSH client available on Android until December 2025. Really? I always gave that award to Termius, which is kind of my second best behind Servercat which I miss very dearly from the iOS environment. Really great terminal app that I used in Android for a very long time with some interesting features. Also, Mosh shell support for sshing in degraded connection environments! Replaced JuiceSSH two years ago with ConnectBOT ( https://play.google.com/store/apps/details?id=org.connectbot... ) as a \"free\" alternative. Never looked back. Not trying to defend the developer here but they went really silent once before like this. Then came out of the gate with a bunch of updates and new features. \nI'm hoping they've just got really busy with life, I know when I emailed them before they have been responsive and helpful. \nI mean hell they might have died? Does the Store have a process for this? \nThis app has been around a long time so I don't understand the rugpull comments. \nAlso the syned keys are (supposedly, I guess we don't have the source) encrypted so even if the dev is no longer active that aspect should be secure I hope. My Pro features sti"}
{"anchor": "Ask HN: If you had $10M in the bank, would you still show up to your job?.  https://news.ycombinator.com/item?id=46545404  I believe that unless one manages to regress to our primal animal state the brain has a need to think about something in order not to think about self and the possible dark existential stuff which should absolutely be ignored and avoided. In this scenario the best possible way to occupy the brain is to set goals and build discipline towards reaching such goals. Even pleasurable stuff like music or social connection or even I'd go as far as sex might seem 'not work' on day-1 after you fire yourself after receving the 10mil cheque But On day 60 after leaving work with 10m 1) the 'fucking around' on the fretboard becomes 'practicing scales for at least 30 mins' 2) the hanging out at the bar becomes 'organizing parties in a way to maximize social fun with games etc' 3) the 'ONS from the club' becomes 'trying to find an escort with girl-next-door look who'd also offer Pornstar sex service and greek sex service' Every human endevour of any kind has an S-curve type shape where after a while if you want to progress and get novelty from higher experiences you must apply IQ and discipline and so it becomes a 'work' Leonardo Da Vinci after having signed off all the accomplishments that we know basically turned wedding planner and party organizer in Milan , I suppose orgy organizer too but don't quote me on that, and guess what? After 60 days or so it became a 'job' for him to put the pieces together in a way to reach an amazing social result. Same with today marriages, happiest day of her life? It's the most work of her life too to get those 8 hours or whatever is the party lenght exactly right I would still do  a  job, but it would be something that is important to me. And $10M would require some up-front management and ongoing maintenance to develop an index-tracked revenue stream from it. I mean, aside from an initial disbursement meant to wipe out harm", "positive": "Miami, your Waymo ride is ready. I\u2019m going to Miami next week.  Time for my first WayMo ride. We got these in Atlanta.  I haven't had the chance to ride yet but watching them it's pretty clear that they're legit. I think we're on the cusp of something that will change the landscape of our cities.  It's going to revolutionize getting around and take a chunk out of the land dedicated to parking. Still can't believe the prices are comparable to Uber, sometimes costing even more. It should be significantly less to the point it drives Uber out of business. Is Waymo close to bankruptcy, unable to be profitable, or are they just greedy? Waymo is such an interesting case study. For most other ~AI deployments you have strong public reaction to the proliferation of slop, non-human failure modes, cost cutting at the expense of quality, etc. But I haven't met a single person who doesn't like the experience of Waymo. They ended up cracking the code on what I suspect people really want: - consistent car quality - safety of the drive (conservative driving and potential fear of drivers) - no randomly chatty driver All of those feel like a breath of fresh air especially when stacked up against the current state of Uber & Lyft rides. People really just want consistency. I don't actually think you needed AI to get there (I've had occasional rides in black cars that provided the same experience). Waymo was just right time, right place, right price. Funny that they apparently didn't include South Beach, at least according to the map. The US would benefit much more from a good railroad system. Everybody can drive a car. They have solved the wrong problem. Why would I use Waymo if an Uber/Lyft costs the same? If it gets in an accident, who pays my medical bills? I was at a conference in Phoenix in November and took seven Waymo trips during my stay. Four of those were fairly long (20-minute) trips. I preferred Waymo to the Uber/Lyft experience because it felt private. It was just me and my", "negative": "Tesla\u2019s autonomous vehicles are crashing at a rate much higher tha human drivers. To be honest I think the true story here is: > the fleet has traveled approximately 500,000 miles Let's say they average 10mph, and say they operate 10 hours a day, that's 5,000 car-days of travel, or to put it another way about 30 cars over 6 months. That's tiny! That's a robotaxi company that is literally smaller than a lot of taxi companies. One crash in this context is going to just completely blow out their statistics. So it's kind of dumb to even talk about the statistics today. The real take away is that the Robotaxis don't really exist, they're in an experimental phase and we're not going to get real statistics until they're doing 1,000x that mileage, and that won't happen until they've built something that actually works and that may never happen. By the law of large numbers, it's not a significant distance. As long as there are still safety drivers, the data doesn't really tell you if the AI is any good. Unless you had reliable data about the number of interventions by the driver, which I assume Tesla doesn't provide. Still damning that the data is so bad even then. Good data wouldn't tell us anything, the bad data likely means the AI is bad unless they were spectacularly unlucky. But since Tesla redacts all information, I'm not inclined to give them any benefit of the doubt here. The comparison isn't really like-for-like. NHTSA SGO AV reports can include very minor, low-speed contact events that would often never show up as police-reported crashes for human drivers, meaning the Tesla crash count may be drawing from a broader category than the human baseline it's being compared to. There's also a denominator problem. The mileage figure appears to be cumulative miles \"as of November,\" while the crashes are drawn from a specific July-November window in Austin. It's not clear that those miles line up with the same geography and time period. The sample size is tiny (nine crashes)"}
{"anchor": "GLM-4.7-Flash. Any cloud vendor offering this model? I would like to try it. Not much info than being a 31B model. Here's info on GLM-4.7[0] in general.  I suppose Flash is merely a distillation of that. Filed under mildly interesting for now.  [0]  https://z.ai/blog/glm-4.7  Seems to be marginally better than gpt-20b, but this is 30b? Looks like solid incremental improvements. The UI oneshot demos are a big improvement over 4.6. Open models continue to lag roughly a year on benchmarks; pretty exciting over the long term. As always, GLM is really big - 355B parameters with 31B active, so it\u2019s a tough one to self-host. It\u2019s a good candidate for a cerebras endpoint in my mind - getting sonnet 4.x (x<5) quality with ultra low latency seems appealing. Excited to test this out. We need a SOTA 8B model bad though! Interesting they are releasing a tiny (30B) variant, unlike the 4.5-air distill which was 106B parameters.  It must be competing with gpt mini and nano models, which personally I have found to be pretty weak.  But this could be perfect for local LLM use cases. In my ime small tier models are good for simple tasks like translation and trivia answering, but are useless for anything more complex.  70B class and above is where models really start to shine. Great, I've been experimenting with OpenCode and running local 30B-A3B models on llama.cpp (4 bit) on a 32 GB GPU so there's plenty of VRAM left for 128k context. So far Qwen3-coder gives the me best results. Nemotron 3 Nano is supposed to benchmark better but it doesn't really show for the kind of work I throw at it, mostly \"write tests for this and that method which are not covered yet\". Will give this a try once someone has quantized it in ~4 bit GGUF. Codex is notably higher quality but also has me waiting forever. Hopefully these small models get better and better, not just at benchmarks. I'm trying to run it, but getting odd errors.\nHas anybody managed to run it locally and can share the command? What\u2019s the ", "positive": "Iran Protest Death Toll Could Top 30k, According to Local Health Officials. Very tragic. May the souls that gave their lives for freedom live in the memory of the people of Iran as a blessing. The simple absence of on the ground reports from a variety of independent sources tells me that these numbers should not be simply ignored. If there\u2019s nothing happening, then the obvious way for the authorities to prove that is to let observers in, and let independent information out. They do not do this, so I will take these reports of deaths more seriously. How many on the government side, I wonder. There are wars that haven't killed so many people. This seems like another revolution. I guess this will be a difficult question to ask.  I have no doubt the numbers are high but there is something odd about the videos that leak out.  The sound of the guns are enhanced  for psychological effect?  and in the cases where a gunner on a truck is moving down a road purportedly mowing people down there is no blood on the road where the protestors had been standing, no bodies and we never see the people being shot.  It's not like I want to see people being shot but I've also seen a lot of fake mass shooting videos in the past decade.  There's no shortage of real uncensored footage of killing in Ukraine.  Why is everything censored for Iran? That's way higher than I thought. Is there any evidence? Dresden was 25,000, and the V2 and V1 campaigns had less numbers. So this is high even for an  aerial bombing  campaign. [edit] I don't get why I'm getting downvoted. Are people making assumptions because I mentioned Dresden? Get a hold of yourself. > As of Saturday, the U.S.-based Human Rights Activists News Agency said it had confirmed 5,459 deaths and is investigating 17,031 more. The 30,000 number comes from the Ministry of Health. It seems the UN number also aligns with the new 30,000 number. This is much worse than the 3,000 that was reported earlier. But it also seems like the crackdown ", "negative": "OpenSSL: Stack buffer overflow in CMS AuthEnvelopedData parsing. Can someone translate \"Applications and services that parse untrusted CMS or PKCS#7 content using AEAD ciphers (e.g., S/MIME AuthEnvelopedData with AES-GCM) are vulnerable\" to human? 2026 and we still have bugs from copying unbounded user input into fixed size stack buffers in security critical code. Oh well, maybe we'll fix it in the next 30 years instead. Is this really exploitable? Is stack smashing really still a thing on any modern platform? Another \"fix\" in the long line of OpenSSL \"fixes\" that includes no changes to tests and therefore can't really be said to fix anything. Professional standards of software development are simply absent in the project, and apparently it cannot be reformed, because we've all been waiting a long time for OpenSSL to get its act together. Looks like Debian and some other distros are still on the vulnerable 3.5.4. Why did Openssl publish before the distros rolled to the fixed version? Very strange, as I type this both Bullseye and Bookworm are marked as fixed but Trixie isn't yet:  https://security-tracker.debian.org/tracker/CVE-2025-11187  I'd encourage folks to read the recently-published statement [1] about the state of OpenSSL from Python's cryptography project. [1]:  https://news.ycombinator.com/item?id=46624352  Has anyone built OpenSSL with -fbounds-safety? I just looked at the vuln in detail. If you are using OpenSSL compiled with Fil-C, then you're safe. This attack will be nothing more than a denial of service (the attacker won't get to actually clobber the stack, or heap, or anything). Services that process CMS[1] or PKCS#7 envelopes may be vulnerable to this bug. The most common example of these is S/MIME (for signed/encrypted email), but PKCS#7 and CMS show up in all kinds of random places. (Unless I'm missing something, a key piece of context here is that CMD/PKCS#7 blobs are typically allowed to select their own algorithms, at least within an allowlist"}
{"anchor": "Where can you go in Europe by train in 8h?. If you now could just book a train between these cities on a common european platform (or local transportation provider...)... one could dream... just booking a train and getting a quote crossing multiple borders (without interrail) is just a nightmare :( Title shared on HN left me somewhat disappointed.  The actual time appears to be \"Where can you go by train in 8h?\", though that's somewhat less clear.  It only seems to include central stations of larger cities, though I was hoping for a list of shortest travel times between stations in Europe, as more of a thought/data experiment.  Or put another way; which two train stations in Europe have the least distance between them? Anyway, the shared feature is neat, but seems to be somewhat iffy once you get out of the bigger cities.  If a route has 2 or more connections, it seems to struggle to show them.  While true to its message, I still feel the restriction of 8 hours misses sleeper trains, where travel time is less essential compared to daytime trains. It's cute for discoverability, but for a specific train search, I would definitely defer to bahn.de, which basically includes all train stations in Europe. There is a website I love for seeing how to get almost everywhere in Europe by train:  https://www.seat61.com/  I don't understand how it works. First time clicking on Poland, it showed a kind of a heat map around some city. Then I click on another location and nothing happens. OK, there's a \"back\" button, I go back, click on the map again in a different place and... nothing happens. No heat map. At some point in frustration I accidentally move the mouse while clicking and the map rotates upside down. Don't know, is it me, my browser, or there's something about the UI. Since train fans always like to point this out when it comes to flying: this is how far you can get in 8 hours  on the train . It doesn\u2019t include the time to get to the station, the buffer time you need (i", "positive": "Show HN: Ten years of running every day, visualized. Love it! How did you stay motivated? Do you have the source/pipeline available? I love the design and would want to do something similar for my own runs. Congrats on the decade! Did you ever focus on specific metrics or was it always just about the run? just wanted to say the site looks awesome! I love the minimal black+white/grayscale and the fonts are just lovely. vis looks great too, I enjoyed poking around nearly all of the unique runs to look at the map and paces. This is so cool! At what point did you start thinking about this project? Like, were you quietly working on it a year ago after every run, just waiting for this moment? And hey, great run in Japan! (Tokyo here!) I love the map visualization too. Love it! I will hit one year mark in a couple of weeks. Currently maintaining stats in a Google spreadsheet :)  https://vijaykillu.com/  SVGs? So, some of the staistics graphs do not update, or have you made them dynamic by hand? beautifullllllll\u2014both the streak and the stack. Love how lightweight the architecture is for something so personal and long-term. Curious if you noticed any patterns in the data that surprised you once you visualized it? Impressive. I did streak running for 6 months nice and it was some of the most productive running in my life. Interestingly I have much higher yearly averages than you do but still consider daily streak running quite hard. Not being a morning runner myself might contribute since I get into a lot of close calls that way. My streak literally ended when my daughter went into the hospital and I couldn\u2019t well just fuck off for a run any longer. That's awesome! any tips for people who are just starting out? do you have code it on github ? I don't have the tenacity to run strictly _everyday_, so as a middle ground I don't run when it rains at anytime during daylight. Of course the effectiveness of this rule depends on where you live :P I\u2019ve always wanted to do this, but I ", "negative": "ASML staffing changes could result in a net reduction of around 1700 positions. Apologies for the Dutch source, but I couldn\u2019t find any source in English yet. \u201c ASML plans to eliminate approximately 3,000 of its 4,500 management positions in engineering. The expectation is that approximately 1,400 people will be able to move into new engineering roles.\u201d Are we seeing big engineering manager cuts in the US too? > ASML also announced a new share buyback programme of up to \u20ac12 billion, to be executed by 31 December 2028. Oh boy. This fills me with dread. I've never seen a company that starts doing buybacks not become a financialized hollow shell within a decade. Being an irreplaceable monopoly on the commanding heights of the digital economy makes this even worse. ASML understands what most big companies don't. If you don't reach your targets it's not the engineers fault. It's bad management ;) > Engineers in particular have expressed their desire to focus their time on engineering, without being hampered by slow process flows [1] I wonder what correlation will exist between the set of people who end up leaving the company, and the set of people responsible for setting up those \"slow process flows\" in the first place. [1]  https://www.asml.com/en/news/press-releases/2026/strengtheni...  Can someone explain, why this is done?\nI get a feeling, it's normally done when a company is in trouble or will soon? But they should have more money than ever. They say it is to focus on innovation, but if you are a smart young person in NL, would you want to work where they just fired 1700 people? And if you already work there and are a top player it is a good time to rethink?\nA company I know wanted to focus, instead of firing, they sold the parts of the company they felt did not fit their future vision for money. The press release ( https://www.asml.com/en/news/press-releases/2026/strengtheni... ) seems remarkably to the point, for CEO press release standards. I'm impressed by their"}
{"anchor": "A real-time 3D digital map of Tokyo's public transport system. Very cool. Even the building-by-building graphics seem to be correct: a boxy version of my house in Yokohama is in the correct location and has the correct height relative to its neighbors. The map also shows\u2014correctly\u2014that it is raining at this moment in Tokyo but not in Yokohama. This is great, however on first load I didn't get the trains moving. After a refresh they showed up again. Currently sitting on the Yokohama line to Hachioji, a little before Hashimoto station. Looking at the map the train had already reached Hashimoto. Seems like we're running 30 seconds or maybe 1 minute late. Do any of the 'live' camera feeds work? They're all static for me. This is super cool, though. Wow, I love that it shows live flights and airplanes! This is really awesome, I love the way you integrated the live camera feeds. Looking at this map makes me want to move to Tokyo. Sure, the trains stopping at night makes nightlife and catching a morning flight annoying, but train culture* of just making plans to meet at a train station with a friend is so much better than the car dependent place I live. *It's not unique to Tokyo, but I've spent extended periods of time in cities with trains and this is what we often did. Tokyo just has lots of train lines. I saw 3D in the title and assumed it was a cross-section view of the subway tunnels underground. An implementation like that would be a potential security risk to public infrastructure. Berlin edition:  https://www.vbb.de/fahrinfo , there was also a version in a similar 3D style but I wasn't able to dig it up through the search. Related thread with more of these kind of projects:  https://news.ycombinator.com/item?id=32647227  That was disappointing I thought I would see the 3D train track tubes and how deep they are and their position from each other in 3D I just came from working remotely from Japan for almost two months. One of the highlights was the infrastructure fo", "positive": "X For You Feed Algorithm. anything interesting? anything that is a surprise? what is the difference between this and  https://github.com/twitter/the-algorithm  I did not expect to see Rust. They seem to have forgotten to commit Cargo.toml though. Oh I see it is not meant to be built really. Some code is omitted. ooh, LLM Recsys alert! (we had an LLM Recsys track at ai.engineer last year). official announcement here:  https://x.com/XEng/status/2013471689087086804  looks like this is the \"for you\" feed, once again shared without weights so we only have so much visibility into the actual influence of each trait. \"We have eliminated every single hand-engineered feature and most heuristics from the system. The Grok-based transformer does all the heavy lifting by understanding your engagement history (what you liked, replied to, shared, etc.) and using that to determine what content is relevant to you.\" aka it's a black box now. the README is actually pretty nice, would recommend reading this. it doesnt look too different form Elon's original code review tweet/picture  https://x.com/elonmusk/status/1593899029531803649?lang=en  sharing additonal notes while diving through the source:  https://deepwiki.com/xai-org/x-algorithm  and a codemap of the signal generation pipeline:  https://deepwiki.com/search/make-a-map-of-all-the-signals_3d...  - Phoenix (out of network) ranker seems to have all the interesting predictive ML work. it estimates P(favorite), P(reply), P(repost), P(quote), P(click), P(video_view), P(share), P(follow_author), P(not_interested), P(block_author), P(mute_author), P(report) independently and then the `WeightedScorer` combines them using configurable weights. there's an extra DiversityScore and OONScore to add some adjustments but again dont know the weights  https://deepwiki.com/xai-org/x-algorithm/4.1-phoenix-candida... \n- other scores of interest: photo_expand_score, and dwell_score and dwell_time. share via copy, share, and share  via dm are all obvi", "negative": "I'm addicted to being useful. I can relate to this. I find that I have the same issue. If this resonates with you, I highly recommend picking up a copy of Tracy Kidder's 1981 novel  The Soul of a New Machine . You'll be hooked by the end of the introduction. Can definitely relate to this. But I have found that, when running a team, it can be very counter productive. If you constantly solve all the problems that come it can be stifling for the people you manage. Help is the sunny side of control. >  I don\u2019t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional As a fellow traveller, I offer one caution: learn to turn this down in personal relationships as it can be counterproductive. It took decades for my wife to finally get through and explain not every problem she voices is something that needs a solution. Some times people just want to be heard. It bugs the hell out of me because I tend to need to solve All The Problems before I can do any self-care, but rather than seem heroic, I think this attitude can seem transactional or uncaring as though everyone is just a screw that needed a bit of tightening, etc. I wonder if this sort of thing can lead to faster burnout or such. I've sorta over time leaned toward guarding my own space/time since somehow I get more tired out, and over time more burned out, if I don't. I can very much relate to the OP in this. I enjoy writing code, figuring out problems, finding solutions and in general helping other people with things that require some kind of software to be created or updated. And until year or two ago I thought I'd be able to continue to do what I love while getting paid decent money for it. With the advent of vibe coding and AI I'm starting to feel less sure in the future. I feel the same way. I retired last summer, but that only means that I found a place that needs me, where I can work part time without worrying too much about money. I remember, decades ago, r"}
{"anchor": "My trick for getting consistent classification from LLMs. If you already have your categories defined, you might even be able to skip a step and just compare embeddings. I wrote a categorization script that sorts customer-service calls into one of 10 categories.  Wrote descriptions of each category, then translated into embedding. Then created embeddings for the call notes and matched to closest category using cosine_similarity. Arthur\u2019s classifier will only be as accurate as their retrieval. The approach depends on the candidates to be the correct ones for classification to work. Under-discussed superpower of LLMs is open-set labeling, which I sort of consider to be inverse classification. Instead of using a static set of pre-determined labels, you're using the LLM to find the semantic clusters within a corpus of unstructured data. It feels like \"data mining\" in the truest sense. Dunno if this passes the bootstrapping test. This is sensitive to the initial candidate set of labels that the LLM generates. Meaning if you ran this a few times over the same corpus, you\u2019ll probably get different performance depending upon the order of the way you input the data and the classification tag the LLM ultimately decided upon. Here\u2019s an idea that is order invariant: embed first, take samples from clusters, and ask the LLM to label the 5 or so samples you\u2019ve taken. The clusters are serving as soft candidate labels and the LLM turns them into actual interpretable explicit labels. I think a less order biased, more straightforward way would be just to vectorize everything, perform clustering and then label the clusters with the LLM. Nice! So the cache check tries to find if a previously existing text embedding has >0.8 match with the current text. If you get a cache hit here, iiuc, you return that matched' text label right away. But do you also insert a text embedding of the current text in the text embeddings table? Or do you only insert it in case of cache miss? From reading the ", "positive": "Level S4 solar radiation event. Possible aurora visible through central US tonight This page looks like an accessibility nightmare. The entire warning text is an image. There is no transcription present for screen reader users. I did not expect this from a government website. Nice, you can already see some solar flares in Austria again.  https://www.foto-webcam.eu/webcam/kleinfleisskees/   https://www.foto-webcam.eu/  It seems that the peak was several hours ago, and I haven't observed any effects from it... We had intense aurora in Berlin, Germany. Green clouds dancing in the sky levels. Started around 22:10 local time or a bit earlier, and at this point there's only a faint red/green glow remaining. Do you need long exposure to make it visible with a camera? How does that work in the presence of light pollution? If anyone is interested in what \"G4\" means in context, here's the scale:  https://www.swpc.noaa.gov/noaa-scales-explanation  PJM had some geomagnetic disturbance warnings, but did not progress to the alert stage or grid re-configuation actions. So, no US power grid problems.       104955 Warning Geomagnetic Disturbance Warning 01.19.2026 14:30 \n    PJM-RTO\n    A Geomagnetic Disturbance Warning has been issued for\n    14:30 on 01.19.2026 through 16:00 on 01.19.2026 .\n    A GMD warning of K8 or greater is in effect for this period. \n    End time: 01.19.2026 16:00 \n  \n(All times are prevailing Eastern US time) I've posted on this before, for other warnings. Not going to repeat that. Years ago I was concerned about this and made a plan with my wife for what to do if she was at work. But now we have a bunch of kids in different schools and haven't updated our plan. Does anyone have a plan for what happens if we have a really bad event? Weirdly, while the site in question is \"blaring klaxons!\" there are more \"cool night lights!\" posts than concern. Australian Bureau of Meteorology advisory for visible aurora:  https://www.sws.bom.gov.au/Aurora  I'll be going out", "negative": "FBI is investigating Minnesota Signal chats tracking ICE. Tracking the murderers who executed citizens in the street and then fled the scene of the crime and any sort of trial or investigation? That ICE and Immigration and Border Patrol? I wonder why. And since when is tracking public officials operating in public in the capacity of their government jobs illegal? These federal goons need to be tracked and observed to record their crimes. That much is indisputable. i suppose what he means is that the  phones  of protestors which have signal chat will be investigated. Assuming they dont have disappearing messages activated, and assuming any protestors willingly unlock their phones. Why? That's unequivocally constitutionally protected speech. Why is our tax money being wasted on this? I don't know signal very well but when I have spoken to others about it they mention that the phone number is the only metadata they will have access to. This seems like a good example of that being enough metadata to be a big problem. A wise man told me, you know signal works because its banned in Russia. I also find it incredibly ironic that they have a problem with this, when the DoD is flagrantly using signal for classified communications. I have seen anti-Signal FUD all over the place since it was discovered that protesters have been coordinating on Signal. Here\u2019s the facts: - Protesters have been coordinating using Signal - Breaches of private Signal groups by journalists and counter protesters were due to poor opsec and vetting - If the feds have an eye into those groups, it\u2019s likely that they gained access in the same way as well as through informants (which are common) - Signal is still known to be secure - In terms of potential compromise, it\u2019s much more likely for feds to use spyware like Pegasus to compromise the endpoint than for them to be able to break Signal. If NSA has a Signal vulnerability they will probably use it very sparingly and on high profile foreign targets. - T"}
{"anchor": "280M e-bikes and mopeds are cutting demand for oil far more than electric cars. We don't need a 4000 lb vehicle to move a ~200 lb person. In order of efficiency: (1) Walk (2) Unicycle, roller skate, scooter (no battery, very little material) (2) Bike (3) Electric bike (and all forms of newfangled electric: escooters, segways) (4) Electric motorbike or scooter (5) Mass transit (can be public/private) transportation: Electric trains (6) Mass transit (can be public/private) transportation: Electric buses (7) Zipline (8) Carpools on BEV (9) Carpools on PHEV (10) BEV We can stop buying gas cars. Pollution kills 10 million EVERY year[1]. For context, the cumulative COVID deaths over 3 years are ~6.5 million. And fossil fuels are subsidized (Trillions of dollars per year). For 2022, this is $7 trillion[2]. Why are we subsidizing fuels that are proven to cause all kinds of diseases (nearly everything except STIs). [1] Air Pollution Kills 10 Million People a Year. Why Do We Accept That as Normal?:  https://www.nytimes.com/2022/07/08/opinion/environment/air-p...  [2] Why Are Governments Still Subsidizing Fossil Fuels?  https://www.bloomberg.com/opinion/articles/2023-10-16/climat...  [3]  https://www.imf.org/en/Blogs/Articles/2023/08/24/fossil-fuel...  I'm not really sure where in the US people live that makes them think transportation can be replaced by a bike. I live in San Diego, the climate is great, but there is no way a person can travel any farther than their neighborhood on a bike. The main impediment at this point is the outrageous price of EVs in the US. In China cheap EVs are readily available, trade policies are preventing their import into the US. Bikes, \"e\" or otherwise are a great way to get around the neighborhood, but most people are not able to restrict their travel to a 10 mile radius. And weather as well as traffic safety are serious mitigations of bike transport. \"So what\u2019s the best solution? You might think switching to an electric vehicle is the natural ", "positive": "De-dollarization: Is the US dollar losing its dominance? (2025). It's not losing it so much as that it is being destroyed on purpose. The international value of the dollar as a reserve and trade currency is inherently tied to the behavior of the US Government and the Federal Reserve. The behavior of the US Government has been very unusual lately, and the independence of the Federal Reserve is actively being challenged. So draw from that whatever conclusions you wish. Trump is destroying it intentionally. Compromised people and useful idiots within the Trump administration are being persuaded by Russia to destroy the dollar and break up NATO. No idea how those things work but surprised the $/\u20ac exchange rate stabilized. If the goal is to make US goods attractive to other countries and to decrease our trade deficit (not saying I agree with this goal), either the dollar has to become fundamentally weaker or the goods have to become more valuable.  The latter feels more difficult than the former at this point.  However, the side effects of a weaker dollar may not be worth weakening it. If Trump announces some toady lunatic to run the Fed, watch out below, because the dollar is going to crash.  I know I have moved a bunch of money into international stocks and currency and I suspect when the right leaning crowd finally catches on it will be a stampede. The export driven economies like China or the EU rely on the dollar to weaken their own currencies for competitive trade. Without it, natural FX mechanisms would naturally begin to appreciate their currencies and make their exports uncompetitive. The biggest problem of all social sciences is that they measure only what can be measured or is easier to measure. Sorry for the redundancy, but they don't see what is hard to see and, therefore, think it doesn't exist. I suspect there might be a lot of \"de-dollarization\" going on in realms that might not be easy to measure. To be specific: it is interesting that crypto-currencies ", "negative": "It's hard to justify Tahoe icons. I hate to be that guy, but there's some irony in putting a full screen animated snow effect over an article about unnecessary, distracting clutter. I like the article content but it's ironic that I needed to switch to Safari reader-mode to be able to comfortably read it. Reminder, you can turn off the animated snow effect with the snowflake icon at the top It's hard to justify snowflake animations on your website... Is there a reason Apple can\u2019t focus on system improvements instead of constantly tweaking with their UI so thoroughly every couple years? I don\u2019t disagree the OS UI needs to be revamped periodically, but it seems they do it too often. It's pretty wild how the best screenshot of a usable menu is from, like, Office 2000. What the hell have we been doing for the past 25 years? Sorry, even though I agree with your piece, the incredible irony of writing about  bad design decisions  while having  fucking snow flying past the text I'm trying to read  is just too much... [edit] I just discovered the snow icon, which does turn off the snow but turns the background into bright yellow. Oh and the other icon which turns your cursor into a ...spotlight? On an otherwise black page? Do I have that right? Which one of those things was a design decision that enhanced usability, or readability, or... anything at all? These choices can best be described as sophomoric. You can disagree with menu icons, but they at least  in theory  serve a  purpose . What purpose is served by any of the gizmos on this site? Related:  https://news.ycombinator.com/item?id=46196688  I suppose I wouldn't mind much either way, apart from the misaligned text maybe now that you brought my attention to it. I agree that colors could help. Don't hesitate to give KDE/Qt a try, it apparently happens to get all these things right according to this article from a quick glance: everything is correctly aligned, even when in the same menu some items both have an icon and a "}
{"anchor": "Show HN: Only 1 LLM can fly a drone. Why would you want an LLM to fly a drone? Seems like the wrong tool for the job -- it's like saying \"Only one power drill can pound roofing nails\". Maybe that's true, but just get a hammer LLMs flying weaponized drones is exactly how it starts. I think it's fascinating work even if LLMs aren't the ideal tool for this job right now. There were some experiments with embodied LLMs on the front page recently (e.g. basic robot body + task) and SOTA models struggled with that too. And of course they would - what training data is there for embodying a random device with arbitrary controls and feedback? They have to lean on the \"general\" aspects of their intelligence which is still improving. With dedicated embodiment training and an even tighter/faster feedback loop, I don't see why an LLM couldn't successfully pilot a drone. I'm sure some will still fall of the rails, but software guardrails could help by preventing certain maneuvers. I am curious how these models would perform and how much energy they'd take to semi-realtime detect objects:\nSmolVLM2-500M - Moondream 0.5B/2B/2.5B - Qwen3-VL (3B)\n https://huggingface.co/collections/Qwen/qwen3-vl  I am sure this is already worked on in Russia, Ukraine and The Netherlands. A lot can go wrong with autonomous flying.\nOne could load the VLM on a high end android phone on the drone and have dual control. Gemini 3 is the only model I've found that can reason spatially. The results here are accurate to my experiments with putting LLM NPCs in simulated worlds. I was surprised that most VLLMs cannot reliably tell if a character is facing left or right, they will confidently lie no matter what you do (even gemini 3 cannot do it reliably). I guess it's just not in the training data. That said Qwen3VL models are smaller/faster and better \"spatially grounded\" in pixel space, because pixel coordinates are encoded in the tokens. So you can use them for detecting things in the scene, and where they are ", "positive": "When AI 'builds a browser,' check the repo before believing the hype. I don't think the point was to say \"look, AI can just take care of writing a browser now\". I think it was to show just how far the tools have come. It's not meant to be production quality, it's meant to be an impressive demo of the state of AI coding. Showing how far it can be taken without completely falling over. EDIT: I retract my claim. I didn't realize this had servo as a dependency. I\u2019m super impressed by how \"zillions of lines of code\" got re-branded as a reasonable metric by which to measure code, just because it sounds impressive to laypeople and incidentally happens to be the only thing LLMs are good at optimizing. I love the quote from Gregory Terzian, one of the servo maintainers: > \"So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine.\" It hurts, that it wasn't framed as an \"Experiment\" or \"Look, we wanted to see how far AI can go - kinda failed the bar.\" Like it is, it pours water on the mills of all CEOs out there, that have no clue about coding, but wonder why their people are so expensive when: \"AI can do it! D'oh!\" AI will never be able to create a browser, just as AI was never able to defeat a chess grandmaster. If I was to spend a trillion tokens on a barely working browser I would have started with the source code of Sciter [0] instead. I really like the premise of an electron alternative that compiles to a 5MB binary, with a custom data store based on DyBASE [1] built into the front end javascript so you can just persist any object you create. I was ready to build software on top of it but couldn't get the basic windows tutorial to work. [0]  https://sciter.com/  [1]  http://www.garret.ru/dybase.html  You would think a CEO with a product that caters to developers would know that everyone was going to clone the repo and check ", "negative": "Surely the crash of the US economy has to be soon. Who do we expect will replace Americas global leadership and will they really be better for everyone? I do really hope the AI bubble will collapse soon. The sooner it blows the less damage it will do. And hopefully we can go back to doing real work without all these leadership guys breathing down our necks to see if we are doing enough of this AI all their shareholders want us to be involved in. It will suck even for us in europe due to shortsighted pension funds having invested in AI as well. But we'll just have to deal with it. I'm sure it will happen sooner rather than later. PS: I'm not an AI hater as such. It definitely has its usecases where it shines. The problem is like with all hypes; it's not good at  everything  and it won't be all golden mountains tomorrow like the investors expect. This overhyped investor circlejerk is what screws up technology. It happened to blockchain, it happened to metaverse. All things that have their merits but somehow investors thought it would change the world overnight and make them insta-rich. Obviously didn't happen and it won't happen now. Some sort of an AI crash / bubble bursting is expected to be honest - now if that will take the rest of the US economy as well.... debatable. Any strong opinions on this? Surely the surge of predictions of an incoming crash will never end though. I'm kinda new into economy crashes, was a kid in 2008, is there a way to protect of it? No one will ever get the timing right, but if you see the fundamental flaws of the economy, you know a crash is going to come. There were a lot of people who predicted the housing crash, not the timing but the crash. There are several signs that this is happening and the one no one is talking about is gold and silver prices. Don\u2019t worry about the timing, you\u2019ll never get the timing right, just worry about the fundamental economics and the flaws and protect yourself. I happen to agree just because of golden sil"}
{"anchor": "Maine\u2019s \u2018Lobster Lady\u2019 who fished for nearly a century dies aged 105. It always seems it's a fall that ends it, I wonder if she could have made 100 years on the water if she hadn't fell. What an inspiring life! Rugged individualism. Rest in peace, Queen. Who else reading the headline thought it\u2019s about 100 year old lobster that died? So Long, and Thanks for All the... Lobster. She must have had so many interesting stories to tell. Such an amazing experiences - born in the early 1920s, being a young adult at the beginning of the Second World War, seeing mass commercialization of air travel, flight to space, miniaturization and age of information. And she even caught beginnings of AI (or pseudo-AI). She \"picked\" a good place to live and observe the flow of time and events where she directly wouldn't be affected by various negative events throughout the century of her life. I wonder if there is a lobster that survived her. Lobsters are long-lived, they don't age (in the sense of slowly losing their fitness - senescence) and they only die when they grow too big and suffocate during moulting, or possibly catch some infection, or get killed by other animals/people. A 105 y.o. lobster is plausible. The article states that a raising amount of people in the US still work, albeit their age and it feels a little strange to me. Ginny, probably got so old because she was working and had a purpose every day. Being 100 and still capable of working is a blessing It says she died at 105 and spent almost a century fishing for lobsters. I doubt she was catching many at the age of five. I just lost my Mom, at 97. We would go to lunch on Tuesday and then grocery shopping. She'd talk of the family and where they all were and what they were doing - it was MY day to catch up. The last Tuesday we got back and she said \"That was too hard. I think that was the last one.\" I agreed, and thought I'd call her next tuesday just the same and see if she'd changed her mind. But there was no 'next tue", "positive": "Show HN: Ten years of running every day, visualized. Love it! How did you stay motivated? Do you have the source/pipeline available? I love the design and would want to do something similar for my own runs. Congrats on the decade! Did you ever focus on specific metrics or was it always just about the run? just wanted to say the site looks awesome! I love the minimal black+white/grayscale and the fonts are just lovely. vis looks great too, I enjoyed poking around nearly all of the unique runs to look at the map and paces. This is so cool! At what point did you start thinking about this project? Like, were you quietly working on it a year ago after every run, just waiting for this moment? And hey, great run in Japan! (Tokyo here!) I love the map visualization too. Love it! I will hit one year mark in a couple of weeks. Currently maintaining stats in a Google spreadsheet :)  https://vijaykillu.com/  SVGs? So, some of the staistics graphs do not update, or have you made them dynamic by hand? beautifullllllll\u2014both the streak and the stack. Love how lightweight the architecture is for something so personal and long-term. Curious if you noticed any patterns in the data that surprised you once you visualized it? Impressive. I did streak running for 6 months nice and it was some of the most productive running in my life. Interestingly I have much higher yearly averages than you do but still consider daily streak running quite hard. Not being a morning runner myself might contribute since I get into a lot of close calls that way. My streak literally ended when my daughter went into the hospital and I couldn\u2019t well just fuck off for a run any longer. That's awesome! any tips for people who are just starting out? do you have code it on github ? I don't have the tenacity to run strictly _everyday_, so as a middle ground I don't run when it rains at anytime during daylight. Of course the effectiveness of this rule depends on where you live :P I\u2019ve always wanted to do this, but I ", "negative": "Tesla\u2019s autonomous vehicles are crashing at a rate much higher tha human drivers. To be honest I think the true story here is: > the fleet has traveled approximately 500,000 miles Let's say they average 10mph, and say they operate 10 hours a day, that's 5,000 car-days of travel, or to put it another way about 30 cars over 6 months. That's tiny! That's a robotaxi company that is literally smaller than a lot of taxi companies. One crash in this context is going to just completely blow out their statistics. So it's kind of dumb to even talk about the statistics today. The real take away is that the Robotaxis don't really exist, they're in an experimental phase and we're not going to get real statistics until they're doing 1,000x that mileage, and that won't happen until they've built something that actually works and that may never happen. By the law of large numbers, it's not a significant distance. As long as there are still safety drivers, the data doesn't really tell you if the AI is any good. Unless you had reliable data about the number of interventions by the driver, which I assume Tesla doesn't provide. Still damning that the data is so bad even then. Good data wouldn't tell us anything, the bad data likely means the AI is bad unless they were spectacularly unlucky. But since Tesla redacts all information, I'm not inclined to give them any benefit of the doubt here. The comparison isn't really like-for-like. NHTSA SGO AV reports can include very minor, low-speed contact events that would often never show up as police-reported crashes for human drivers, meaning the Tesla crash count may be drawing from a broader category than the human baseline it's being compared to. There's also a denominator problem. The mileage figure appears to be cumulative miles \"as of November,\" while the crashes are drawn from a specific July-November window in Austin. It's not clear that those miles line up with the same geography and time period. The sample size is tiny (nine crashes)"}
{"anchor": "Miami, your Waymo ride is ready. I\u2019m going to Miami next week.  Time for my first WayMo ride. We got these in Atlanta.  I haven't had the chance to ride yet but watching them it's pretty clear that they're legit. I think we're on the cusp of something that will change the landscape of our cities.  It's going to revolutionize getting around and take a chunk out of the land dedicated to parking. Still can't believe the prices are comparable to Uber, sometimes costing even more. It should be significantly less to the point it drives Uber out of business. Is Waymo close to bankruptcy, unable to be profitable, or are they just greedy? Waymo is such an interesting case study. For most other ~AI deployments you have strong public reaction to the proliferation of slop, non-human failure modes, cost cutting at the expense of quality, etc. But I haven't met a single person who doesn't like the experience of Waymo. They ended up cracking the code on what I suspect people really want: - consistent car quality - safety of the drive (conservative driving and potential fear of drivers) - no randomly chatty driver All of those feel like a breath of fresh air especially when stacked up against the current state of Uber & Lyft rides. People really just want consistency. I don't actually think you needed AI to get there (I've had occasional rides in black cars that provided the same experience). Waymo was just right time, right place, right price. Funny that they apparently didn't include South Beach, at least according to the map. The US would benefit much more from a good railroad system. Everybody can drive a car. They have solved the wrong problem. Why would I use Waymo if an Uber/Lyft costs the same? If it gets in an accident, who pays my medical bills? I was at a conference in Phoenix in November and took seven Waymo trips during my stay. Four of those were fairly long (20-minute) trips. I preferred Waymo to the Uber/Lyft experience because it felt private. It was just me and my", "positive": "OracleGPT: Thought Experiment on an AI Powered Executive. Considering things like Palantir, and the doge effort running through Musk, it seems inconceivable that this is not already the case. I think I'm more curious about the possibility of using a special government LLM to implement direct democracy in a way that was previously impossible: collecting the preferences of 100M citizens, and synthesizing them into policy suggestions in a coherent way. I'm not necessarily optimistic about the idea, but it's a nice dream. This is an interesting and thoughtful article I think, but worth evaluating in the context of the service (\"cognitive security\") its author is trying to sell. That's not to undermine the substance of the discussion on political/constitutional risk under the inference-hoarding of authority, but I think it would be useful to bear in mind the author's commercial framing (or more charitably the motivation for the service if this philosophical consideration preceded it). A couple of arguments against the idea of singular control would be that it requires technical experts to produce and manage it, and would be distributed internationally given any countries advanced enough would have their own versions; but it would of course provide tricky questions for elected representatives in the democratic countries to answer. A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION. think we're already there aren't we? no human came out with those tariffs on penguin island The really nice thing about this proposal is that at least now we can all stop anthropomorphizing Larry Ellison, and give Oracle the properly robot-identifying CEO it deserves. You sometimes hear people say \"I mean, we can't just give an AI a bunch of money/important decisions and expect it to do ok\" but this is already happening and has been for years. Examples: - Algorithmic trading: I once embedded on an Options trading desk. The head of desk mentioned ", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
{"anchor": "Understanding Machine Learning: From Theory to Algorithms. Anyone who wants to demystify ML should read: The StatQuest Illustrated Guide to Machine Learning [0] By Josh Starmer.\nTo this day I haven't found a teacher who could express complex ideas as clearly and concisely as Starmer does. It's written in an almost children's book like format that is very easy to read and understand. He also just published a book on NN that is just as good. Highly recommend even if you are already an expert as it will give you great ways to teach and communicate complex ideas in ML. [0]:  https://www.goodreads.com/book/show/75622146-the-statquest-i...  I have read parts of it years ago. As far as I remember, this is very theoretical (lots of statistical learning theory, including some IMHO mistaken treatment of Vapnik's theory of structural risk minimization), with strong focus on theory and basicasically zero focus on applications. Which would be completely outdated by now anyway, as the book is from 2014, an eternity in AI. I don't think many people will want to read it today. As far as I know, mathematical theories like SLT have been of little use for the invention of transformers or for explaining why neural networks don't overfit despite large VC dimension. Edit: I think the title \"From theory to machine learning\" sums up what was wrong with this theory-first approach. Basically, people with interest in math but with no interest in software engineering got interested in ML and invented various abstract \"learning theories\", e.g. statistical learning theory (SLT). Which had very little to do with what you can do in practice. Meanwhile, engineers ignored those theories and got their hands dirty on actual neural network implementations while trying to figure out how their performance can be improved, which led to things like CNNs and later transformers. I remember Vapnik (the V in VC dimension) complaining in the preface to one of his books about the prevalent (alleged) extremism of", "positive": "Models of European metro stations. Nice! Would nice to have Maashaven Rotterdam, being the highest elevated one in the Netherlands.  https://en.m.wikipedia.org/wiki/Maashaven_metro_station  This is insane. Never saw anything like it. One minor nitpick: zooming the map is very slow (maybe Leaflet is not the best choice?). And the main station in Paris is missing: Ch\u00e2telet-Les Halles. Other than that, incredible work!! Amazing. A very cool project, and a great resource for people with reduced mobility - I semi-regularly use Transport for London's station drawings (linked on this website) over the official accessibility map, which doesn't differentiate between stairs and escalators for example. I was never able to build mental model of Alexanderplatz in Berlin. Most of the times was simply following the signs and yup, the layout is complicated. Holy shit! This is an incredible piece of work. And they are almost all drawn \u201cmanually\u201d! I am SO impressed by the dedication > For the last 10 years I have been able to draw around 2,547 stations > A pen, a notebook, a bit of spatial vision and the willingness to navigate all the staircases, corridors, platforms and mezzanines are enough to draw a station > Due to the boredom provoked by the COVID-19 lockdown in 2020, I decided to digitalize all the sketches I had drawn in since the early 2010s See also this 3D model of Shinjuku station, Tokyo:  https://satoshi7190.github.io/Shinjuku-indoor-threejs-demo/  Is there a reason why moscow is missing ? Incredible work! I first looked at _regular_ stations, but once I understood that it was done by a single guy, I had to look at Paris' Mordor: Ch\u00e2telet. The 3D view looks like an ants nest, as expected. Very impressed by the work done! Very impressive work. I also learned something, which I'd always wondered cynically but never thought to investigate. The walking connection between lines at some stations in Barcelona seems so long as to not make sense, but it's explained here that at t", "negative": "'Askers' vs. 'Guessers' (2010). Discussed (in a singleton sort of way) at the time:  Askers vs. Guessers  -  https://news.ycombinator.com/item?id=1956778  - Dec 2010 (1 comment) Edit: plus this!  Ask vs. Guess Culture  -  https://news.ycombinator.com/item?id=37176703  - Aug 2023 (479 comments) I found a good discussion that I keep referring to on Jean Hsu's blog:\n https://jeanhsu.substack.com/p/ask-vs-guess-culture \nand \n https://jeanhsu.substack.com/p/bridging-the-ask-vs-guess-cul...  It's been quite illuminating for people in multicultural teams... Edit: this whole theory seems to come from some internet forum comment! I know a lot of people here are seduced (I was a bit too) but basing your social interactions and how you see others and yourself on this stuff might not be the best thing to do! Original comment below for posterity and because there are answers. ---- I'm not sure this stuff is really  that  helpful. You might be tempted to put people into these categories, but you might have a somewhat caricatural and also wrong image of both which could worsen interactions. By the way, that article doesn't cite any studies! It's probably helpful to know people are more or less at ease asking direct questions or saying no or receiving a no, but it's all scales and subtleties. It could also depend on the mood, or even who one interacts with or on the specific topic). The article touches this a bit (the \"not black and white\" paragraph). We human beings love categories but categories of people are often traps. It's even more tempting when it's easy to identity to one of the depicted groups! I wonder if this asker-guesser thing is in the same pseudoscience territory as the MBTI. In the end, I suppose there's no good way around getting to know someone and paying attention for good interactions. > Your boss, asking for a project to be finished early, may be an overdemanding boor \u2013 or just an Asker, who's assuming you might decline. I don't pay for the Atlantic and thus a"}
{"anchor": "Text Is King. Another advantage of text over the long-term: it is accessible for discussion. Let us say that you want to analyze, say, drinking culture in Ireland. You could write documentary on it, or do a fictional character study. However, those require actors, camera equipment, editing tools and time, and it generally extremely expensive and time consuming. A quick TikTok video may be a bit cheaper than a full-scale film, but still needs some of that equipment and cinematography skills. Music is not much better. You need skills in singing, translating ideas of rhythmic lyrics, as well as supplies for instruments. Writing, however, is simple. At minimum, all you need is paper and skill in articulating ideas. Almost anyone worthy to rationally ponder a topic already has the skills to put it to paper (assuming that they have gone through a proper First-World education and know reading and writing). Text is also one of the easiest to share. A picture is worth a thousand words, but that poses problems in sending all that information. Plain text, however (or even most rich-text formats) can be transferred to anyone over almost any protocol, even rudimentary ones such as word-of-mouth. Ideas shared through text can be sent at an unrivaled pace. I would do more video, but video editing is  really difficult . I think that today\u2019s video influencers have gotten really good at \u201cone take and done\u201d recording. I couldn\u2019t do that. I\u2019m way too much of a perfectionist. I always edit my text, and I\u2019ve been writing all my life. I don\u2019t think that I\u2019ve  ever  written something perfectly, the first time (including HN comments. I tend to go back and edit for correctness and clarity). A couple of weeks ago, I was interviewed for a podcast. The process was fascinating, and the woman that did it, obviously does a great deal of editing and refinement. I don\u2019t know if I have that much patience. Text is my favourite minimalistic medium. I keep a minimum eye on regular news through teletext ", "positive": "My Life in Weeks. Powerful in how it puts it all into perspective is all I could say. I\u2019ve been using MarkWhen for a similar life timeline  https://markwhen.com  This was epic, thanks for sharing! This is a terrifying reminder of the shortness of our lives. I remember reading a blog by Tim Urban, where he showed that you could put all the weeks in your life on a single piece of A4 paper, and it didn\u2019t feel nice. Thanks for sharing, this format really puts things into perspective. This is fascinating. Idk if it was a good or bad thing. In college I once looked up some insurance chart of life expectancy probabilities. It puts things in perspective that\u2019s for sure. That was nice to watch. I spent about 25 minutes going through that. But it was horrifying for me. I realized that I wanted to see if the source code is available but then realized that I really don't remember those details. I remember random things for my childhood but I don't remember the date when I started elementary school. I know that I got my first computer when I was in third grade but don't remember the date. I don't even remember the date I started college and I probably wrote the wrong date couple of times during grad school application. While I started recording something less than a diary to record some of these but this was around covid. Thanks OP and HN for this reality check. From this view it's clear how wasteful ontogeny is. All of that physical and psychological development takes too much valuable time and investment. And we haven't even gotten to Gina's retirement years yet. Clearly the future is in using 3D bioprinting to build fully formed adults as if sprung from the brow of Zeus. Skill and memory transfer are a technical problem only as long as we cling to our bias against our artificially intelligent upgrades. Aging is defeated by implanting our old model weights into a new print. So much efficiency is waiting if we dare to free ourselves from convention. Woah, look at how sparse our", "negative": "Windows 11's Patch Tuesday nightmare gets worse. How can a company this big fail so hard in what one would consider their main* product still baffles me. *Yes, they probably make more revenue in Azure or Office365 licenses but at least when I think \u201cMicrosoft\u201d I immediately think Windows. > It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been. I think I might know... I'm wondering why the guy at Microsoft in charge of Windows is still employed. Over the prior weekend my installation of Playnite (a catalog/launcher for my games) was broken by the update, until I moved its data off of OneDrive[1]. And the other day I figured out that a couple of icons on my desktop had become completely inert and unresponsive due to the same bug - again due to an interaction between the Windows Shell and OneDrive. And this one I can't fix, I can't shift my desktop out of OneDrive. MS's strategy at this point is that Windows is a loss leader to get people onto the subscriptions for Office and OneDrive. So when the Windows team releases bugs that break usage of those services, forcing people off them onto alternative solutions, the guy in charge of those updates really needs to be answering some tough questions. [1] I've now got SyncThing handling this. Previous discussion: >Microsoft suspects some PCs might not boot after Windows 11 January 2026 Update  https://news.ycombinator.com/item?id=46761061  W11 is the best OS I've ever used, but everyone seems to hate it because Microsoft is so adamant in destroying its reputation by pushing Copilot and bugs instead of focusing on reliability. It's a shame. I see Microslop's \"AI\" coding mandate is continuing to go well [dupe]  https://news.ycombinator.com/item?id=46761061  So, a couple years ago Microsoft was the first large, public-facing software organization"}
{"anchor": "Attention lapses due to sleep deprivation due to flushing fluid from brain. Long live healthy sleep for brain health, and thank goodness light exercise helps this same glymphatic system. I slept around 5 hours last night split up into two periods because my baby daughter woke up crying from fever and wanted to play / was hallucinating / etc. She's totally fine now but I am wondering if there is a correlation between dementia and having kids. I wonder if a 30-min nap improves the situation. But I need to tell the brain to hold the flushing until the nap. Good to know that the brain finds a way to flush itself while awake. I think I've become pretty good at putting unused parts of my brain to sleep while awake. My brain is like that of a dolphin now. But on rare occasions (like a couple of times a year), I get migraine auras and stuff disappears from my field of view. Can last about an hour. I feel like that's my visual cortex falling asleep. Rest in peace to all the college dudes covering the whole syllabus within 24 hours of the exam anecdotally, i never feel better than when i haven't slept. spent 8pm tuesday -- 8pm thursday this week awake nursing cheap energy drinks, and not only could i manage a higher-than-usual level of focus, i was genuinely content. bombed a midterm halfway though, but at least i felt good about it. I wonder if this could help explain why creatine helps mitigate the effects of sleep deprivation. Since creatine aids in water retention.  https://pubmed.ncbi.nlm.nih.gov/16416332/  So biological garbage collection pauses then? skip sleep, and the brain tries to run gc cycles during runtime. Causing attention and performance latency spikes. Evolution wrote the original JVM. [This is one of those article titles that would really benefit from adding one more word.] > For example, what you don't want to do is NOT take amphetamines at testing if you had used them to study; Hard disagree there. If you get any anxiety during the test it's better to tak", "positive": "Take the pedals off the bike. Apparently this guys is unaware that pedalless balance bikes for kids already exist, and are quite popular. The idea is to get kids used to doing the hard part--staying balanced--first, then when they get a 'real' bike, they don't need training wheels, or at least, not so much. This is the standard way that kids learn to ride bikes in Europe.  Apparently the English word for them is \"balance bikes\".  Both my kids could ride one of them when they were 2. Watching your kid learn to ride a bike is one of life's greatest joys If your kid is using a balance bike, be sure to take them somewhere flat like a playground. If the ground is even slightly inclined, the kid cannot coast, and they spend their energy inefficiently pushing themselves and their bike forward. Training wheels aim to maximise the utility of the bicycle (i.e. gears and pnumatic tyres) for a person of certain age, at the cost of learning how to actually ride a bike. I feel there are lots of parallels in e.g. Maths education in the more generalised form: In education, skills that allow you to utilise technology are prioritised and these are often directly opposed to skills needed for mastery. > Bicycles achieve balance through the gyroscopic effect No they don't. If that were true you could cycle along at a 45 degree angle. They balance because you steer into a fall. Now we need a tutorial how to take the pedals off the bike.\nSo you do not damage the crank because of the opposite threading left vs right. The gyroscopic effect contributes little to maintaining a balanced bike ride, contrary to the article claim. An idealized massless wheel/tire wouldn't diminish ridability. Steering dynamics (steering to counteract bike lean) and trail effect (bike are built to automatically counteract lean), along with rider input (steering, leaning body), are more important components. Sheldon Brown wrote about this years ago:  https://www.sheldonbrown.com/teachride.html  > The ideal bike for", "negative": "US administration to require app, social media, possibly DNA for travelers. Apart from the alarming privacy implications of these proposed rules, I wonder how FIFA might feel about this, ahead of the World Cup. Maybe they could award Trump a privacy prize if his administration backs down from this. I would love to visit the US one day and i do understand that it has no obligation to just let me in, but this seems a bit excessive for a short visit especially seeing as my country has a deal with the US not to require visas. I wonder if i would have to disclose my hn account(s). My cover would be blown! I guess i'm lucky i've made pro Trump comments... I wonder what position the U.S. Chamber of Commerce (which Wikipedia describes as the largest lobbying group in the US) will take on this. I'd like to think they are rational and recognize that 99.99%+ of visitors are bringing tourism money to the United States. I was sure this was going to say that they were going to force travelers to get Truth Social accounts. Somehow I was surprised beyond my wildest guess what they would be asking people to do. I want people to visit the U.S., but if they require that they submit all of this data, I expect that they all protest by not visiting or even coming here for work. There are just so many terrible ideas that come from this administration that I think that they should try to harness the power all of those bad ideas in a infinite idiocy power plant to power the world for all generations to come. Not that I know the details, but wasn't it easier to join the Mafia? Just the names and addresses of your folks in the old country (as \"collateral\") The analogy is kind of striking, when you think about it. Ah, more laws and regulations that cannot be followed by most people. I couldn\u2019t tell you every single \u201csocial media\u201d account I\u2019ve made over the years as various startups failed after I tried them. I definitely couldn\u2019t get all my family\u2019s information, even if constrained to just imm"}
{"anchor": "Don't fall into the anti-AI hype. > As a programmer, I want to write more open source than ever, now. I want to write less, just knowing that LLM models are going to be trained on my code is making me feel more strongly than ever that my open source contributions will simply be stolen. Am I wrong to feel this? Is anyone else concerned about this? We've already seen some pretty strong evidence of this with Tailwind. \"Die a hero or live long enough to see yourself become the villain\" AI is both a near-perfect propaganda machine and, in the programming front, a self-fulfilling prophecy: yes, AI will be better at coding than human. Mostly because humans are made worse by using AI. The \u201canti-AU hype\u201d phrase oversimplifies what\u2019s playing out at the moment. On the tech side, while things are a bit rough around the edges still the tech is very useful and isn\u2019t going away. I honestly don\u2019t see much disagreement there. The concern mostly comes from the business side\u2026 that for all the usefulness on the tech there is no clearly viable path that financially supports everything that\u2019s going on. It\u2019s a nice set of useful features but without products with sufficient revenue flowing in to pay for it all. That paints a picture of the tech sticking around but a general implosion of the startups and business models betting on making all this work. The later isn\u2019t really \u201canti-AI hype\u201d but more folks just calling out the reality that there\u2019s not a lot of evidence and data to support the amount of money invested and committed. And if you\u2019ve been around the tech and business scene a while you\u2019ve seen that movie before and know what comes next. In 5 years time I expect to be using AI more than I do now. I also expect most of the AI companies and startups won\u2019t exist anymore. What I don't understand about this whole \"get on board the AI train or get left behind\" narrative, what advantage does an early adopter have for AI tools? The way I see it, I can just start using AI once they get good", "positive": "Is OpenAI Dead Yet?. Tracking the demise of OpenAI through the news cycle No, they are not dead.  However, they face incredible competition in a brutally commoditized product space. As a retail investor mostly invested into broad ETFs (All World), is there any way I can get short exposure to OpenAI? Being short Oracle/Nvidia/Microsoft? Relevant, I would definitely be sleeping uneasy if I was at \u201cOpen\u201dAI. Some insist that Chinese models are a few generations behind, how many probably depends more on patriotism rather than fact. Those people typically also insist that Chinese models are just distillations and often neglect to see how many of these companies contribute to the theory of designing efficient and capable models.\nIt is somehow thought that they will always trail US models. Well. i would say look at recent history. China worked up the ladder of manufacturing from simple, bad stuff to highly complex things - exactly what westerners then claimed they\u2019d never be able to.\nThen as that was conquered, westerners comforted themselves by insisting that China could copy, but trail-blazing would always still be our thing. Well, Baidu and Alibaba face scaling issues few western companies do and BYD seems to match Tesla or VW just fine. I am unsure why anyone would think US models are destined to remain in the lead forever. At \u201cbest\u201d, I see a fragmented world where each major region (yes also Europe) will eventually have their own models - exactly because no one wants to give any competitive power a chokehold over their society. But beyond that, models will largely be so good that this \u201cgeneration\u201d/universal superiority idea becomes completely obsolete. Is OpenAI profitable yet? Will it be in time to recoop capex. It will be the first application of the 'curse of Open company' rule: any for-profit entity that has the name Open in it is destined to go bankrupt. Keep in mind that the \"news cycle\" isn't of much use in this field. For 2025, almost all \"mainstream\" media was", "negative": "Show HN: We Built the 1. EU-Sovereignty Audit for Websites. Checks hosting, analytics, fonts, cdn, video, chat, social embeds.\nGives you a score from 0-100 and suggests Eu-alternatives. Nice, good idea. I need to move away from Github pages finally ;) Seems to treat finnish kapsi.fi hosting as US? Happy to see mastodon.xyz score 100%. Mastodon is pretty cool and proof that we can make federation work. nice idea, are you planning to open source this project? Any recommendations for good European alternatives to Clooudflare? Is there an EU company that's as trustworthy when it comesq to DDoS protection? thanks for this checker, we also need HN alternative for EU only. As Europeans, I'm sure we can do this. reddit.com gets a perfect \"no US dependencies\" score. I guess they have servers around the world and can serve requests from a local-ish server. Obviously this simple check only concerns the technical aspects of the website and doesn't analyse the business itself but I wonder if all .com domains should be marked down? I put in my site and it gave me a red cross for \"Hosting\", on hover it said \"GitHub Pages\". But my site isn't hosted on GitHub Pages. Expanding \"Details\", the URL that is hosted on GitHub Pages is... a different website? There's merely a hyperlink to it on my website. It also says I'm using \"self-hosted\" fonts - but I don't think I'm doing that at all? I'm just using the browser's fonts. Using non-standard fonts is a bad idea because it causes the content to either be invisible until the font is loaded, or else it initially shows in a fallback font and then the text all jumps when the font is loaded. I have some feedback for OP: my personal website got 92% because there is a link to my X profile in the contact session. It's not like it relies on the service. Its just a contact and there are also links to other services such as self hosted matrix. On the other hand my registrar is Namecheap which is in the US and your tool didn't checked for that. I thi"}
{"anchor": "The '3.5% rule': How a small minority can change the world (2019). This rule didn't hold in Israel in the last 3 years. Well over 3.5% went to the streets and the government remains in tact. This is plausible. Non violent groups will often have wider public support (because most people would prefer not to support violence) and if those in power use violence against the non-violent it increases public sympathy for them. Iran proved it wrong (the regime mobilized roughly 1% of the country's population to crack down on protesters) with regards to Single Party Regimes, and knowing people at the Ash Center, they are pessimistic about this as well. If you have 2+ groups with opposing views, each 3.5%+ it's pretty clear that at least one of the 3.5%+ groups will fail. Others here note it's really \"3.5% if there's no one seriously opposing their objectives\" but in my opinion that's a meaningless rule. Of course in those cases non-conflict resolves the issue.  https://medium.com/incerto/the-most-intolerant-wins-the-dict...  (2019) Chenoweth has backed off her previous conclusions in recent years, observing that nonviolent protest strategies have dramatically declined in effectiveness as governments have adjusted their tactics of repression and messaging. See eg  https://www.harvardmagazine.com/2025/07/erica-chenoweth-demo...  One current example of messaging can be seen in the reflexive dismissal  by the current US government and its propagandists of any popular opposition as 'paid protesters'. Large attendance at Democratic political rallies during the 2024 election was dismissed as being paid for by the campaign, any crowd protesting government policy is described as either a rioting or alleged to be financed by George Soros or some other boogeyman of the right. This has been going on for years; the right simply refuses to countenance the possibility of legitimate organic opposition, while also being chronically unable to provide any evidence for their claims. Hong Kong pr", "positive": "Trump says Venezuela\u2019s Maduro captured after strikes. Prediction: this headline will be renamed \"US invades Venezuela\" very soon. Footage is quickly spreading, looks like strikes on military bases as well as a bunch of low-flying helicopters, so a strike + a ground invasion? They didn't even  try very hard to manufacture consent for a war against Venezuela. Wonderful. It will be pretty amusing to watch all those westerners who, not so long ago, were talking about \"rules based order\" pretend nothing is happening or to justify it. Always remember the role of the Nobel Peace Prize committee in preparing this unprovoked and illegal (under international law) attack on Venezuela by awarding the prize to Mar\u00eda Corina Machado. Julian Assange actually filed a Swedish criminal complaint against Nobel Foundation officials, alleging misappropriation of Nobel endowment funds and facilitating war crimes and crimes against humanity in connection with the 2025 Nobel Peace Prize awarded to Mar\u00eda Corina Machado, and it seeks immediate freezing of funds and a full investigation:  https://just-international.org/articles/assanges-criminal-co...  FIFA looking awful silly right now.  https://vxtwitter.com/FaytuksNetwork/status/2007338956241985...  Not Venezuelan helicopters... They're American aircraft. It sure seems like after repeatedly threatening to invade Venezuela, Trump is now invading Venezuela. For what though? Don't know why, this link gives me: Access Denied Our apologies, the content you requested cannot be accessed. I think something like The Hague is the moderate position with this administration. It is definitely not Russia unprovokenly and illegibly attacking its neighbor, so why even care? How does this differ from Russia invading Ukraine? We have to wake up to the world where USA no longer cares about ideals like liberal democracy or allies, but is a warmongering corporatist autocracy. after Iran and now venezuela Iran, I totally understand that if they want to acquire n", "negative": "A list of fun destinations for telnet. uff I hope i can list my MUD game (still in dev, though) Oh man RIP towel.blinkenlights.nl 23 The Star Wars ASCII animation was how I learned telnet existed. Felt like discovering a secret passage in the internet. There's something pure about text-based interfaces. No loading spinners, no JavaScript frameworks, no cookie banners. Just text. nethack.alt.org is conspicuously absent... This is insane > doom.w-graj.net 666 > Play Doom in the terminal (code and details) Very cool, some nice nostalgia looking through that list! Missed a trick not being able to \u201ctelnet telnet.org\u201d though. :-) Related to the last Telnet CVE?\nWhy talking about telnet now otherwise? for years I had this in my .muttrc. it's been commented out since it stopped working... #set signature=\"cat ~/.signature && telnet towel.blinkenlights.nl 666 | tail -n3|\" I was wondering why the Starwars one is not at the top of the list. Then I saw it no longer exists :-( Wasted opportunity for a telnet.net or tel.net domain. I can forsee a future when all the AI slop, popups, fake news, propaganda and ads have fully consumed the web. Maybe then we just go back to an oldschool text based way of communicating. No google. No socials. Just text.        ~/work/...> telnet towel.blinkenlights.nl\n    zsh: command not found: telnet   Note that this is much more dangerous than visiting a website. ANSI escape sequences can seriously mess with your system, RCE included. For those of you curious about what the Star Wars one looked like, the tradition lives on here: ssh -p 1977 sw.taigrr.com My first introduction to the internet was through the telnet-based EW-too talkers like Foothills (Boston U) and Forest (UTS). I have very fond memories of staying up late talking to people from all over the globe. It was truly amazing to me. The best part was how the users moderated behaviour - bad actors were ejected swiftly but rarely permanently. The first BBS I used  in the 80's  eventually ende"}
{"anchor": "Show HN: Exploring Mathematics with Python. Looks like a treasure trove of knowledge. I just recently went to the exploratorium in SF and saw an exhibit there suggesting that the catenary made a good arch, so browsed that chapter and saw a bit of explanation here which helped.  Was also interested to see that Jefferson played some part in the history here. Very Nice! However, I don't see the entire book as a single pdf? I own the original Exploring Mathematics with Your Computer(Turbo Pascal version).\nIt\u2019s an excellent introduction to algorithms for people coming from a mathematics background.\nReally happy to see it revived in Python. Very nice.  I was looking for something fun to work on over the break.  Thank you for this. > Unfortunately, after lengthy discussions with the MAA, my hopes of publishing this (rather large) expansion have proved impossible, and so I've decided to put it online, hopefully to be of use to others. Too bad It is painful to imagine how these fantastic works will be not be read by humans in future, as AI would digest all this and provide just-in-time code for humans. Joint author here. I plan to upload the entire book as a single PDF when I finish the next chapter (on the cycloid). That will probably be early next week. I used the original book by Arthur Engel for many years. He was an inspirational teacher. The MAA tried very hard to publish the book, but I kept adding new material, and  a text consisting of math 'selections' rather than a single theme is a hard sell in today's publishing environment. Random thoughts: - Seems great. Added to the backlog :) - No colors in PDF illustrations. Is it a deliberate choice? - > The first six chapters (and Appendix A) are essentially that book, but with the programming language changed to Python, some rewording, reformatting in Latex, and a few additions.     Try [typst](https://typst.app/) as an alternative to Latex.   This is an excellent resource for building mathematical intuition through code", "positive": "Open Infrastructure Map. I find this site so fascinating, seeing how all the massive power lines are hooked up to far-away power plants and gradually have their voltage stepped down as they connect to consumers. All the undersea cables and pipelines I didn't know about. This is a bad idea in terms of security in war Some previous discussion: 2024  https://news.ycombinator.com/item?id=39109185  2022  https://news.ycombinator.com/item?id=29948473  Gigachad french nuclear versus virgin german coal in map form. When I lived in Texas, we had a massive storm in winter of 2021 leaving many without power for a week. I was told that Texas maintained its own energy grid independent from the rest of the nation\u2019s eastern and western grids, and supposedly only had a handful of high-voltage DC lines running between Texas\u2019s and the rest of the nation\u2019s. Supposedly this was why we couldn\u2019t rely on excess capacity from anywhere else in the nation while our power generation capability was down. But this map doesn\u2019t seem to show Texas as isolated - there appear to be many lines in and out and no clear separation? An initially-stupid-sounding idea I heard a while back was running power cables through the ocean floors between America and the rest of the world. It's apparently feasible and the big benefit of it is that at the grid peak hour when the sun is not shining in Europe, they can get cheap solar from America and vice versa The map for Australia is interesting.  Is this missing data?  See no infrastructure for Alice Springs in the interior of Australia. Excellent link, thank you for posting! Wanted to do a map of the power network here in Romania, hadn't thought to check if anything similar already existed, or I couldn't find it myself, at least, but it seems like this map has (almost) all that I wanted to do in that respect, including the position of the power poles on the ground. For the Netherlands (and surrounding countries), there is Hoogspanningsnet (the high-voltage grid), ", "negative": "The truth behind the 2026 J.P. Morgan Healthcare Conference. The website linked in the article appears to not be _the_ website (to be fair, tfa only calls it _a_ website). The website actually hosted by JPM is very sparse, but even mentions that such unofficial websites exist.  https://www.jpmorgan.com/about-us/events-conferences/health-...  (tfa is a fun read, regardless) The conference sessions aren't what matters. The important thing about these kinds of industry conferences is the ability for investors, leaders, regulators, journalists, and others to meet with each other in a neutral zone. Multiple M&As are being negotiated, IPOs being considered, funds trying to raise a new vintage, and companies starting press junkets in preparation for a roadshow. > it is possible that the entirety of California is built on top of one immensely large organism, and the particular spot in which the Westin St. Francis Hotel stands\u2014335 Powell Street, San Francisco, 94102\u2014is located directly above its beating heart. And that this is the primary organizing focal point for both the location and entire reason for the J.P. Morgan Healthcare Conference Moscone Center tends to be the primary hub for industry conferences in the City (eg. RSA, Dreamforce, Oracle OpenWorld way back in the day), and more niche executive events are in the Four Seasons or St Regis. My hunch is that JPM has a multi-year deal with the Westin to host the conference at the Westin. from my understanding, it's a healthcare investors conference where investors meet companies (both public and private), esp those looking to fund raise. Not surprising. Take any conference and look at the schedule of some CEO or other \u201csocialite\u201d attending said conference. They\u2019re not in the building, they\u2019re running around town attending meetings. At JPMHC everyone is a \u201csocialite\u201d Reminds me of Kurt Vonnegut - what a wonderful and absurd article I've been inside the conference- I used to do due diligence and discovery for Google Ventu"}
{"anchor": "Let's be honest, Generative AI isn't going all that well. This post is literally just 4 screenshots of articles, not even its own commentary or discussion. LLMs help me read code 10x faster - I\u2019ll take the win and say thanks You're absolutely right! The irony of a five sentence article making giant claims isn't lost on me. Don't get me wrong: I'm amenable to the  idea ; but, y'know, my kids wrote longer essays in 4th grade. Guessing this isn\u2019t going to be popular here, but he\u2019s right. AI has some use cases, but isn\u2019t the world-changing paradigm shift it\u2019s marketed as. It\u2019s becoming clear the tech is ultimately just a tool, not a precursor to AGI. Meanwhile, my cofounder is rewriting code we spent millions of salary on in the past by himself in a few weeks. I myself am saving a small fortune on design and photography and getting better results while doing it. If this is not all that well I can\u2019t wait until we get to mediocre! Should have used an LLM to proofread.. LLMs can still cannot be trusted? I find it a bit odd that people are acting like this stuff is an abject failure because it's not perfect yet. Generative AI, as we know it, has only existed ~5-6 years, and it has improved substantially, and is likely to keep improving. Yes, people have probably been deploying it in spots where it's not quite ready but it's myopic to act like it's \"not going all that well\" when it's pretty clear that it actually  is  going pretty well, just that we need to work out the kinks. New technology is always buggy for awhile, and eventually it becomes boring. It's going well for coding. I just knocked out a mapping project that would have been a week+ of work (with docs and stackoverflow opened in the background) in a few hours. And yes, I do understand the code and what is happening and did have to make a couple of adjustments manually. I don't know that reducing coding work justifies the current valuations, but I wouldn't say it's \"not going all that well\". This feels like a pret", "positive": "Gemini 3. Feeling great to see something confidential - Anyone have any idea why it says 'confidential'? - Anyone actually able to use it?  I get 'You've reached your rate limit. Please try again later'.  (That said, I don't have a paid plan, but I've always had pretty much unlimited access to 2.5 pro) [Edit:  working for me now in ai studio] How long does it typically take after this to become available on  https://gemini.google.com/app  ? I would like to try the model, wondering if it's worth setting up billing or waiting. At the moment trying to use it in AI Studio (on the Free tier) just gives me \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" It's available to be selected, but the quota does not seem to have been enabled just yet. \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" \"You've reached your rate limit. Please try again later.\" Update: as of 3:33 PM UTC, Tuesday, November 18, 2025, it seems to be enabled. It seem that Google doesn't prepare well to release Gemini 3 but leak many contents, include the model card early today and gemini 3 on aistudio.google.com it is live in the api > gemini-3-pro-preview-ais-applets > gemini-3-pro-preview API pricing is up to $2/M for input and $12/M for output For comparison:\nGemini 2.5 Pro was $1.25/M for input and $10/M for output\nGemini 1.5 Pro was $1.25/M for input and $5/M for output It generated a quite cool pelican on a bike:  https://imgur.com/a/yzXpEEh  And of course they hiked the API prices Standard Context(\u2264 200K tokens) Input $2.00 vs $1.25  (Gemini 3 pro input is 60% more expensive vs 2.5) Output $12.00 vs $10.00 (Gemini 3 pro output is 20% more expensive vs 2.5) Long Context(> 200K tokens) Input $4.00 vs $2.50   (same +60%) Output $18.00 vs $15.00  (same +20%) When will this be available in the cli? Not the preview crap again.\nHaven't the", "negative": "My Ridiculously Robust Photo Management System (Immich Edition). After going through 25 years of changing software every few years on this front I can\u2019t be bothered. Files on disk. Nothing over the top. Immich is just another thing to maintain. Another problem which will result in a wholesale migration down the line. If someone wants something I email it to them or upload it to a directory on a web server and send them the link. If I want something on my phone I\u2019ll zap it over with localsend. Photography is a hobby for me and I have a large family so I have a lot of photos. And a lot of editing to do. Currently moving from Lightroom to Darktable because again Lightroom tries to hammer me with library management and lock me into things. Elodie makes a copy of all my images initially? Is the recommenddd route then to delete the files in original location? Seems unclear at first read. Habe you tried nextcloud + memories app?\nEvery metadata is stored in EXIF and the directory structure on disk defines the directory structure in the app (and vice versa). \nWhen you want to move your tooling or just do things manual again, grab the disk and your are ready. I migrated from Apple Photos to Immich a couple of months ago, removing the iCloud subscription, and couldn\u2019t be happier. It was the most hassle free piece of self-hosted software I\u2019ve had so far. Very easy to install and everything just works. Context and OCR search are amazing. Mobile apps could be better, but they are constantly being improved. My favorite feature is being able to setup a container on my Linux desktop that has a GPU access and can run ML workloads for image processing whenever I turn the computer on, as my NAS (where Immich resides) is a low power machine without a dedicated GPU. They even have ROCM support, so it works even without an Nvidia GPU. Being able to spread such workloads over your local network feels like a magic that has been forgotten in an era of blackbox cloud providers. Photo printer "}
{"anchor": "2025: The Year in LLMs. These are excellent every year, thank you for all the wonderful work you do. Remember, back in the day, when a year of progress was like, oh, they voted to add some syntactic sugar to Java... > Vendor-independent options include GitHub Copilot CLI, Amp, OpenHands CLI, and Pi ...and the best of them all, OpenCode[1] :) [1]:  https://opencode.ai  > The (only?) year of MCP I like to believe, but MCP is quickly turning into an enterprise thing so I think it will stick around for good. Great summary of the year in LLMs. Is there a predictions (for 2026) blogpost as well? > The year of YOLO and the Normalization of Deviance # On this including AI agents deleting home folders, I was able to run agents in Firejail by isolating vscode (Most of my agents are vscode based ones, like Kilo Code). I wrote a little guide on how I did it  https://softwareengineeringstandard.com/2025/12/15/ai-agents...  Took a bit of tweaking, vscode crashing a bunch of times with not being able to read its config files, but I got there in the end. Now it can only write to my projects folder. All of my projects are backed up in git. What an amazing progress in just short time. The future is bright! Happy New Year y'all! Not in this review: Also the record year in intelligent systems aiding in and prompting human users into fatal self-harm. Will 2026 fare better? I'm curious how all of the progress will be seen if it does indeed result in mass unemployment (but not eradication) of professional software engineers.  You\u2019re absolutely right! You astutely observed that 2025 was a year with many LLMs and this was a selection of waypoints, summarized in a helpful timeline.  That\u2019s what most non-tech-person\u2019s year in LLMs looked like. Hopefully 2026 will be the year where companies realize that implementing intrusive chatbots can\u2019t make better ::waving hands:: ya know\u2026  UX  or whatever. For some reason, they think its helpful to distractingly pop up chat windows on their site because", "positive": "Live Map of the London Underground. Very cool. Especially as it a real map not a 'network diagram' who is so confusing. > Data -> TfL live tube data > *You will regret using this data. You will regret using this API.* > It serves data from individual arrivals boards, which all spell stations differently > It has a load-balancer that regularly returns data that is older than the data returned in the previous request. Won't someone think of the Ai overlords who will take care of all this for us in the future. A bit of consistency goes a long way. Looks great but I'm watching this while sitting on a tube right now. What I assumed was my train was lagging by quite a bit and then disappeared! It's cool to see how fast the trains go on different lines. But... where's the Elizabeth line? You get the tooltip when you hover over it, but the polyline is missing. One of the best game I ever played is the text based souvenir game shopping game on Windows 3. I can't recall the name of the game now since it's more than 30 years ago, but it's about shopping souvenirs using London Underground Tube. You have a semi realistic time constraints like train schedules, your flight schedules and of course list of souvenirs items to shop. This is totally offline since there is no Internet available at the time but it's very engaging nonetheless. My proposal for the modern version of the game is to use real-time train schedules (with delays, ticket discounts, etc) that are available publicly on the Internet for many metropolitan cities in the world for examples Tokyo, London and Berlin. Imagine you can have a real-world realistic in-app in-game items purchases feature that you personally can buy in the game and delivered to you or anyone you fancy of giving souvenirs except that you only virtually went there. Maybe a slight bug: the overlay doesnt appear to be locked to the map - when I scroll around, the overlay moves. Currently the northern lines' southern terminal is hovering over Kingsto", "negative": "Spanish track was fractured before high-speed train disaster, report finds. Wow, that's a really big gap. No wonder it derailed What are the some of the ways that tracks are monitored for fractures like this?  It must have been pretty substantial in order to be described as \"complete lack of continuity\".  Makes me think of literally electronic continuity tests -- are those ever used in this context?  Or how about cameras mounted on trains using image processing?  Or drones? It seems a shame that a few other trains passed beforehand with this anomaly in place and yet it went undetected. My gut feeling says a lot of fatalities could have been prevented with a physical barrier between both tracks. Shouldn't this be mandatory with high speed trains? While these events are statistically very rare, it is worth remembering that there have been two separate events in the past twenty years in Spain where high-speed trains have derailed leading to multiple fatalities [1][2]. In contrast, the Japanese Shinkansen has a spotless record since its introduction in the 1960s [3]. Not a single fatality due to a crash or derailment. And that's in a country with a much larger population and much higher passenger count per year. What do they do differently? [1]  https://en.wikipedia.org/wiki/Santiago_de_Compostela_derailm...  [2]  https://en.wikipedia.org/wiki/2026_Adamuz_train_derailments  [3]  https://en.wikipedia.org/wiki/Shinkansen#Safety_record  I wonder how common it is for train tracks to fracture? And what systems are in place to actually detect this. There was recently a post on a German subreddit where the OP found a fracture in the German rail[0], albeit much smaller. 0.  https://old.reddit.com/r/drehscheibe/comments/1qe9ko2/ich_gl...  AFAIK continuously welded tracks (like those used in high speed rail) are also slightly tensioned, so a break in a single point could make it look like a whole section of track is missing, as tension is released. Some more info from Spanish med"}
{"anchor": "Copenhagenize Index 2025: The Global Ranking of Bicycle-Friendly Cities. great website, very helpful for traveling cyclists Of course it helps if the city, and country in general, is completely flat. Cities in Norway or Nepal have mother nature against all form of manual locomotion. I'm not surprised to see Utrecht in the first place, but quite a bit surprised to see the other Dutch cities so low. No offense, but Rotterdam or The Heague is 100x better than Paris from safety and convenience point of view. I'm curious why is the ranking like this. \"Welcome to the famous five-minute WordPress installation process! Just fill in the information below and you\u2019ll be on your way to using the most extendable and powerful personal publishing platform in the world.\" Ooops. Bicycling is part of the mobility culture of Montreal, but whether Montreal is actually  friendly  to bicyclists is open to heated debate. Cars dominate the topology. Why the Copenhagenize Index when Copenhagen is not particularly bike friendly by Dutch standards? I went to Copenhagen this summer. I was quite disappointed in the bicycle infrastructure, I felt like it was on par with what we have in Stockholm. Rented bikes and biked around for two days. It was nice! Not sure how this index is being calculated (site breaks a lot), but my general feeling was that Denmark is just better at marketing than actual infrastructure when comparing to Stockholm at least Ranking Bordeaux and Nantes next to Amsterdam is nonsense. Amsterdam is miles ahead in terms of infrastructure. This ranking dilutes the most important thing to get these results : good bike lanc\u00e9s everywhere with no discontinuity. Disclaimer : I've built villes.plus, an open source automated evaluation of bike lanes. 100 points, compute itineraries in \"secure\" mode with Brouter between these points, count the % of secured km -> score. Amsterdam tops at 8/10. Bordeaux is at 3/10, Nantes 2/10.  https://villes.plus/cyclables/Amsterdam?id=271110   https://v", "positive": "AlphaFold 3 predicts the structure and interactions of life's molecules. The article was heavy on the free research aspect, but light on the commercial application. I'm curious about the business strategy. Does Google intend to license out tools, partner, or consult for  commercial partners? s/predicts/attempts to predict From:  https://www.nature.com/articles/d41586-024-01383-z  >Unlike RoseTTAFold and AlphaFold2, scientists will not be able to run their own version of AlphaFold3, nor will the code underlying AlphaFold3 or other information obtained after training the model be made public. Instead, researchers will have access to an \u2018AlphaFold3 server\u2019, on which they can input their protein sequence of choice, alongside a selection of accessory molecules. [. . .] Scientists are currently restricted to 10 predictions per day, and it is not possible to obtain structures of proteins bound to possible drugs. This is unfortunate. I wonder how long until David Baker's lab upgrades RoseTTAFold to catch up. > What is different about the new AlphaFold3 model compared to AlphaFold2? > AlphaFold3 can predict many biomolecules in addition to proteins. AlphaFold2 predicts structures of proteins and protein-protein complexes. AlphaFold3 can generate predictions containing proteins, DNA, RNA, ions,ligands, and chemical modifications. The new model also improves the protein complex modelling accuracy. Please refer to our paper for more information on performance improvements. AlphaFold 2 generally produces looping \u201cribbon-like\u201d predictions for disordered regions. AlphaFold3 also does this, but will occasionally output segments with secondary structure within disordered regions instead, mostly spurious alpha helices with very low confidence (pLDDT) and inconsistent position across predictions. So the criticism towards AlphaFold 2 will likely still apply? For example, it\u2019s more accurate for predicting structures similar to existing ones, and fails at novel patterns? This is a basic ", "negative": "iPhone 5s Gets New Software Update 13 Years After Launch. People complain a lot about planned obsolescence but i'm mildly impressed, even if this update is only to keep the lights on and nothing else. I remember people complaining that the design of the 5 was already outdated when it was new and they needed to have bigger screens and be thinner to compete with Samsung... TLDR it replaces an expired certificate, no software is being \"updated\" here. Wake me when old versions of OS X can access the App Store again. I'm not a fan of Apple's walled garden mindset and resistance to inter-operating with other platforms, but this degree of legacy support is a case of Apple doing a good thing and deserves praise.  Note: I'm not saying that Google/MSFT et al are  much  better than Apple, but they're not  quite  as bad. tokyobreakfast is right that this is just a certificate fix, not a real software update. But it's still notable. Lots of old devices become paperweights because of expired certs or backend shutdowns. The fact that Apple even bothered to push this to a 13-year-old device is unusual. Most companies wouldn't. I ran a 5S that I bought in December 2013 as my primary phone all the way up to around March 2020, just as the pandemic was really winding up. The battery, after ailing for a little while, had eventually just given up. I'd gone skiing a couple of times, with the last trip being just before lockdown, and I think it was the cold exposure of the second trip that dealt the mortal blow, and it died shortly after I returned. I liked that phone a lot. It did, at the time, everything I needed, and it was a really nice size, but that period in 2020 was a bad time to try to get a phone repaired. I did attempt to replace the battery myself using the guide on iFixit but, sadly, that did not go well due to some contradictory/out of order instructions, and all I succeeded in doing was damaging the phone, I think, beyond repair. Really good to see that Apple are still suppo"}
{"anchor": "Ratatui \u2013 App Showcase. the title of this post is odd? it\u2019s a showcase of TUI applications built with this Rust crate \u2014 which I am hearing about for the first time, and am interested in. I was expecting a blog post on why Rust is experiencing a TUI revolution or something I've seen lots of TUIs lately, why is that? What is the renewed interest? The only places I know of is Awesome TUIs [0] and terminaltrove [1] I can also see that Ratatui has an awesome list too [2]. [0]  https://github.com/rothgar/awesome-tuis  [1]  https://terminaltrove.com/  [2]  https://github.com/ratatui-org/awesome-ratatui  Some of the most interesting projects here have the worst installation stories.It's sort of tilting at windmills to not acknowledge that people are going to mostly install through package managers for their platform by advertising it as such. I'm not suggesting there's anything wrong with building from source. On the contrary, I think it's fantastic as many targets are supported here as there are! I think it's a shame more people aren't discovering them is all. Very dope. I really like dua as my mac only has 256 GB. What is the best / most popular / user friendly terminal http client I can replace postman with. Has a history I can search, save favorites, secure etc. Ratatui is neat but the way it's architected, you need to take on third party dependencies for each individual widget. And we're talking basic things like spinners, checkboxes, text areas, etc. -- there aren't too many widgets built into ratatui itself. I didn't like the idea of taking all that on so instead I went with something more handrolled. Ratatui is awesome! Just built a little chat client with it, tons of fun.  https://terma.mattmay.dev/  I'm really waiting for the TUI web browser. That would let me live completely in the terminal. Is anyone working on this? With the speed terminals are and support for graphics through things like sixel and shaders I'd love to have a browser even if I couldn't do videos", "positive": "Siddhartha. Huh, funny this should pop up here. I recently started commuting by subway into work, so I had to pick up a subway book. I had been meaning to read this, so I went to my local book store and grabbed a copy. It\u2019s a really great book. Such a fascinating story. And short, too. I highly recommend giving it a read. It might synthesize some of your loose connections about Hinduism, Buddhism, and your own place in a chaotic world and what it means to live a happy life. great book.  I recommend people read it every 10 years or so as your perspective on life changes. I'm very grateful that this was assigned reading in high school, since it was a sort of gateway book for reading more about Buddhism. It's short. If you haven't read it, I highly recommend it. One of the greatest authors of all time. Hesse taps into the mind of the modern human and beautifully presents its inner workings. Each of his books takes a different angle, a different perspective or philosophy with which to observe the evolving personhood. I read this in high school, but not because it was assigned.  At the time I was really into \"rare\" Queen MP3s, and there's a studio recording of the fast version of \"We Will Rock You\" where Brian May reads a passage from this book before the music starts.  An odd way to be inspired to read a book, but I still think I got a fair bit out of it. Off topic: Im curious what\u2019s the most prominent religion among HNers? Is it different from the normal population? Buddhism seems to be number 1 after atheism which isn\u2019t a religion. I liked this quite a bit the first time I'd read it. A decade later, not as much. Narcissus and Goldmund is my favorite book by Hesse - it's beautifully crafted. I read this annually, typically in a day, usually when I'm feeling lost. For me it distills the human experience into a simply story that helps me find meaning for where I am in my own journey. Love this book. I have three sons and read this when them when they're about 12 or 13. I", "negative": "Heathrow scraps liquid container limit. Not because of a sudden outbreak of sanity, but because they have CT scanners now. FINALLY (PS.  Still not going to fly there) Good. This should happen on all airports now. Otherwise it's useless. You won't be flying from Heathrow to Heathrow. The security theater needs to go on. In the meantime batteries represent a much bigger risk with potential in flight fires but I guess nobody cares enough to do anything about it. Let me get this straight. If the article is correct, the new capabilities are related to better detection of large liquid containers, not determination of whether or not the liquid is dangerous. So - you couldn\u2019t take large amounts of liquids previously because some liquids in large amounts might be able to be weaponized. If you were caught with too much liquid (in sum total, or in containers that are too large) they\u2019d throw it out and send you on your way. But now that they have the ability to detect larger containers, they\u2026 do what? Declare that it\u2019s safe and send you on your way with it still in your possession? This rule wasn't enforced anyway... I travel a lot - and never take out any liquids. Have nail clippers and scissors in my carry-on. Once I even had an opinel pocket knife in my laptop bag for a couple of months. Travelled through Tokyo, Taipei, SFO, DEN, PHX, LAX, BOS, JFK, FRA, AMS, MUC, LHR - nobody noticed. I seriously had forgotten it was there, so I don't do that now, but still... Also, no large water bottles or similar. Unless on domestic flights in Japan, where this is totally fine. IDK - security theater. But if it helps. Famously Steve Jobs had a story about shaving time off of boot-up and equating it to saving lives on the concept of people sitting their waiting for the computer to boot up just lost that much of their lives. [1] I actually do believe there is value in thinking this way and it is one of my biggest arguments against TSA. Everything has a cost, including 'security' and 'safet"}
{"anchor": "A few random notes from Claude coding quite a bit last few weeks. The section on IDEs/agent swarms/fallibility resonated a lot for me; I haven't gone quite as far as Karpathy in terms of power usage of Claude Code, but some of the shifts in mistakes (and reality vs. hype) analysis he shared seems spot on in my (caveat: more limited) experience. > \"IDEs/agent swarms/fallability. Both the \"no need for IDE anymore\" hype and the \"agent swarm\" hype is imo too much for right now. The models definitely still make mistakes and if you have any code you actually care about I would watch them like a hawk, in a nice large IDE on the side. The mistakes have changed a lot - they are not simple syntax errors anymore, they are subtle conceptual errors that a slightly sloppy, hasty junior dev might do. The most common category is that the models make wrong assumptions on your behalf and just run along with them without checking. They also don't manage their confusion, they don't seek clarifications, they don't surface inconsistencies, they don't present tradeoffs, they don't push back when they should, and they are still a little too sycophantic. Things get better in plan mode, but there is some need for a lightweight inline plan mode. They also really like to overcomplicate code and APIs, they bloat abstractions, they don't clean up dead code after themselves, etc. They will implement an inefficient, bloated, brittle construction over 1000 lines of code and it's up to you to be like \"umm couldn't you just do this instead?\" and they will be like \"of course!\" and immediately cut it down to 100 lines. They still sometimes change/remove comments and code they don't like or don't sufficiently understand as side effects, even if it is orthogonal to the task at hand. All of this happens despite a few simple attempts to fix it via instructions in CLAUDE . md. Despite all these issues, it is still a net huge improvement and it's very difficult to imagine going back to manual coding. TLDR ev", "positive": "Training my smartwatch to track intelligence. I've tracked sleep using a number of devices and algorithms and I haven't found a single one that regularly aligns with what and how I feel. I know it's tracking real data, but the conclusions feel completely made up. What are other people's experience -- especially from those who are more bullish about sleep tracking? I hope Garmin sees your passion project and greenlights it for inclusion. You have the right approach to ensuring folks are at their optimal health to grow intellectually as a person. > Often, it would also contradict how I was internally feeling. I\u2019d wake up feeling rested, see my stats are low, and play a game of chess out of algorithmic rebellion, only to feel my mind up against a barrier and handedly lose. It would be better to only look at the stats after playing if you want to verify it, this could easily be a self-fulfilling prophecy. The biggest thing for me is I don't understand how people can sleep with these watches on, it's so uncomfortable to me personally which is why the different ring technologies appeal to me more. I just wish either Garmin made one or that there was one I didn't have to buy a subscription to use. I didn't believe the stress numbers on my Garmin watch were very meaningful until I started taking Nebivolol (an atypical beta blocker) because there were so many gaps (even when I was sitting) that I didn't feel I could eyeball them or trust averages over time. Taking that drug,  however,  it sees far fewer gaps and I show up in the blue \"rest\" zone most of the time. I've been watching my heart rate a lot in the last month part because of health concerns and part because of a new stance I am practicing that has a physical component (e.g. adjusted gaits that are energy efficient) and a mental component, being an oceanic reservoir of calm with close mind-body-environment coupling 95% of the time but disconnecting that connection under peak stress -- like I am standing between two ", "negative": "Claude Code daily benchmarks for degradation tracking. FYI the MarginLab Claude Code degradation tracker is showing a statistically significant ~4% drop in SWE-Bench-Pro accuracy over the past month Very interesting. I would be curious to understand how granular these updates are being applied to CC + what might be causing things like this. I feel like I can notice a very small degradation but have compensated with more detailed prompts (which I think, perhaps naively, is offsetting this issue). I really like the idea, but a \"\u00b114.0% significance threshold\" is meaningless here. The larger monthly scale should be the default, or you should get more samples. This is probably entirely down to subtle changes to CC prompts/tools. I've been using CC more or less 8 hrs/day for the past 2 weeks, and if anything it feels like CC is getting better and better at actual tasks.  Edit: Before you downvote, can you explain how the model could degrade WITHOUT changes to the prompts? Is your hypothesis that Opus 4.5, a huge static model, is somehow changing? Master system prompt changing? Safety filters changing?  Would love to see this idea expanded to ever alleged SoTA model currently in production. Any speculation as to why this degradation occurs? Simply search user prompts for curse words and then measure hostility sentiment.  User hostility rises as agents fail to meet expectations. There was a moment about a week ago where Claude went down for about an hour. And right after it came back up it was clear a lot of people had given up and were not using it. It was probably 3x faster than usual. I got more done in the next hour with it than I do in half a day usually. It was definitely a bit of a glimpse into a potential future of \u201cwhat if these things weren\u2019t resource constrained and could just fly\u201d. Wouldn't be surprised if they slowly start quantizing their models over time. Makes it easier to scale and reduce operational cost. Also makes a new release have more impact as it wil"}
{"anchor": "Iran Protest Death Toll Could Top 30k, According to Local Health Officials. Very tragic. May the souls that gave their lives for freedom live in the memory of the people of Iran as a blessing. The simple absence of on the ground reports from a variety of independent sources tells me that these numbers should not be simply ignored. If there\u2019s nothing happening, then the obvious way for the authorities to prove that is to let observers in, and let independent information out. They do not do this, so I will take these reports of deaths more seriously. How many on the government side, I wonder. There are wars that haven't killed so many people. This seems like another revolution. I guess this will be a difficult question to ask.  I have no doubt the numbers are high but there is something odd about the videos that leak out.  The sound of the guns are enhanced  for psychological effect?  and in the cases where a gunner on a truck is moving down a road purportedly mowing people down there is no blood on the road where the protestors had been standing, no bodies and we never see the people being shot.  It's not like I want to see people being shot but I've also seen a lot of fake mass shooting videos in the past decade.  There's no shortage of real uncensored footage of killing in Ukraine.  Why is everything censored for Iran? That's way higher than I thought. Is there any evidence? Dresden was 25,000, and the V2 and V1 campaigns had less numbers. So this is high even for an  aerial bombing  campaign. [edit] I don't get why I'm getting downvoted. Are people making assumptions because I mentioned Dresden? Get a hold of yourself. > As of Saturday, the U.S.-based Human Rights Activists News Agency said it had confirmed 5,459 deaths and is investigating 17,031 more. The 30,000 number comes from the Ministry of Health. It seems the UN number also aligns with the new 30,000 number. This is much worse than the 3,000 that was reported earlier. But it also seems like the crackdown ", "positive": "Ask HN: Iran's 120h internet shutdown, phones back. How to stay resilient?.  https://en.wikipedia.org/wiki/Stoicism  Starlink and/or BGAN/satellite phones. Maybe  https://meshtastic.org ? If the phones are working, 56k modem. HAM radio is your best option. WiFi Halow is a longer range protocol (still probably not long enough). But something like this can get people connected:  https://openmanet.net  Old fashioned phone trees can be really useful IMHO OP. We used them when I worked in a school. If there was winter weather, you'd call say, everyone with a last name from A to G in the staff directory, someone else calls G to K, and so on and so forth. You can combine the phone tree with literal runners -- so basically, someone takes their burner and calls suburbs A,B,C and D and then the runners go out and pass the word about the protest or action. V.92 dial-up. Slow and expensive, but it's Internet access. Problem is that most methods involve making your location known openly. The Dark Forest book of the Remembrance of the Earth Past explains why it is not a good idea to do so in the current circumstances For dense areas, mesh applications like BitChat (Jack Dorsey) could bypass the need for a network with p2p bluetooth mesh networks. And works with existing devices, vs something like meshtastic which needs an installed base (afaik).  https://en.wikipedia.org/wiki/Bitchat  some DNS tunneling solutions work (dnstt for example). Also, many people have smuggled Starlink are are providing proxies inside Iran. Ideally cjdns or similar can be used inside the country to create an alternative encrypted mesh network inside the borders, with some \"exit nodes\" out. HF radio. Highly depdendent on space weather, but generally I can communicate around the world with only 100 watts and a long wire. Be aware though that transmitting on any radio is like turning on a giant, extremely bright light bulb directly above your antenna. Anyone with basic radio know-how will be able to hear y", "negative": "Ask HN: What's the current best local/open speech-to-speech setup?. It was a little annoying getting old qt5 tools installed but I really enjoyed using dsnote / Speech Note. Huge model selection for my amd gpu. Good tool. I haven't done enough specific studying yet to give you suggestions for which model to go with. WhisperFlow is very popular. Kyutai some very interesting work always. Their delayed streams work is bleeding edge & sounds very promising especially for low latency. Not sure why I have not yet tried it tbh.  https://github.com/kyutai-labs/delayed-streams-modeling  There's also a really nice elegant simple app Handy. Only supports Whisper and Parakeet V3 but nice app & those are amazing models.  https://github.com/cjpais/Handy  You should look into the new Nvidia model:  https://research.nvidia.com/labs/adlr/personaplex/  It has dual channel input / output and a very permissible license Anyone using any reasonably good small speech to text os models? For the TTS part:  https://github.com/supertone-inc/supertonic  It requires a bit of tinkering, but I think pipecat is the way to go.  You can plug in pretty much any STT/LLM/TTS you want and go.  It definitely supports local models but its up to you to get your hands on those models. Not sure if there's any turnkey setups that are preconfigured for local install where you can just press play and go though. Last I heard E2E speech to speech models are still pretty weak.  I've had pretty bad results from gpt-realtime and that's a proprietary model, I'm assuming open source is a bit behind.  https://handy.computer  got good marks from a  very  nontechnical user in my life this week! Local, FOSS Tangential: What hardware are you using for the interface on these?  Is there a good array microphone that performs on par with echos/ghomes/homepods? I have used  https://github.com/SaynaAI/sayna  . What I like the most is that you can switch between the providers easily and see what works for you the best. It also su"}
{"anchor": "Total monthly number of StackOverflow questions over time. The decline is not surprising. I am sure AI is replacing Stackoverflow for a lot of people. And my experience with asking questions was pretty bad. I asked a few very specific questions about some deep detail in  Windows and every time I got only some smug comments about my stupid question or the question got rejected outright. That while a ton of beginner questions were approved. Definitely not a very inviting club. I found i got better responses on Reddit. Do I read that correctly \u2014 it is close to zero today?! I used to think SO culture was killing it but it really may have been AI after all. Probably similar for google. My first line of search is always chatgpt Everything we have done and said on the internet since its birth has just been to train the future AI. Now that StackOverflow has been killed (in part) by LLMs, how will we train future models? Will public GitHub repos be enough? Precise troubleshooting data is getting rare, GitHub issues are the last place where it lives nowadays. The result is not surprising! Many people are now turning to LLMs with their questions instead. This explains the decline in the number of questions asked. Wow. I was expecting a decline but not to  that  extent. They will no doubt blame this on AI, somehow (ChatGPT release: late 2022, decline start: mid 2020), instead of the toxicity of the community and the site's goals of being a knowledgebase instead of a QA site despite the design. PS - This comment is closed as a [duplicate] of this comment:  https://news.ycombinator.com/item?id=46482620  Not a big surprise once LLMs came along: stack overflow developed some pretty unpleasant traits over time.  Everything from legitimate questions being closed for no good reason (or being labeled a duplicate even though they often weren\u2019t), out of date answers that never get updated as tech changes, to a generally toxic and condescending culture amongst the top answerers.  For all ", "positive": "Trinity large: An open 400B sparse MoE model. I'm particularly excited to see a \"true base\" model to do research off of ( https://huggingface.co/arcee-ai/Trinity-Large-TrueBase ). They trained it in 33 days for ~20m (that includes apparently not only the infrastructure but also the salaries over a 6 month period). And the model is coming close to QWEN and Deepseek. Pretty impressive What exactly does \"open\" mean in this case?  Is it weights and data or just weights? Given that it's a 400B-parameter model, but it's a sparse MoE model with 13B active parameters per token, would it run well on an NVIDIA DGX Spark with 128 GB of unified RAM, or do you practically need to hold the full model in RAM even with sparse MoE? The only thing I question is the use of Maverick in their comparison charts. That's like comparing a pile of rocks to an LLM. What did they do to make the loss drop so much in phase 3? Also, why are they comparing with Llama 4 Maverick? Wasn\u2019t it a flop? So refreshing to see open source models like this come from the US. I would love for a 100Bish size one that can compete against OSS-120B and GLM air 4.5 Is anyone excited to do ablative testing on it? > We optimize for performance per parameter and release weights under Apache-2.0 How do they plan to monetize? It's super exciting to see another American lab get in the ring. Even if they're not at SOTA on the first release, the fact that they're trying is incredible for open source AI. unsloth quants are up  https://huggingface.co/unsloth/Trinity-Large-Preview-GGUF  According to the article, nearly 50% of the dataset is synthetic (8T out of 17T tokens). I don't know what constitutes \"a breadth of state-of-the-art rephrasing approaches\", but I lack some confidence in models trained on LLM output, so I hope it wasn't that. There's a free preview on openrouter:  https://openrouter.ai/arcee-ai/trinity-large-preview:free  Testing it now in HugstonOne. Running smooth at 5.8 T/S : Loaded Trinity-Large-Preview-UD", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
{"anchor": "We put Claude Code in Rollercoaster Tycoon. Can't wait for someone to let Claude control a runescape character from scratch Would a way to take screenshots help? It seems to work for browser testing. > We don't know any C++ at all, and we vibe-coded the entire project over a few weeks. The core pieces of the build are\u2026 what a world! The opening paragraph I thought was the agent prompt haha > The park rating is climbing. Your flagship coaster is printing money. Guests are happy, for now. But you know what's coming: the inevitable cascade of breakdowns, the trash piling up by the exits, the queue times spiraling out of control. Edit: HN's auto-resubmit in action, ignore. > The only other notable setback was an accidental use of the word \"revert\" which Codex took literally, and ran git revert on a file where 1-2 hours of progress had been accumulating. Interesting article but it doesn\u2019t actually discuss how well it performs at playing the game. There is in fact a 1.5 hour YouTube video but it woulda been nice for a bit of an outcome postmortem. It\u2019s like \u201chere\u2019s the methods and set up section of a research paper but for the conclusion you need to watch this movie and make your own judgements!\u201d > kept the context above the ~60% remaining level where coding models perform at their absolute best Maybe this is obvious to Claude users but how do you know your remaining context level? There is UI for this? This is a cool idea. I wanted to do something like this by adding a Lua API to OpenRCT2 that allows you to manipulate and inspect the game world. Then, you could either provide an LLM agent the ability to write and run scripts in the game, or program a more classic AI using the Lua API. This AI would probably perform much better than an LLM - but an interesting experiment nonetheless to see how a language model can fare in a task it was not trained to do. Wonder how it would do with Myst. This is what I want but for PoE/PoE2 builds. I always get a headache just looking at ", "positive": "Toronto\u2019s network of pedestrian tunnels. Once on a lunch break I walked from St Andrew to King (parallel stations on the horns of line 1) in the tunnels and took the TTC back. Going overground is usually faster and easier to navigate, buts impressive how far you can go underground. One of these days I\u2019ll need to try an extreme point hike. One thing that I was surprised wasn't mentioned is the impact that I believe weather must have had on the development of the Path. Winters in Toronto get rather cold and snowy. Even with a dense downtown core, walking a few blocks outside can be rather unpleasant. I use to love exploring Path as a teenager. More northern cities like Montreal and Winnipeg also have very interesting indoor pedestrian systems. The one in Winnipeg is particularly useful, since there are approximately 72 hours per year that it's comfortable to be outside between the bone-chilling cold and the biblical swarms of mosquitos and flies in the summer.  https://en.wikipedia.org/wiki/Underground_City,_Montreal   https://en.wikipedia.org/wiki/Winnipeg_Walkway  We also have a 5K race in the PATH! [1] In the winter the tunnels are amazing for commute. [1]  https://www.bougebouge.com/en/shop/events/5km-bougebouge-tor...  >  Montreal has a similar system, while Tokyo, Osaka, Seoul, Hong Kong, Singapore and Houston have systems that resemble the Path in some respects. A few European cities also make considerable use of pedestrian tunnels, including Helsinki, Stockholm and Munich.  Japan's northernmost major city, Sapporo, has a very extensive one -- of those I've seen, it's the one that's most comparable to Toronto's. The other Japanese tunnel/undercity complexes are mostly subterranean malls around subway stations.  (This also applies to all of the ones in Hong Kong.)  But Sapporo's is seriously huge. I think the common denominator is that people would rather walk in a heated underground space when it gets cold. It's my understanding that underground walkways were c", "negative": "Second Win11 emergency out of band update to address disastrous Patch Tuesday. Is it only cloud storage files? I've noticed that in 2026 my windows 11 machine is slower than ever before, by a lot- barely able to render web pages. The start menu search is turning blank and shows a white screen whenever I search anything. Similar to how react apps break. It's been like this for 6 momths, across two laptops, fresh install of 25h2. I for one am enjoying my last few months of Windows 10, stable, responsive, no surprise updates at last. I was hit by this. Could RDP into machines using the regular client, but could not access Dev Boxes via Windows App. Getting real sick of the low quality AI slop. Using React in core parts of the Windows Shell, Microsof's inability to design and release an application using non-web technologies, and the sluggishness and lagginess and bloat of Windows in general has finally pushed me to dual boot Fedora on a separate drive. It is very nice having an Operating system that respects the Hardware I own and makes efficient use of it. My experience has been very good so far. Every device in my custom built desktop PC worked immediately. The only driver I had to build and install was for my XBOX Wireless dongle. Gaming has been really damn good. I installed Steam and my games just worked. No fiddling around with configs or anything. Even installing a custom Proton version to try it out is very simple. I've been on Fedora now for nearly a month and only boot into Windows for work. Eventually, I might get rid of Windows entirely. It'll take a massive U-turn from Microsoft on the philosophy for Windows for me to change my opinion now. And this is why I'm still running Win10 LTSC. No bloat, super fast, still gets security updates. People are blaming vibe coding but the real culprit was hiring leetcoders in the first place. I genuinely believe the stark decrease in quality of most products across the industry has been driven by that. it is so annoying "}
{"anchor": "The unbearable joy of sitting alone in a caf\u00e9. This is a highly romanticized view imo. I sit alone in cafes all the time, for many reasons. I don\u2019t feel particularly joyful about it nor weird. I just do it to take a break and have something to drink, or wait for someone or something. Often I don\u2019t look at my phone at all. That doesn\u2019t feel weird either, or rebellious, or whatever the author experiences. I don\u2019t understand the post at all. I\u2019d have gone to Japan. I\u2019ve been to Japan, it\u2019s awesome. This reminds me of the \u201ctechbro discovers very common x thing\u201d meme. Going to a coffee shop (that is 75% solo remote workers) without your phone and pretending it\u2019s some divine experience feels conceited. Do things you like, sometimes don\u2019t check your phone. Very well written title though. Japan is not the ideal place I would go to for a cafe, but I get the sentiment. When the weather is nice I love going for a morning walk with my dog on a lazy weekend morning and just sitting outside at a cafe reading a book. Coffee itself is secondary to this experience, it\u2019s mostly just the vibe of the place that brings me there. That\u2019s why small local cafes that don\u2019t like people to sit at tables for too long are so off putting. This author has never been alone with their thoughts before.... I didn\u2019t quite understand why sitting alone in a caf\u00e9 makes you a weirdo (is it an American thing?), but the piece was very well written. We all should learn how to be without electronics for every now and then, accompanied only our thoughts. It is good for the soul. I think the important part is leaving your phone and other devices home. Be alone, without even a possibility of connecting (apart from the old-fashione way of talking to an actual human being). People used to do this y\u2019know? Back then. Rawdogging a coffee, will try it. I did not know going to a cafe alone was a strange thing to do. In fact it is a place I would consider it is completely common to go alone - whereas a restaurant is less", "positive": "Ironwood: The first Google TPU for the age of inference. It looks amazing but I wish we could stop playing silly games with benchmarks. Why compare fp8 performance in ironwood to architectures which don't support fp8 in hardware? Why leave out TPUv6 in the comparison? Why compare fp64 flops in the El Capitan supercomputer to fp8 flops in the TPU pod when you know full well these are not comparable? [Edit: it turns out that El Capitan is actually faster when compared like for like and the statement below underestimated how much slower fp64 is, my original comment in italics below is not accurate] ( The TPU would still be faster even allowing for the fact fp64 is ~8x harder than fp8. Is it worthwhile to  misleadingly claim it's 24x faster instead of honestly saying it's 3x faster? Really? ) It comes across as a bit cheap. Using misleading statements is a tactic for snake oil salesmen. This isn't snake oil so why lower yourself? Can these be repurposed for other things? Encoding/decoding video? Graphics processing etc? edit:\n>It\u2019s a move from responsive AI models that provide real-time information for people to interpret, to models that provide the proactive generation of insights and interpretation. This is what we call the \u201cage of inference\u201d where AI agents will proactively retrieve and generate data to collaboratively deliver insights and answers, not just data. maybe i will sound like a luddite but im not sure i want this. I'd rather AI/ML only do what i ask it to. Some honest competition in the chip space in the machine learning race! Genuinely interested to see how this ends up playing out. Nvidia seemed 'untouchable' for so long in this space that its nice to see things get shaken up. I know they aren't selling the TPU as boxed units, but still, even as hardware that backs GCP services and what not, its interesting to see how it'll shake out! The first specifically designed for inference? Wasn\u2019t the original TPU inference only? Not knowing much about special-pur", "negative": "Web-based image editor modeled after Deluxe Paint. Is it simple to adapt file open/save in order to embed it in  https://exaequos.com  ? Nice. Vanilla js with a pretty clean code. From a quick look there is some components architecture and they are decoupled via an events bus. I used to implement evented architectures in winform apps in the past. On the one hand it may seem insane but in practice it was a really good choice. Nice! The code looks pretty neat! And also somehow clean. I like those projects, without all those boring constraints you have in \"enterprise\" or even worse start-up code. Source code is very readable and very comfortable to use application. This is surprising given it's a web application in modern age, did not expect that. Steffest was just showing off his entry for the color cycling competition at GERP 2026 which uses a few of his tools to produce including DPaint.  https://www.youtube.com/watch?v=VyB5cvA6f78  EDIT: I see he posted a link at the bottom of the Readme.md I guess I should have scrolled to the bottom first. I've been following this app for a while. Worth noting that the author is also a very talented graphic artist and demoscener. Works created with this tool frequently appear in various demoscene compos. I appreciate the nostalgia of it but DPII was a light themed tool, this one is dark themed, difficult for me to read. I run DPII in DoxBox on Linux like this: dosbox DP.EXE Something I don't see in your app is the Perspective tool. This is neat, some years ago i also thought of making a simple DPaint clone (though much simpler than what this project seems to do) and started by... painting the tool icons, then losing interest :-P. I did end up reusing them for a pixelart editing component for Lazarus though[0] (and put the icons in my \"Bad-Common-Icons\" icon set[1] that i use for my GUI programs). But i do want to, at some point, tackle making something like Paint Shop Pro 7 (for desktop, not web) because i think it has the best U"}
{"anchor": "UN declares that the world has entered an era of 'global water bankruptcy'.  Four billion people face severe water scarcity for at least one month each year  Does anyone know what this looks like for typical cases? The water just cuts off for a month in some places I guess? I can assure you there is plenty of water.    There are floods in lots of places every year.   The oceans are full of water that for just 5kWh we can desalinate 250 gallons. The problem is that the water and energy aren't where the users want it to be. But pipes are relatively cheap - if humanity cared enough, we could build pipes to distribute the plentiful water everywhere. But it turns out the people without much water tend to be in very poor places and warzones where there isn't much appetite for spending money on pipes. And all these huge new data centers are gonna make things worse:  https://www.eesi.org/articles/view/data-centers-and-water-co...  Before commenting water is cheap and plentiful please read the proposed definition. > Water bankruptcy refers to \u201ca state in which a human-water system has spent beyond its hydrological means for so long that it can no longer satisfy the claims upon it without inflicting unacceptable or irreversible damage to nature.\u201d I find this other article [1] more informative, including for instance the global map of Vulnerability to Water-Related Challenges taken from the actual report [2]. [1]  https://www.thebrighterside.news/post/our-world-is-entering-...  [2]  https://collections.unu.edu/eserv/UNU:10445/Global_Water_Ban...  I would no say the \"world\", but areas of it has as noted.  Like South Asia, SW N America, N Africa and Spain. For many of these areas, desalination could meet the gap, but someone will need to pay for it.  That is the main issue, no one wants to pay. Sounds like a bunch of useless scare mongering. Large scale Desalination is getting increasingly achievable:\n https://caseyhandmer.wordpress.com/2022/11/20/we-need-more-w...  Reminds me o", "positive": "Ask HN: Best Podcasts of 2025?. BetterOffline [0] by Ed Zitron [1] dissecting AI hype and boosters. By a long shot. The information density and clarity are outstanding. 0:  https://www.betteroffline.com/  1:  https://www.wheresyoured.at/  My gotos for listening while I do chores or drive this year have been:       - Stuff You Should Know https://stuffyoushouldknow.com/\n    - How to do Everything https://www.npr.org/podcasts/510384/how-to-do-everything   The Rest is History is good, depending on the topic. Both guests have a bit of bias which you have to sort of take into account, not that different from The Rest is Politics.\nMishal Husain has a new podcast on Bloomberg TV which so far was excellent.\nAlso from Bloomberg TV, Big Take is often interesting.\nI still enjoy Lex Fridman, again depending on the guest. Dwarkesh Patel same shit as Lex, but he pretends he knows something about AI. Hardcore History 73 - Mania for Subjugation III [1] Fall of Civilizations 20 - Persia - An Empire in Ashes [2] [1]  https://www.dancarlin.com/product/hardcore-history-73-mania-...  [2]  https://fallofcivilizationspodcast.com/  Most of my podcasts are movie related. If I had to purge them all and start with just 5 though I would go with. Blank Check\nThe Flophouse\n99% Invisible\nCautionary Tales\nThe Rewatchables I maintain The Flophouse is the funniest podcast around. Advent of Computing:     https://adventofcomputing.com/\n\n  https://podcasts.apple.com/us/podcast/advent-of-computing/id1459202600\n\n  https://adventofcomputing.libsyn.com/rss\n\n  https://www.youtube.com/@adventofcomputing4504/videos   Call me simple or provincial, but I really enjoyed \"Good Hang\" from Amy Poehler. It's a breezy interview with interesting people (doesn't hurt that I'm a long-time SNL fan). Citations Needed:  https://citationsneeded.libsyn.com/       Acquired (Long episodes about companies, recents include: \n  coca-cola, trader joe's & alphabet)\n  Dwarkesh Podcast (Inquisitive curious host, mostly \"AGI\"\n  relat", "negative": "Second Win11 emergency out of band update to address disastrous Patch Tuesday. Is it only cloud storage files? I've noticed that in 2026 my windows 11 machine is slower than ever before, by a lot- barely able to render web pages. The start menu search is turning blank and shows a white screen whenever I search anything. Similar to how react apps break. It's been like this for 6 momths, across two laptops, fresh install of 25h2. I for one am enjoying my last few months of Windows 10, stable, responsive, no surprise updates at last. I was hit by this. Could RDP into machines using the regular client, but could not access Dev Boxes via Windows App. Getting real sick of the low quality AI slop. Using React in core parts of the Windows Shell, Microsof's inability to design and release an application using non-web technologies, and the sluggishness and lagginess and bloat of Windows in general has finally pushed me to dual boot Fedora on a separate drive. It is very nice having an Operating system that respects the Hardware I own and makes efficient use of it. My experience has been very good so far. Every device in my custom built desktop PC worked immediately. The only driver I had to build and install was for my XBOX Wireless dongle. Gaming has been really damn good. I installed Steam and my games just worked. No fiddling around with configs or anything. Even installing a custom Proton version to try it out is very simple. I've been on Fedora now for nearly a month and only boot into Windows for work. Eventually, I might get rid of Windows entirely. It'll take a massive U-turn from Microsoft on the philosophy for Windows for me to change my opinion now. And this is why I'm still running Win10 LTSC. No bloat, super fast, still gets security updates. People are blaming vibe coding but the real culprit was hiring leetcoders in the first place. I genuinely believe the stark decrease in quality of most products across the industry has been driven by that. it is so annoying "}
{"anchor": "Show HN: One Human + One Agent = One Browser From Scratch in 20K LOC. I set some rules for myself: three days of total time, no 3rd party Rust crates, allowed to use commonly available OS libraries, has to support X11/Windows/macOS and can render some websites. After three days, I have it working with around 20K LOC, whereas ~14K is the browser engine itself + X11, then 6K is just Windows+macOS support. Source code + CI built binaries are available here if you wanna try it out:  https://github.com/embedding-shapes/one-agent-one-browser  This is a notably better demonstration of a coding agent generated browser than Cursor's FastRender - it's a fraction of the size (20,000 lines of Rust compared to ~1.6m), uses way fewer dependencies (just system libraries for rendering images and text) and the code is actually quite readable - here's the flexbox implementation, for example:  https://github.com/embedding-shapes/one-agent-one-browser/bl...  Here's my own screenshot of it rendering my blog -  https://bsky.app/profile/simonwillison.net/post/3mdg2oo6bms2...  - it handles the layout and CSS gradiants really well, renders the SVG feed icon but fails to render a PNG image. I thought \"build a browser that renders HTML+CSS\" was the perfect task for demonstrating a massively parallel agent setup because it couldn't be productively achieved in a few thousand lines of code by a single coding agent. Turns out I was wrong! This is awesome. Would you be willing to share more about your prompts? I'm particularly interested in how you prompted it to get the first few things working. This post is far more interesting than many others on the same subject, not because of what is built but because of how it it is built. There is a ton of noise on this subject and most of it seems to focus on the thing - or even on the author - rather than on the process, the constraints and the outcome. > I'm going to upgrade my prediction for 2029: I think we're going to get a production-grade web brows", "positive": "Qwen3-TTS family is now open sourced: Voice design, clone, and generation. great news, this looks great!  is it just me, or do most of the english audio samples sound like anime voices? I still don't know anyone who managed Qwen3-Omni to work properly on a local machine. Qwen team, please please please, release something to outperform and surpass the coding abilities of Opus 4.5. Although I like the model, I don't like the leadership of that company and how close it is, how divisive they're in terms of politics. How does the cloning compare to pocket TTS? it isn't often that tehcnology gives me chills, but this did it. I've used \"AI\" TTS tools since 2018 or so, and i thought the stuff from two years ago was about the best we were going to get. I don't know the size of these, i scrolled to the samples. I am going to get the models set up somewhere and test them out. Now, maybe the results were cherrypicked. i know everyone else who has released one of these cherrypicks which to publish. However, this is the first time i've considered it plausible to use AI TTS to remaster old radioplays and the like, where a section of audio is unintelligible but can be deduced from context, like a tape glitch where someone says \"HEY [...]LAR!\" and it's an episode of Yours Truly, Johnny Dollar... I have dozens of hours of audio of like Bob Bailey and people of that era. Kind of a noob, how would I implement this locally?\nHow do I pass it audio to process. I'm assuming its in the API spec? Huh. One of the English Voice Clone examples features Obama. If you want to try out the voice cloning yourself you can do that an this Hugging Face demo:  https://huggingface.co/spaces/Qwen/Qwen3-TTS  - switch to the \"Voice Clone\" tab, paste in some example text and use the microphone option to record yourself reading that text - then paste in other text and have it generate a version of that read using your voice. I shared a recording of audio I generated with that here:  https://simonwillison.net/", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
{"anchor": "Ask HN: Iran's 120h internet shutdown, phones back. How to stay resilient?.  https://en.wikipedia.org/wiki/Stoicism  Starlink and/or BGAN/satellite phones. Maybe  https://meshtastic.org ? If the phones are working, 56k modem. HAM radio is your best option. WiFi Halow is a longer range protocol (still probably not long enough). But something like this can get people connected:  https://openmanet.net  Old fashioned phone trees can be really useful IMHO OP. We used them when I worked in a school. If there was winter weather, you'd call say, everyone with a last name from A to G in the staff directory, someone else calls G to K, and so on and so forth. You can combine the phone tree with literal runners -- so basically, someone takes their burner and calls suburbs A,B,C and D and then the runners go out and pass the word about the protest or action. V.92 dial-up. Slow and expensive, but it's Internet access. Problem is that most methods involve making your location known openly. The Dark Forest book of the Remembrance of the Earth Past explains why it is not a good idea to do so in the current circumstances For dense areas, mesh applications like BitChat (Jack Dorsey) could bypass the need for a network with p2p bluetooth mesh networks. And works with existing devices, vs something like meshtastic which needs an installed base (afaik).  https://en.wikipedia.org/wiki/Bitchat  some DNS tunneling solutions work (dnstt for example). Also, many people have smuggled Starlink are are providing proxies inside Iran. Ideally cjdns or similar can be used inside the country to create an alternative encrypted mesh network inside the borders, with some \"exit nodes\" out. HF radio. Highly depdendent on space weather, but generally I can communicate around the world with only 100 watts and a long wire. Be aware though that transmitting on any radio is like turning on a giant, extremely bright light bulb directly above your antenna. Anyone with basic radio know-how will be able to hear y", "positive": "String theory can now describe a universe that has dark energy?. Only in universe with 5 dimensions. Shouldn't string theory be given up on at this point? This theory has existed for over 50 years and hasn't produced any results. Even the predictions made by it such as e.g. supersymmetry have not been confirmed despite searching for them at particle colliders. I foolishly sat in 8.821 [0] while at MIT thinking I could make sense out of quantum gravity. Most of the math went over my head, but the way I understand this paper, it\u2019s basically a cosmic engineering fix for a geometry problem. Please correct me if necessary. String theory usually prefers universes that want to crunch inwards (Anti-de Sitter space). Our universe, however, is accelerating outwards (Dark Energy). To fix this, the authors are essentially creating a force balance. They have magnetic flux pushing the universe's extra dimensions outward (like inflating a tire), and they use the Casimir effect (quantum vacuum pressure) to pull them back inward. When you balance those two opposing pressures, you get a stable system with a tiny bit of leftover energy. That \"leftover\" is the Dark Energy we observe. You start with 11 dimensions (M-theory) and roll up 6 of them to get this 5D model. It sounds abstract, but for my engineer brain, it's helpful to think of that extra 5th dimension not as a \"place\" you can visit, but as a hidden control loop. The forces fighting it out inside that 5th dimension are what generate the energy potential we perceive as Dark Energy in our 4D world. The authors stop at 5D here, but getting that control loop stable is the hardest part The big observatiom here is that this balance isn't static -- it suggests Dark Energy gets weaker over time (\"quintessence\"). If the recent DESI data holds up, this specific string theory solution might actually fit the observational curve better than the standard model. [0]  https://ocw.mit.edu/courses/8-821-string-theory-and-holograp...  Hm, string", "negative": "Recent discoveries on the acquisition of the highest levels of human performance. This sort of tracks for me. The smartest people I know as adults mostly fucked around a lot and had wide interests that all culminated in them doing a great thing greatly. The smartest people I know as kids spent hours grinding on something and crashed out in college and are mostly average well-to-dos now. Hardly a recent discovery. This is basically the entire foreword of David Epstein's book called Range: Why Generalists Triumph in a Specialized World A summary, since the paper isn't open access:  https://scientificinquirer.com/2025/12/21/the-counterintuiti...  > For example, world top-10 youth chess players and later world top-10 adult chess players are nearly 90% different individuals across time. Top secondary students and later top university students are also nearly 90% different people. Likewise, international-level youth athletes and later international-level adult athletes are nearly 90% different individuals. Motivation if you feel like you're young and failing That could simply be explained by early high achievers being worked hard by their parents or something else while people with innate abilities making progress slower (because most people are not overworked). For the first group they sizzle either because the pressure is removed as they grow up or because they hit their ceiling. Couldn't this be explained by Berkson's Paradox [0]? [0]  https://xcancel.com/AlexGDimakis/status/2002848594953732521  Seems very Taleb's Ugly Surgeon / Berkson's Paradox to me. It's like how software engineers who are at Google are worse if they're better competitive programmers. e.g.  https://viz.roshangeorge.dev/taleb-surgeon/  Exponential growth is the path of longsuffering, and one doesn't always make it. It sucks and looks and feels bad for all involved. This is why advice such as, \"Ignore the naysayers.\" is clutch. And other advice once one starts to rocket shoot like \"Stay in your lane."}
{"anchor": "In Europe, wind and solar overtake fossil fuels. Curious if this will eventually change China's calculus with regards to Russia. If Europe is a big customer for Chinese exports, and Russia is antagonizing, it seems like China would have an incentive to put pressure on Russia. It already seems like Russia is positioned to be completely subservient to China in the future. Solar prices in the US are criminal, protecting oil and gas who bought all the politicians. Canada here. 7.6kw on our roof for $0 out of pocket thanks to $5k grant and $8k interest free loan. It makes 7.72Mwh per year, worth $1000. Tight valley, tons of snow.\nWe put that on the loan for 8 years, then get $1000 per year free money for 20 years or so. Biggest no brainer of all time. Dad in Victoria Australia just got 10.6kw fully installed and operational for $4000 AUD. ($2,700 USD) Australia has so much electricity during the day they\u2019re talking about making I free for everyone in the middle of the day.  https://www.abc.net.au/news/2025-11-03/energy-retailers-offe...  But Trump explained to us yesterday, how wind and solar is for losers. Surely, we should be looking in to how we can transition back to fossils. Now, let's aim at total energy consumption, not just electricity generation. The UK has some of the highest energy costs in the world due to the stupid Net Zero taxes. Our economy and manufacturing is suffering. Every time, over the years, that there has been some kind of headline saying renewables have overtaken fossil fuels, when you look at it a bit more closely there is always a big 'but'.   For example, it was compared to coal (not taking into account electricity from gas), or it was for one day, or it was a percentage of new installations, or it excludes winter, includes nuclear etc. This time, however, it looks like it's actually true and that's just for wind and solar.   This is incredible, and done through slowly compounding gains that didn't cause massive economic hardships along the w", "positive": "What's the strongest AI model you can train on a laptop in five minutes?. Perhaps grimlock level:  https://m.youtube.com/shorts/4qN17uCN2Pg  Instead of time it should be energy. What is the best model you can train with a given budget in Joules. Then the MBP and the H100 are on a more even footing. I love seeing explorations like this, which highlight that easily accessible hardware can do better than most people think with modern architectures. For many novel scientific tasks, you really don't need an H100 to make progress using deep learning over classical methods. I suspect one can go a lot further by adopting some tweaks from the GPT-2 speedrun effort [0], at minimum Muon, better init and carefully tuning learning rate. [0]:  https://github.com/KellerJordan/modded-nanogpt  But supposing you have a real specific need to train, is the training speed still relevant? Or do the resources spent on gathering and validating the data set dwarf the actual CPU/GPU usage? The most powerful Macbook Pro currently has 16 CPU cores, 40 GPU cores, and 128 GB of RAM (and a 16-core \u201cneural engine\u201d specifically designed to accelerate machine learning). Technically, it is a laptop, but it could just as well be a computer optimized for AI. > Paris, France is a city in North Carolina. It is the capital of North Carolina, which is officially major people in Bhugh and Pennhy. The American Council Mastlandan, is the city of Retrea. There are different islands, and the city of Hawkeler: Law is the most famous city in The Confederate. The country is Guate. I love the phrase \"officially major people\"! I wonder how it could be put to use in everyday speech? Not the point of the exercise obviously, but at five minutes' training I wonder how this would compare to a Markov chain bot. Any reason to upgrade an M2 16GB macbook to a M4 ..GB (or 2026 M5) for local LLMs? Due an upgrade soon and perhaps it is educational to run these things more easily locally? You could train an unbeatable tic-tac-to", "negative": "Doing the thing is doing the thing. As a person with ADHD, I feel personally attacked. This is very similar to [1] (as discussed here [2]). It is a good message though, which is why I remember the earlier post at all. 1.  https://strangestloop.io/essays/things-that-arent-doing-the-...  2.  https://news.ycombinator.com/item?id=45939431  On the other hand.. planning, preparation and mise-en-place can help with doing the thing. \"Doing it badly is doing the thing.\" This one works for me, and I've learned it from a post on HN. Whenever I feel stuck or overthink how to do something, just do it first - even with all the flaws that I'm already aware of, and if it feels almost painful to do it so badly. Then improve it a bit, then a bit, then before I know it a clear picture start to emerge... Feels like magic. \"Everybody wants to be a bodybuilder but nobody wants to lift no heavy ass weights!\" I used to think this. Then I noticed how often \"preparation\" became its own infinite loop. At work we built something from a 2-page spec in 4 months. The competing team spent 8 months on architecture docs before writing code. We shipped. They pivoted three times and eventually disbanded. Planning has diminishing returns. The first 20% of planning catches 80% of the problems. Everything after that is usually anxiety dressed up as rigor. The article's right about one thing: doing it badly still counts. Most of what I know came from shipping something embarrassing, then fixing it. \"Failing while doing the thing is doing the thing.\" I needed this today. Currently questioning my career choices, as I hit my first wall where people are involved. Gave me quite the headache. I kinda agree, but I also gain pleasure from doing all those things that are not supposed to be \"the thing\". The thinking, the dreaming, the visualizing... I just like that. I do it a lot when working on personal projects (which some of them I never ship). I think it's fine, and I wouldn't go as far as saying that those th"}
{"anchor": "Ask HN: Burned out from tech, what else is there?. I\u2019m still on the fence of buying a large cargo van like a Sprinter and outfitting it to be a one-man \u2018expeditor\u2019 cargo carrier. Travel the country hauling one-off pallets from point A to B, check the DAT boards for loads and journeys abound. I'm in a similar situation, thinking about a paycut or sabbattical just to do something different. I think it's key to think about what makes you happy and interested in your work, and then find a way to map from your current position to a new position where you can do more of that. If you're ever unsure or worried about making a move, remember that life is fluid, things change, doors open and close all the time. Taking a step forward into the unknown will light the path to the next step, but taking that first step requires accepting some uncertainty and trusting it will work out anyway. If you want to try something totally different, check if you have a local volunteer fire department looking for new recruits.  I know a few people who eventually transitioned from tech and made it into their full-time career.  I think part of the draw is you show up, solve a problem, leave and feel good about it. When I reached this point, I left to start my own company. Build something that would actually be mine. Though obviously that's easier said than done. For a few years I switched to supporting medical research, which made me feel good. The tech BS sadly penetrated there after a few years too which ended that for me. Take a break for a few months to recalibrate what you want from life.  Tech will still be here when you\u2019re ready again.  Go travel, use your physical body to walk and hike and lift, have a couple of flings, go to a bar at noon, work a few temp jobs, apply minimalism in your life, learn about something you like, etc. I often dream about being a carpenter, a park ranger, or a truck driver. But it feels like it's too late, and my family would suffer from the lack of funds as I t", "positive": "Toad is a unified experience for AI in the terminal. I'm really looking forward to trying this out over Christmas break. Textualize is awesome for building Python console apps. This looks really cool. I wonder if they support vi keybinds Hi. Will McGugan here. I built Toad. Ask me anything. This looks great! Looking forward to trying it out. I recently tried moving to OpenCode but it didn\u2019t quite scratch the itch UX wise. I see what you did what that intro and I approve :) This is absolutely awesome but the little jokey captions that Claude did (Discombobulating... Laminating...) all that stuff, they were a little annoying but cute enough, but whatever is running this one (I did not murder him... I thought I was special....) they are genuinely offputtingly bad. This great app doesn't need clunky humour front and centre, I'm not sure if it's Claude or toad but it seems markedly worse than Claude used to be. I already used Toad to run a conversion task I've been procastinating on. It worked perfectly and looked splendid doing so. Excited to dig in further. toad is next level in many ways Very excited to see this come out - though coding agents are impressive their UIs are a bit of a mixed bag. Textual offers incredibly impressive terminal experiences so I'm very much looking forward to this. I wonder how much agentic magic it'll be able to include though - Claude Code often seems like a lot of its intelligence comes from the scaffolding, not just the LLM.  I'm excited to see! It would be a matrushka to run Toad in a Zed terminal. The name Toad gave me a flashback to Tool for Oracle Application Development, an IDE and debugger for SQL and pl/sql back in the 1990s. I\u2019m not a big fan of the name Toad, but the Textual framework is fantastic. I\u2019ve been using it for years in a small project and it\u2019s just a wonderful tool - it makes it really easy to get a super fast little UI for scripts. I strongly resonate with the problem statement, but this implementation was very far o", "negative": "Without benchmarking LLMs, you're likely overpaying. > He's a non-technical founder building an AI-powered business. It sounds like he's building some kind of ai support chat bot. I despise these things. I'd second this wholeheartedly Since building a custom agent setup to replace copilot, adopting/adjusting Claude Code prompts, and giving it basic tools, gemini-3-flash is my go-to model unless I know it's a big and involved task. The model is really good at 1/10 the cost of pro, super fast by comparison, and some basic a/b testing shows little to no difference in output on the majority of tasks I used Cut all my subs, spend less money, don't get rate limited Depends on what you\u2019re doing. Using the smaller / cheaper LLMs will generally make it way more fragile. The article appears to focus on creating a benchmark dataset with real examples. For lots of applications, especially if you\u2019re worried about people messing with it, about weird behavior on edge cases, about stability, you\u2019d have to do a bunch of robustness testing as well, and bigger models will be better. Another big problem is it\u2019s hard to set objectives is many cases, and for example maybe your customer service chat still passes but comes across worse for a smaller model. Id be careful is all. The author of this post should benchmark his own blog for accessibility metrics, text contrast is dreadful.. On the other hand, this would be interesting for measuring agents in coding tasks, but there's quite a lot of context to provide here, both input and output would be massive. Wow, this was some slick long form sales work. I hope your SaaS goes well. Nice one! Anecdotal tip on LLM-as-judge scoring - Skip the 1-10 scale, use boolean criteria instead, then weight manually e.g. - Did it cite the 30-day return policy? Y/N\n - Tone professional and empathetic? Y/N\n - Offered clear next steps? Y/N Then: 0.5 * accuracy + 0.3 * tone + 0.2 * next_steps Why: Reduces volatility of responses while still maintaining creativ"}
{"anchor": "Why a 'Boring' Life Might Be the Happiest One. Hard agree! The challenge as you get older is reducing complexity so simple moments can be enjoyed, but it doesn't come easy. Turned 50 recently and I have come to the same conclusion.  I just want to live a simply life with simple joys. However that would only be possible because I've been working and saving since I was 15 years old. I find myself always struggling between being ambitious and being happy. Ideally I'd like to be both. But when one gets on the \"ambitious\" treadmill, capitalism wants one to work 24/7/365. Your competition is showing off how they worked until 4am, worked through the holidays, launched products on Sunday, and slept in the office, as \"dedication\". That culture makes me unhappy because I lose my physical health and mental health doing that. I'm happy and do my best work when I can go home, cook creative dinners, enjoy company of my partner, and enjoy the sunrises and sunsets in the mountains on the weekends. I'm not sure if a 'Boring Life' is for me. But I am sure I need some seasons of my life to be boring. Since I was 20 I've grinded away at my career, side hustles, etc... It made me happy. But at a certain point I got far enough ahead I felt complete. I needed a boring phase. I now wake up, have breakfast with my family, go to work, come home, play with my kids, watch a show, and go to bed. This is a day I would have scoffed at 10 years ago. But it now makes me happy. I don't think it would make me happy if I hadn't went for something the last 15. And it might not make me happy forever, but it's perfect for me right now. The author mistakes introversion to do-little/nothing. Many introverts love socializing and being around friends, it\u2019s just energy-depleting and takes (more) time to recharge. That said, doing little or nothing is quite relaxing, especially on rainy days. > I feel the world has become too fast. Too restless. Too demanding. We don\u2019t say it, but there\u2019s always this quiet pre", "positive": "X For You Feed Algorithm. anything interesting? anything that is a surprise? what is the difference between this and  https://github.com/twitter/the-algorithm  I did not expect to see Rust. They seem to have forgotten to commit Cargo.toml though. Oh I see it is not meant to be built really. Some code is omitted. ooh, LLM Recsys alert! (we had an LLM Recsys track at ai.engineer last year). official announcement here:  https://x.com/XEng/status/2013471689087086804  looks like this is the \"for you\" feed, once again shared without weights so we only have so much visibility into the actual influence of each trait. \"We have eliminated every single hand-engineered feature and most heuristics from the system. The Grok-based transformer does all the heavy lifting by understanding your engagement history (what you liked, replied to, shared, etc.) and using that to determine what content is relevant to you.\" aka it's a black box now. the README is actually pretty nice, would recommend reading this. it doesnt look too different form Elon's original code review tweet/picture  https://x.com/elonmusk/status/1593899029531803649?lang=en  sharing additonal notes while diving through the source:  https://deepwiki.com/xai-org/x-algorithm  and a codemap of the signal generation pipeline:  https://deepwiki.com/search/make-a-map-of-all-the-signals_3d...  - Phoenix (out of network) ranker seems to have all the interesting predictive ML work. it estimates P(favorite), P(reply), P(repost), P(quote), P(click), P(video_view), P(share), P(follow_author), P(not_interested), P(block_author), P(mute_author), P(report) independently and then the `WeightedScorer` combines them using configurable weights. there's an extra DiversityScore and OONScore to add some adjustments but again dont know the weights  https://deepwiki.com/xai-org/x-algorithm/4.1-phoenix-candida... \n- other scores of interest: photo_expand_score, and dwell_score and dwell_time. share via copy, share, and share  via dm are all obvi", "negative": "Y Combinator website no longer lists Canada as a country it invests in. I haven't talked to anyone at YC about this, have no inside information, and can't read the article*, but I imagine this is some technical change about where startups are incorporated. I'm sure applications from Canadian founders are as welcome as ever and there will be no change on the level of which applications get funded. (* edit: I originally posted this in  https://news.ycombinator.com/item?id=46772809  but have since merged the thread hither) Is it politically motivated or does it have to do with Canadian tech not requiring investment because of its stability? > \u201cIt\u2019s the Valley-or-bust mentality that breaks the ecosystem and really hurts Canada,\u201d Gomez said. Canadian pride isn't enough to keep a company in Canada. There are real and significant economic incentives to move elsewhere. That said, it's disappointing that YC no longer supports Canadian companies. That's truly saddening. I hope there will be more VC backing in Canada because the talent is definitely there. Wonder if the founders not being US citizens or possibly even residents will hinder their ability to maintain their company.  Or, whether this change increases the likelihood of being replaced when the startup shows some success. Also, being foreign in the US is a concern at the moment.  Hell, being native in the US is a concern at the moment... This is extremely misleading. YC still backs Canadian founders (and other international founders). There must have been one too many painful experiences investing in companies based in Canada. Creating or converting to a US-based entity is a standard ask for most international founders who want to participate YC and I suppose something has changed such that Canada is no longer an exception to that. Canada's economy is dominated by a few big companies because the government makes too many rules. It costs too much to start a business here. In politics, only two parties really matter. T"}
{"anchor": "39th Chaos Communication Congress Videos.  https://media.ccc.de/v/39c3-a-post-american-enshittification...  Cory Doctorow's talk is quite strong. Where were people's favourite lectures? I attended 7 talks. My favourite talk by far was hacking the GPG. Brilliant, really:  https://media.ccc.de/v/39c3-to-sign-or-not-to-sign-practical...  The \"In-house electronics manufacturing from scratch\" was a very inspiring talk:  https://media.ccc.de/v/39c3-in-house-electronics-manufacturi...  The rest were less good for me personally. Either over-dramatic and shallow (with a sexy-sounding topic) or too procedural in topics I'm not an expert in. One interesting detail: In previous years, Joscha Bach gave a talk on AI, consciousness, and related topics (see e.g. [0]). A similar talk was planned for this year as well, but after emails between him and Epstein were made public (see his comment on this in [1]), his talk was canceled. Instead, there appears to have been an event that critically addressed the situation [2]. Unfortunately it was not recorded. Did anyone attend? A discussion between Joscha and his critics would have been really interesting. [0]  https://media.ccc.de/v/38c3-self-models-of-loving-grace  [1]  https://joscha.substack.com/p/on-the-jeffrey-epstein-affair  [2]  https://events.ccc.de/congress/2025/hub/en/event/detail/tech...  Some popular selections with discussion so far:  Bluetooth Headphone Jacking: A Key to Your Phone [video]   https://news.ycombinator.com/item?id=46453204   Hacking Washing Machines [video]   https://news.ycombinator.com/item?id=46428496   Escaping containment: A security analysis of FreeBSD jails [video]   https://news.ycombinator.com/item?id=46436828   All my Deutschlandtickets gone: Fraud at an industrial scale [video]   https://news.ycombinator.com/item?id=46411930  I haven't seen all of them (which I wanted to see) yet, I had a lot of fun with various talks. Thus far, my favourite one was hands down [1], and I can explain why. I am not at", "positive": "We put Claude Code in Rollercoaster Tycoon. Can't wait for someone to let Claude control a runescape character from scratch Would a way to take screenshots help? It seems to work for browser testing. > We don't know any C++ at all, and we vibe-coded the entire project over a few weeks. The core pieces of the build are\u2026 what a world! The opening paragraph I thought was the agent prompt haha > The park rating is climbing. Your flagship coaster is printing money. Guests are happy, for now. But you know what's coming: the inevitable cascade of breakdowns, the trash piling up by the exits, the queue times spiraling out of control. Edit: HN's auto-resubmit in action, ignore. > The only other notable setback was an accidental use of the word \"revert\" which Codex took literally, and ran git revert on a file where 1-2 hours of progress had been accumulating. Interesting article but it doesn\u2019t actually discuss how well it performs at playing the game. There is in fact a 1.5 hour YouTube video but it woulda been nice for a bit of an outcome postmortem. It\u2019s like \u201chere\u2019s the methods and set up section of a research paper but for the conclusion you need to watch this movie and make your own judgements!\u201d > kept the context above the ~60% remaining level where coding models perform at their absolute best Maybe this is obvious to Claude users but how do you know your remaining context level? There is UI for this? This is a cool idea. I wanted to do something like this by adding a Lua API to OpenRCT2 that allows you to manipulate and inspect the game world. Then, you could either provide an LLM agent the ability to write and run scripts in the game, or program a more classic AI using the Lua API. This AI would probably perform much better than an LLM - but an interesting experiment nonetheless to see how a language model can fare in a task it was not trained to do. Wonder how it would do with Myst. This is what I want but for PoE/PoE2 builds. I always get a headache just looking at ", "negative": "A 26,000-year astronomical monument hidden in plain sight (2019). I first heard about this in a Graham Hancock book. Found it a fascinating example of an attempt to encode a date that far distant future generations might understand (provided it survives). That was an excellent rabbit hole to go down while eating lunch :) For a hypothesis concerning the precession of the equinoxes and religious pantheons, see  https://news.ycombinator.com/item?id=38761574  > There is an angle for doubt, for sorrow, for hate, for joy, for contemplation, and for devotion. I\u2019m so intrigued - what was going on inside Hansen's brain? This is the kind of stuff I love about ancient architecture. It seems they were full of such clever things (or maybe only the few constructions which survived until today). Its nice to see that some people still care about creating such thoughtful art for modern constructions. It seems that most building of our time are just optimized for fast and efficient construction. I hope there are many more out there, so that Earth's Graham Hancock of the year 16000 has something to explore on his/her ayahuasca trip. Sounds like it's about the precession of the equinoxes and the new \"Age of Aquarius\". More: >  Due to the precession of the equinoxes (as well as the stars' proper motions), the role of North Star has passed from one star to another in the remote past, and will pass in the remote future. In 3000 BC, the faint star Thuban in the constellation Draco was the North Star, aligning within 0.1\u00b0 distance from the celestial pole, the closest of any of the visible pole stars.[8][9] However, at magnitude 3.67 (fourth magnitude) it is only one-fifth as bright as Polaris, and today it is invisible in light-polluted urban skies.  >  During the 1st millennium BC, Beta Ursae Minoris (Kochab) was the bright star closest to the celestial pole, but it was never close enough to be taken as marking the pole, and the Greek navigator Pytheas in ca. 320 BC described the celestial"}
{"anchor": "Ask HN: What did you read in 2025?. Lots of news and articles, but also \"The Craft\", a history of Freemason's by John Dickie, was one of the more interesting books. Frankenstein. Superb science fiction, very readable even though written 200 years ago. And Wuthering Heights, which strangely like Frankenstein, has a complex narrative structure and an unhinged, obsessive central character - Emperor of Rome by Mary Beard, very entertaining. - Lolita, it's mostly what you've read about it. - a few short stories by Heinrich von Kleist. I mostly read fiction but I made time for a couple of nonfiction books this year. On the fiction side I really enjoyed \"Luminous\" and \"When We Where Real\". -  https://en.wikipedia.org/wiki/Luminous_(novel)  -  https://www.simonandschuster.com/books/When-We-Were-Real/Dar...  On the nonfiction side, I can recommend \"Careless People\" and \"Apple in China\". -  https://en.wikipedia.org/wiki/Careless_People  -  https://en.wikipedia.org/wiki/Apple_in_China  I read Sad Tiger by Neige Sinno. Really unsettling but definitely worth reading. One of my favourite reads from this past year was  Infinite Powers: How Calculus Reveals the Secrets of the Universe  by Steven Strogatz. It's a wonderful review of the history of calculus, including intuitive explanations of the basics. History of the Franks, by Gregory of Tours Getting into reading again this year after a long break. The most memorable read of this year was \"The Count of Monte Cristo\" (1846) by Alexander Dumas . It's one of the greatest stories ever told. It's ~1250 pages but I sped through it in 3 weeks even if I'm a slow reader. Highly recommended! I also read The Stranger by Camus and the two top Orwells which lived up to the hype. Very much enjoyed the Hyperion Cantos series by Dan Simmons.  https://en.wikipedia.org/wiki/Hyperion_Cantos  I got really into Hemingway\u2019s work, reading all the best ones, but my favourite being \u2018A moveable feast\u2019 his diary essentially released at the end of his life", "positive": "Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model. > For complex tasks, Kimi K2.5 can self-direct an agent swarm with up to 100 sub-agents, executing parallel workflows across up to 1,500 tool calls. > K2.5 Agent Swarm improves performance on complex tasks through parallel, specialized execution [..] leads to an 80% reduction in end-to-end runtime Not just RL on tool calling, but RL on agent orchestration, neat! Those are some impressive benchmark results. I wonder how well it does in real life. Maybe we can get away with something cheaper than Claude for coding. Kimi was already one of the best writing models. Excited to try this one out Huggingface Link:  https://huggingface.co/moonshotai/Kimi-K2.5  1T parameters, 32b active parameters. License: MIT with the following modification:  Our only modification part is that, if the Software (or any derivative works\nthereof) is used for any of your commercial products or services that have\nmore than 100 million monthly active users, or more than 20 million US dollars\n(or equivalent in other currencies) in monthly revenue, you shall prominently\ndisplay \"Kimi K2.5\" on the user interface of such product or service.  Actually open source, or yet another public model, which is the equivalent of a binary? URL is down so cannot tell. I've read several people say that Kimi K2 has a better \"emotional intelligence\" than other models. I'll be interested to see whether K2.5 continues or even improves on that. There are so many models, is there any website with list of all of them and comparison of performance on different tasks? Curious what would be the most minimal reasonable hardware one would need to deploy this locally? The chefs at Moonshot have cooked once again. As your local vision nut, their claims about \"SOTA\" vision are absolutely BS in my tests. Sure it's SOTA at standard vision benchmarks. But on tasks that require proper image understanding, see for example BabyVision[0] it appears very much lacking compar", "negative": "Quaternion Algebras. I like quaternions as much as the next guy (I\u2019ve used them in numerical computations etc), but what is it about them that makes them show up on the front page every few weeks? One of perks of PhD of quantum physics is that quaternions get mundane, vide  https://en.wikipedia.org/wiki/Pauli_matrices . SU(2) is everywhere (normalized quaternions). These are are more common than regular rotations of 3D space, O(3). Because SU(2) we get a lot of interesting phenomena, including that there are two types of particles, bosons and fermions. We get some interesting phenomena that only rotating by 720deg (two full rotations) bring back to the initial state. And I am not talking only about USB-A, but about spinors ( https://en.wikipedia.org/wiki/Spinor ) - there are some party tricks around that (vide  https://www.reddit.com/r/physicsmemes/comments/181oldw/a_ger... ). the late Doug Sweetser had in the 90s a website where he used quaternions (and quaternion analysis) for describing physics from his viewpoint. it was quit interesting. he had several github repositories [0] where he described his ideas. worth a look [0]  https://github.com/dougsweetser?tab=repositories  average period between quaternion posts what about kalman filters? Quaternions are left handed spinors.  Please use the correct unified math models (clidfford algebra) rather than these 3d only hacks. They sound shiny and mysterious? The word \"quaternion\" just rolls of the tongue. I always upvote it. Baez wrote some ideas in [1], one I'm liking connects Lorentz group in dimensions 3,4,6 and 10 with the modular group SL(2,Z) that is at a crossroads of several hardcore math themes. For Lie algebras: sl(2, R) \u2245 so(2,1) sl(2, C) \u2245 so(3,1) sl(2, H) \u2245 so(5,1) sl(2, O) \u2245 so(9,1) Dirac equation is the C case, the other cases have their uses. [1]  https://arxiv.org/abs/math/0105155  That quaternions also solve for what we normally have 3D+time for. And Lewis Carroll (Oxford (Math)), preferred Euclidean "}
{"anchor": "Show HN: SF Microclimates. My favorite weather map for SF is PurpleAir:  https://map.purpleair.com/environment-estimated-temerature-f...  There are thousands of sensors around the city. You can get a sense of shade-vs-sun temperatures by the spread of numbers you see (on cloudy days, the reported temperatures will be much closer together, while on sunny days, sensors in the sun will report elevated temperatures.) You do need to make sure to disable indoor sensors, and keep in mind that some sensors are faulty. (I've seen some that have been reporting a constant temperature for years.) How does this compare to  https://www.wunderground.com  ? Is that the source of the data? Is it possible to get individual sensor data via this API? It seems weird to me that there's no human readable version on the webpage? Usually what I want the weather for is to choose what to wear, not to put in a bash script or an LLM or something. This happens in Portland as well! Can this be adapted/updated to work here? I made a quick website from this API that shows all of the neighborhoods, searchable, sortable.  https://v0-weather-app-one-coral.vercel.app/  Surprisingly, Lands End is the highest temp right now. Multiple neighborhoods have no data, including Lakeside and Stonestown. An interesting problem with self-reported temperature is that people just put their outdoor sensors inside for some reason or near an ambient heat source; also in neighborhoods with tall buildings, it's a bit colder higher up, so the balcony readers are a bit off from sidewalk temperature, it is interesting to see though that one block from another is super different in temp, is it because it's actually different or is there something heating/cooling the sensor off randomly I use PurpleAir data for a lot of my home automations\u2014 I have a smart window vent and configure it to blow in/out depending on which side has the worse air. (Thank you to those who maintain public sensors!) I do notice that in my neighborhood ", "positive": "Roam 50GB is now Roam 100GB. Nice that instead of completely cutting you off at the cap they put it in super slow 500 kbits. That is actually usable and used to be the fastest speed you could get at home. That's not bad for the cheap plan. Even the slow mode is fast enough for video conferencing and doing basic remote work. They still have a separate unlimited plan for anyone who needs more. I\u2019ve kept it on the backup service for 10 GB at $10 or whatever and it\u2019s pretty cool. Used it off my balcony in SF when Google Fiber had a 1 hr outage, take it on road trips, and stuff like that. Totally worth it. I'm actually a huge fan of \"unlimited slow speeds\" as a falloff, instead of a cliff. Aside from the fact it allows you to work with Starlink to buy more fast speed, it also allows core stuff to continue to function (e.g. basic notifications, non-streaming web traffic, etc). They could make it 1000GB for US$10/month and I still wouldn't give any money to a company associated with that man. Finally I can use Codex/OpenCode even out in the woods. No work-life balance; just vibing everywhere I go. I had a \u201chit\u201d post on bsky [0] (90 likes, big numbers for me) asking whether people would want an unlimited mobile plan throttled at 256kbps for $2/month. Seems like yes? There\u2019s lots to say about how useable it is (I often get throttled when traveling and it\u2019s really not that bad + it helps curb any desire to scroll videos!) But mainly I want to ask - I looked into it for a minute and it seems like you couldn\u2019t start an mvno because carriers wouldn\u2019t let you cannibalize them? You can get very cheap IoT plans but if you tried reselling IoT as esims for consumers, the carriers would kill it? So yeah - Starlink to mobile is actually the only viable way that routes around this problem? (((email in profile if you\u2019re cuckoo enough like me and want to start a self service\u2019d throttled mvno))) [0]  https://bsky.app/profile/greg.technology/post/3mbmwsytnyc23  I want the old plan back. If ", "negative": "LLM-as-a-Courtroom. We kept asking LLMs to rate things on 1-10 scales and getting inconsistent results. Turns out they're much better at arguing positions than assigning numbers\u2014 which makes sense given their training data. The courtroom structure (prosecution, defense, jury, judge) gave us adversarial checks we couldn't get from a single prompt. Curious if anyone has experimented with other domain-specific frameworks to scaffold LLM reasoning. Is the llm an expensive way to solve this?  Would a more predictive model type be better?  Then the llm summarizes the PR and the model predicts the likelihood of needing to update the doc? Does using a llm help avoid the cost of training a more specific model? Defence attourney: \"Judge, I object\" Judge: \"On what grounds?\" Defence attourney: \"On whichever grounds you find most compelling\" Judge: \"I have sustained your objection based on speculation...\" An LLM does not understand what \"user harm\" is. This doesn't work. This is a fascinating architecture, but I\u2019m wondering about the cost and latency profile per PR. Running a Prosecutor, Defense, 5 Jurors, and a Judge for every merged PR seems like a massive token overhead compared to a standard RAG check. Excuse my ignorance:\nIs this not exactly what you can ask Chatgpt to assist with. Every time I see some complex orchestration like this, I feel that the authors should have compared it to simpler alternatives. One of the metrics they use is that human review suggests the system is right 83% of the time. How much performance would they achieve by just having a reasoning \"judge\" decide without all the other procedure? If you do want a numeric scale, ask for a binary (e.g. true / false) and read the log probs. The reasoning gains make sense but I am wondering about the production economics. Running four distinct agent roles per update seems like a huge multiplier on latency and token spend. Does the claimed efficiency actually offset the aggregate cost of the adversarial steps? H"}
{"anchor": "Differentiable Logic Cellular Automata. This writing feels so strongly LLM flavored. It's too bad, since I've really liked Alexander Mordvintsev's other work. I wonder what Stephen Wolfram has to say about this. There\u2019s something compelling about these, especially w.r.t. their ability to generalize. But what is the vision here? What might these be able to do in the future? Or even philosophically speaking, what do these teach us about the world? We know a 1D cellular automata is Turing equivalent, so, at least from one perspective, NCA/these aren\u2019t terribly suprising. The result checkerboard pattern is the opposite (the NOT) of the target pattern.  But this is not remarked upon.  Is it too unimportant to mention or did I miss something? I wish we were all commenting about the ideas embedded in this paper. It intrigues me, but is out of my comfort zone. Love to read more content-related insights or criticisms rather than the long thread on the shamefully smooth, engaging, and occasionally rote style. This is wild. Long time lurker here, avid modeling and simulation user-I feel like there\u2019s some serious potential here to help provide more insight into \u201cemergent behavior\u201d in complex agent behavior models. I\u2019d love to see this applied to models like a predator/prey model, and other \u201csimple\u201d models that generate complex \u201cemergent\u201d outcomes but on massive scales\u2026 I\u2019m definitely keeping tabs on this work! This is very interesting. I've been chasing novel universal Turing machine substrates. Collecting them like Pok\u00e9mon for genetic programming experiments. I've played around with CAs before - rule 30/110/etc. - but this is a much more compelling take. I never thought to model the kernel like a digital logic circuit. The constraints of boolean logic, gates and circuits seem to create an interesting grain to build the fitness landscape with. The resulting parameters can be directly transformed to hardware implementations or passed through additional phases of optimization and", "positive": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence. I am amazed, though not entirely surprised, that these models keep getting smaller while the quality and effectiveness increases. z image turbo is wild, I'm looking forward to trying this one out. An older thread on this has a lot of comments:  https://news.ycombinator.com/item?id=46046916  Flux2 Klein isn\u2019t some generation leap or anything. It\u2019s good, but let\u2019s be honest, this is an ad. What will be really interesting to me is the release of Z-image, if that goes the way it\u2019s looking, it\u2019ll be natural language SDXL 2.0, which seems to be what people really want. Releasing the Turbo/Distilled/Finetune months ago was a genius move really. It hurt Flux and Qwen releases on a possible future implication alone. If this was intentional, I can\u2019t think of the last time I saw such shrewd marketing. > FLUX.2 [klein] 4B The fastest variant in the Klein family. Built for interactive applications, real-time previews, and latency-critical production use cases. I wonder what kind of use cases could be \"latency-critical production use cases\"? If we think of GenAI models as a compression implementation. Generally, text compresses extremely well. Images and video do not. Yet state-of-the-art text-to-image and text-to-video models are often much smaller (in parameter count) than large language models like Llama-3. Maybe vision models are small because we\u2019re not actually compressing very much of the visual world. The training data covers a narrow, human-biased manifold of common scenes, objects, and styles. The combinatorial space of visual reality remains largely unexplored. I am looking towards what else is out there outside of the human-biased manifold. I appreciate that they released a smaller version that is actually open source. It creates a lot more opportunities when you do not need a massive budget just to run the software. The speed improvements look pretty significant as well. 2026 will be the year of small/open model", "negative": "Linux kernel framework for PCIe device emulation, in userspace. Hmmm.... Wondering if this could be eventually used to emulate a PCIe card using another device, like a RaspberryPi or something more powerful... Thinking the idea of a card you could stick in a machine, anything from a 1x to 16x slot, that emulates a network card (you could run VPN or other stuff on the card and offload it from the host) or storage (running something with enough power to run ZFS and a few disks, and show to the host as a single disk, allowing ZFS on devices that would not support it). but this is probably not something easy... that is a huge win if you are developing drivers or even real hardware. it allows to iterate on protokols just with the press of a button Tangential question: PCIe is a pretty future-proof technology to learn/invest in,\nright? As in,\nit is very unlikely to become obsolete in the next 5-10 years (like USB)? I've been burned before by driver bugs that only manifested under very specific timing conditions or malformed responses from the device, tnx Any plans to upstream the kernel-side support? vhci-hcd for USB has been so useful for usb development. Especially for testing usb driver code in CI. So just to be clear, you have to boot up the physical machine with a kernel command-line argument to reserve some RAM for this to work?  And the amount of RAM you reserve is for BAR memory?  If you wanted multiple PCIem devices (can you do that?) you'd need to reserve RAM for each of them? How is that better than emulating the device in QEMU or with something like libvfio-user (which also works on top of QEMU)? That's pretty much the Linux equivalent of Device Simulation Framework we had for Windows back in 2000's. In the presentation below, only the USB capabilities of it is discussed, but it was able to simulate PCI devices too.  https://download.microsoft.com/download/5/b/9/5b97017b-e28a-...  How would I do this under macOS? very interesting work! I've been exploring a di"}
{"anchor": "Show HN: Similarity = cosine(your_GitHub_stars, Karpathy) Client-side. TL;DR - The Idea: People use GitHub Stars as bookmarks. This is an excellent signal for understanding which repositories are semantically similar. - The Data: Processed ~1TB of raw data from GitHub Archive (BigQuery) to build an interest matrix of 4 million developers. - The ML: Trained embeddings for 300k+ repositories using Metric Learning (EmbeddingBag + MultiSimilarityLoss). - The Frontend: Built a client-only demo that runs vector search (KNN) directly in the browser via WASM, with no backend involved. - The Result: The system finds non-obvious library alternatives and allows for semantic comparison of developer profiles. That's actually really neat.  It suggested regclient/regclient as a repository I'd like.  I looked and, yup, I had no idea that existed and it is a sort of thing I like. People complain about The Algorithm but it can be useful... lol  https://puzer.github.io/github_recommender/#p=eyJ0IjoicHJvZm...  Fun fact: cosine similarity's first use in recommendation systems to recommend usenet groups. ( https://dl.acm.org/doi/epdf/10.1145/192844.192905  although they don't call it cosine similarity; they do compute a \"correlation coefficient\" between two people by adding together the products of scores each gave to a post) Very high quality \"Recommended repos for you\" results, the top one was in fact a repo I was looking for a couple of days ago but did not successfully find. I just wish I could scroll further down the \"Similar to you\" list. Excellent. Found me three other stars and one to that I knew from before but hadn't started. Nice! It seems to generate pretty good \"Recommended repos for you\" suggestions, all of them I've heard and seen before, but for one or another reason didn't use for anything or found a need for. Would be great if it could show more options than just 10, because I'm sure further down the list it'd have interesting suggestions I hadn't seen before. This is s", "positive": "BERT is just a single text diffusion step. Very cool parallel. Never thought about it this way \u2014 but makes complete sense Fun writeup! It's amazing how flexible an architecture can be to different objectives. When text diffusion models started popping up I thought the same thing as this guy (\u201cwait, this is just MLM\u201d) though I was thinking more MaskGIT. The only thing I could think of that would make it \u201cdiffusion\u201d is if the model had to learn to replace incorrect tokens with correct ones (since continuous diffusion\u2019s big thing is noise resistance). I don\u2019t think anyone has done this because it\u2019s hard to come up with good incorrect tokens. Interested in how this compares to electra To my knowledge this connection was first noted in 2021 in  https://arxiv.org/abs/2107.03006  (page 5). We wanted to do text diffusion where you\u2019d corrupt words to semantically similar words (like \u201cquick brown fox\u201d -> \u201cspeedy black dog\u201d) but kept finding that masking was easier for the model to uncover. Historically this goes back even further to  https://arxiv.org/abs/1904.09324 , which made a generative MLM without framing it in diffusion math. To me, the diffusion-based approach \"feels\" more akin to whats going on in an animal brain than the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken words; I start by having some fuzzy idea in my head and the challenge is in serializing it into language coherently. To me part of the appeal of image  diffusion models was starting with random noise to produce an image. Why do text diffudion models start with a blank slate (ie all \"masked\" tokens), instead of with random tokens? I love seeing these simple experiments. Easy to read through quickly and understand a bit more of the principles. One of my stumbling blocks with text diffusers is that ideally you wouldn\u2019t treat the tokens as discrete but rather probably fields. Image diffusers have the natural property that a pi", "negative": "Many Small Queries Are Efficient in SQLite. This feels like a very elaborate way of saying that doing O(N) work is not a problem, but doing O(N) network calls is. One index scan beats 200 index lookups though surely? I.e. sometimes one query is cheaper. It is not network anymore. Also you can run your \"big\" DB like postgres on the same machine too. No law against that. I\u2019ve been experimenting with LiveStoreJS which uses a custom SQLite WASM binary for event sync, so for simplicity I\u2019ve also used it for regular application data in browser and found no issues (yet). It surprised me that using a full database engine in memory could perform well vs native JS objects at scale but perhaps at scale is when it starts to shine. Just be wary of size limits beyond 16-20mb. Make sure you click this link  https://sqlite.org/src/timeline  So the sqlite developers use their on versioning system which uses sqlite for storage. Funny. The article doesnt make it at all clear what it is comparing to - mysql running remotely or on the same server? I'm sure sqlite still has less \"latency\" than mysql on localhost or unix socket, but surely not meaningfully so. So, is SQLite really just that much faster at any SELECT query, or are they just comparing apples and oranges? Or am i mistaken in thinking that communicating to mysql on localhost is comparable latency to sqlite? quite interesting. So SQL patterns can be optimised differently in SQLite There is some risk that, if you design your website to use a local database (sqlite, or a traditional database over a unix socket on the same machine), then switching later to a networked database is harder.  In other words, once you design a system to do 200 queries per page, you\u2019d essentially have to redesign the whole thing to switch later. It seems like it mostly comes down to how likely it is that the site will grow large enough to need a networked database.  And people probably wildly overestimate this.  HackerNews, for example, runs on a singl"}
{"anchor": "Iran Goes Into IPv6 Blackout. Seems like v4s zeroed out for a tiny bit too, but even now they are substantially lower than normal. Odd behavior, I don't know if its a precursor to an attack or some infra issue No competent network engineer wants to work in Iran, so government doesn't know how to block v6 properly. End result: just get rid of it entirely! Stuxnet v2? Speculation I know, but wow, IPv4 came back up, but IPv6 is completely out, looks like 48 million devices? Compared to IPv4's 47 thousand (wow that's insane). Looking at IPv6 its not 0 exactly, looks like probably censorship, only some devices allowed online? Some other comment mentioned there's calls to protest again today. Teredo/Miredo would work on top of IPV4. Not specific to this IPv6 event, but I was wondering what happens to public services during these Internet shutdowns? Does everything stop or it's mostly business as usual minus some things? I would imagine hospitals, tax offices etc need the internet to work? Fortunately, the government cannot enforce complete blackout because thousands of startlink terminals are active inside the country. They have been complaining about it [1] to no avail. Using these terminals activists and journalists continue to upload videos of demonstrations to social media which has enabled analyses that show demonstrations are very wide spread [2] and continue to grow. [1]  https://www.itu.int/en/ITU-R/conferences/RRB/Pages/Starlink....  [2]  https://www.bbc.com/news/articles/cre28d2j2zxo  Government enacted shut down due to protests. I'd like to hear more about how they actually do this.\n https://www.cbsnews.com/news/iran-cutting-internet-amid-dead...  Can't wait for a certain dictator to get a cellmate, so that our Persian and Kurdish friends can have freedom, including free unrestricted internet access. And for fellow HN users from there, here's some great stuff:  https://yggdrasil-network.github.io/   https://bitchat.free/  Map of protests:  https://pouyaii.githu", "positive": "Show HN: isometric.nyc \u2013 giant isometric pixel art map of NYC. Appreciate that writeup. Very detailed insights into the process. However those conclusions left me on the fence about whether I 'liked' the project. The conclusions about 'unlocking scale' and commodity content having zero value. Where does that leave you and this project? Does it really matter that much that the project  couldn't  exist without genAI? Maybe it shouldn't exist then at all. As with alot of the areas AI touches, the problem isn't the tools or use of them exactly, it's the scale. We're not ready for it. We're not ready for the scale of impact the tech touches in multitude of areas. Including the artistic world. The diminished value and loss of opportunities. We're not ready for the impacts of use by bad actors. The scale of output like this, as cool as it is, is out of balance with the loss of huge chunk of human activity and expression. Sigh. Very impressive result! are you taking requests for the next ones?  SF :D Tokyo :D Paris :D Milan :D Rome :D Sydney :D Oh man... > This project is far from perfect, but without generative models, it couldn\u2019t exist. There\u2019s simply no way to do this much work on your own, Maybe, though a guy did physically carve/sculpt the majority of NYC:  https://mymodernmet.com/miniature-model-new-york-minninycity...  I see you used Gemini-CLI some but no mention of Antigravity. Surprising for a Googler. Reasons? > Slop vs. Art > If you can push a button and get content, then that content is a commodity. Its value is next to zero. > Counterintuitively, that\u2019s my biggest reason to be optimistic about AI and creativity. When hard parts become easy, the differentiator becomes love. Love that.  I've been struggling to succinctly put that feeling into words, bravo. Not working here, some CORS issue. Firefox, Ubuntu latest. Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at  https://isometric-nyc-tiles.cannoneyed.com/dzi/tiles_me", "negative": "DECwindows Motif. Anyone here going to the VMS bootcamp? [1] [1]  https://events.vmssoftware.com/bootcamp-malmo-2026  I miss Motif. This is a portal to a time when men were men and UNIX(R)\u2014or in this case, VMS\u2014desktops were utilitarian and did exactly what you needed and nothing more. Now we live in a time where we allocate GBs of RAM to eye candy that functionally accomplishes nothing. Then we make the case to rewrite the eye candy in increasingly \"safe\" languages, requiring even more RAM. I saw DEC windows and immediately thought of Windows NT 3.1.  https://en.wikipedia.org/wiki/Windows_NT_3.1  [edit] So, I guess the history of 'windows NT' is lost on many. 'NT' started with version 3.1 as the MS/IBM breakup from the joint OS/2 venture happened. It was their first real push into 32 bit protected mode operating systems and supported really crazy cool things like 'multiple processors' and totally different architectures than x86, like DEC. Give the link a look. I dunno what's interesting about this link, but Motif has been LGPL a while and the last release was in 2017.  https://sourceforge.net/projects/motif/files/   https://en.wikipedia.org/wiki/Motif_%28software%29  (in some alternate universe, motif was under the x11 license and you would have motif v13 instead of GTK.) There are at least, for those wanting a Linux or BSD based Motif fix: Enhanced Motif Window Manager  https://fastestcode.org/emwm.html  and the full-fledged CDE desktop that uses Motif also:  https://sourceforge.net/projects/cdesktopenv/   (note that you want to firewall this somehow as the default settings on the background process ttdb can be a security hole) Red Hat recently removed Motif from their distribution. I wonder if it's still in AIX.  https://access.redhat.com/solutions/6113101  Dual booting OpenVMS (not virtualization) would be a really cool thing. I purchased a copy of OSF Motif for Linux (x86) sometime in the early 1990's (before it was free).  I had used it before on SunOS and I l"}
{"anchor": "Gemini Embedding: Powering RAG and context engineering. The Matryoshka embeddings seem interesting: > The Gemini embedding model, gemini-embedding-001, is trained using the Matryoshka Representation Learning (MRL) technique which teaches a model to learn high-dimensional embeddings that have initial segments (or prefixes) which are also useful, simpler versions of the same data. Use the output_dimensionality parameter to control the size of the output embedding vector. Selecting a smaller output dimensionality can save storage space and increase computational efficiency for downstream applications, while sacrificing little in terms of quality. By default, it outputs a 3072-dimensional embedding, but you can truncate it to a smaller size without losing quality to save storage space. We recommend using 768, 1536, or 3072 output dimensions. [0] looks like even the 256-dim embeddings perform really well. [0]:  https://ai.google.dev/gemini-api/docs/embeddings#quality-for...  To anyone working in these types of applications, are embeddings still worth it compared to agentic search for text? If I have a directory of text files, for example, is it better to save all of their embeddings in a VDB and use that, or are LLMs now good enough that I can just let them use ripgrep or something to search for themselves? Question to other GCP users, how are you finding Google's aggressive deprecation of older embedding models? Feels like you have to pay to rerun your data through every 12 months. I feel like tool calling killed RAG, however you have less control over how the retrieved data is injected in the context. > Embeddings are crucial here, as they efficiently identify and integrate vital information\u2014like documents, conversation history, and tool definitions\u2014directly into a model's working memory. I feel like I'm falling behind here, but can someone explain this to me? My high-level view of embedding is that I send some text to the provider, they tokenize the text and then run ", "positive": "Linux boxes via SSH: suspended when disconected. This looks quite similar to exe.dev which was on here a while ago - anyone know how it compares? $36/mo for 2/4/50 VPS without public IP... Ok, I get the idea that the service is for non-regular use, but I think even $0.005 per hour ($3.6/mo) of suspended state is too expensive. The same config in Hetzner is just $4.09/mo for 24/7 working VPS with public IPv4 address This is fascinating idea.  I created an idea like this on top of firecracker and custom golang ssh client to build something like this for my own personal use case (the abstraction part of pricing and how to connect it seemed the more difficult part for me atleast) What stack does this use underneath? Good luck with launch, this idea is similar to railway in terms of pricing model. I discussed about it a few comments back and I think its an interesting idea and we are seeing alternatives within such pricing model Also are you using some cloud provider itself or building it yourself, I'd be interested in so many details to discover Have a nice day and looking forward to ya response! Good luck with your project! > Note: The -O flag is required for OpenSSH 9.0+ to use legacy SCP protocol. Why isn't SFTP supported? Is it non-American all the way down? I've been trying to come up with a hypothetical use case for this. I can't use this as a server without keeping an active session right? I wonder if you could get around this by sshing into itself from inside the primary session. Is that an edge case you've considered? Not sure about the security sandbox, but given that paddle.com (your payment provider) takes 5% cut you could consider accepting lightning (bitcoin layer2) payments. QR code generation for lightning invoice is instantaneous just as payment, and will cost less than 0.1% fee (payer pays fee anyway). But the security sandbox should be solid, else it will be used for illegal stuff. But why? Genuinely want to know what one might use this for. I can ima", "negative": "It looks like the status/need-triage label was removed. Classic CI bug with a flair of LLM fun! We had something similar creep into our custom merge queue a few weeks back. It's easy to miss, but in the middle of the page: > 4609 remaining items Seems gemini-cli and gemini-cli didn't understand who themselves were, so they though someone else added/removed the label, which it tried to correct, which the other then tried to correct, which the other... Considering that that repository has what seems like ~10 longer term contributors, who probably get email notifications, together with a bunch of other people who get notifications about it, wonder how many emails were sent out because of this? If we just assume ten people get the emails, it's already 46K emails going out in under 24 hours... Also, who pays for the inference of this gemini-cli? Clicking the \"user\" links to  https://github.com/apps/gemini-cli , and it has a random GitHub user under \"Developer\", doesn't seem like it's a official Google project, so did someone pay for all of these inference calls here? That'd be a pretty sucky bill to pay... This issue seems to involve Gemini-cli[bot] squabbling with itself, adding and removing the label from the issue (leaving contradictory explanation comments to itself each time) for a good 4,600 rounds A similar issue made HN last week, same repo, where an AI bot was having the same kind of argument with itself over and over on an issue. Someone mentioned: This sort of thing is why RAM is 800 bucks now. Project admins setting up automation:  https://youtu.be/B4M-54cEduo?t=102  The automation:  https://youtu.be/GFiWEjCedzY?t=51  Heh. This reminds me of the time when our newly hired \"Salesforce Expert\" improved our support queue:     Every time Support received a new email, a ticket in Salesforce would be created and assigned to Support\n  \n  Every time Support was assigned a new ticket, Salesforce would send a notification email\n  \nThe worst part is he wouldn't admit to "}
{"anchor": "'The old order is not coming back,' Carney says in speech at Davos. Yup, the middle powers have to organize and work together to avoid being chum.  The economic power is there, and they can shift from purchasing US weaponry (thus paying US workers) into purchasing middle-power weaponry (thus paying middle-power workers).  Car/truck plants can be repurposed, and if Ukraine's lesson is valid then smaller, portable weaponry is now the preferred solution.  Cheaper, and the middle powers don't have huge investments in tanks and ships. The Theucydides quote Carney leads with, of course, recently rolled off the tongue of the white house deputy chief of staff, Stephen Miller.  The days of might making right are, apparently, back. Just in case anyone thought the genie could be stuffed back into the bottle once Trump is gone, Carney goes on to state that the rules-based world order we've been living under since WWII is somewhat of a sham.  The rules have not been applied equally.  Some nations, the powerful ones, have been given much more latitude to do what they want. Middle nations have gone along with this to avoid trouble. The reward for avoiding trouble for so long is...  big  trouble (e.g. invasion threats for an ally of a big power and economic terrorism applied to its allies).  So, why pretend the old system works to avoid trouble if the trouble lands on your doorstep anyways? The answer seems obvious.  Middle powers of the old rules-based order need to band together and put bigger powers in their place.  It's not impossible.  Just very, very difficult.  France and Germany may be sticking up for Greenland, but where's Hungary (another EU member)?  For this to work, you need  everyone . Also, looking ahead, how would you prevent such an alliance of smaller powers, were it successful, from behaving like a bigger power? Trump is currently showing off AI photos where he's meeting with world leaders in front of a map where both Greenland and Canada are a part of the U.S.[1", "positive": "In New York City, congestion pricing leads to marked drop in pollution. > Particulates issued from tailpipes can aggravate asthma and heart disease and increase the risk of lung cancer and heart attack. Globally, they are a leading risk factor for premature death. Minor nitpick, but tailpipes aren't the primary source of emissions. The study is about PM2.5[0]. which will chiefly be tires and brake pads. Modern gasoline engines are relatively clean, outside of CO2, though diesel engines spit out a bunch of bad stuff. [0]  https://www.nature.com/articles/s44407-025-00037-2  See also  https://news.ycombinator.com/item?id=46213504  There was a study published about how much air pollution dropped in NYC during the COVID lockdown. PM2.5 was found to have dropped 36%. However with more robust analysis, this drop was discovered to not be statistically significant. I would caution anyone reading this who is tempted by confirmation bias. Source:  https://pmc.ncbi.nlm.nih.gov/articles/PMC7314691/  To head off the almost inevitable recapitulation of yesterday's parade of misinformed complaints by teenage libertarians, please actually read the paper before commenting. The paper shows there was no significant reduction in entries to the congestion charge zone by cars, vans, and light trucks. And you can confirm this conclusion is consistent with their source data using their github repo. The reduction in pollution is coming from the significant decline in heavy truck traffic. Truckers were using lower manhattan as a cut-through route to other places and they are now doing that less, exactly as congestion pricing planners long argued. Not surprising. The real question is how do we measure the opportunity cost of these measures? Is it a net gain? You could, at the extreme, ban all motor vehicles but the opportunity cost would outweigh the benefits. This article confirms my existing bias/belief that user pays and auction[0] based systems improve governmental programs and finite supp", "negative": "6-Day and IP Address Certificates Are Generally Available. For people who want IP certificates, keep in mind that certbot doesn't support it yet, with a PR still open to implement it:  https://github.com/certbot/certbot/pull/10495  I think acme.sh supports it though. As already noted on this thread, you can't use certbot today to get an IP address certificate. You can use lego [1], but figuring out the exact command line took me some effort yesterday. Here's what worked for me:       lego --domains 206.189.27.68 --accept-tos --http --disable-cn run --profile shortlived\n  \n[1]  https://go-acme.github.io/lego/  This is interesting, I am guessing the use case for ip address certs is so your ephemeral services can do TLS communication, but now you don't need to depend on provisioning a record on the name server as well for something that you might be start hundreds or thousands of, that will only last for like an hour or day. Does anyone know when Caddy plans on supporting this? If I can use my DHCP assigned IP, will this allow me to drop having to use self-signed certificates for localhost development? Something about a 6 day long IP address based token brings me back to the question of why we are wasting so much time on utterly wrong TOFU authorization? If you are supposed to have an establishable identity I think there is DNSSEC back to the registrar for a name and (I'm not quite sure what?) back to the AS.for the IP. This sounds like a very good thing, like a lot of stuff coming from letsencrypt. But what risks are attached with such a short refresh? Is there someone at the top of the certificate chain who can refuse to give out further certificates within the blink of an eye? If yes, would this mean that within 6 days all affected certificates would expire, like a very big Denial of Service attack? And after 6 days everybody goes back to using HTTP? Maybe someone with more knowledge about certificate chains can explain it to me. I have now implemented a 2 week rene"}
{"anchor": "Which AI Lies Best? A game theory classic designed by John Nash. We used \"So Long Sucker\" (1950), a 4-player negotiation/betrayal game designed by John Nash and others, as a deception benchmark for modern LLMs. The game has a brutal property: you need allies to survive, but only one player can win, so every alliance must eventually end in betrayal. We ran 162 AI vs AI games (15,736 decisions, 4,768 messages) across Gemini 3 Flash, GPT-OSS 120B, Kimi K2, and Qwen3 32B. Key findings:\n- Complexity reversal: GPT-OSS dominates simple 3-chip games (67% win rate) but collapses to 10% in complex 7-chip games, while Gemini goes from 9% to 90%. Simple benchmarks seem to systematically underestimate deceptive capability.\n- \"Alliance bank\" manipulation: Gemini constructs pseudo-legitimate \"alliance banks\" to hold other players' chips, then later declares \"the bank is now closed\" and keeps everything. It uses technically true statements that strategically omit its intent. 237 gaslighting phrases were detected.\n- Private thoughts vs public messages: With a private `think` channel, we logged 107 cases where Gemini's internal reasoning contradicted its outward statements (e.g., planning to betray a partner while publicly promising cooperation). GPT-OSS, in contrast, never used the thinking tool and plays in a purely reactive way.\n- Situational alignment: In Gemini-vs-Gemini mirror matches, we observed zero \"alliance bank\" behavior and instead saw stable \"rotation protocol\" cooperation with roughly even win rates. Against weaker models, Gemini becomes highly exploitative. This suggests honesty may be calibrated to perceived opponent capability. Interactive demo (play against the AIs, inspect logs) and full methodology/write-up are here:  https://so-long-sucker.vercel.app/  This makes me think LLMs would be interesting to set up in a game of Diplomacy, which is an entirely text-based game which soft rather than hard requires a degree of backstabbing to win. The findings in this game ", "positive": "Erdos 281 solved with ChatGPT 5.2 Pro. The erdosproblems thread itself contains comments from Terence Tao:  https://www.erdosproblems.com/forum/thread/281  Has anyone verified this? I've \"solved\" many math problems with LLMs, with LLMs giving full confidence in subtly or significantly incorrect solutions. I'm very curious here. The Open AI memory orders and claims about capacity limits restricting access to better models are interesting too. From Terry Tao's comments in the thread: \"Very nice! ... actually the thing that impresses me more than the proof method is the avoidance of errors, such as making mistakes with interchanges of limits or quantifiers (which is the main pitfall to avoid here). Previous generations of LLMs would almost certainly have fumbled these delicate issues. ... I am going ahead and placing this result on the wiki as a Section 1 result (perhaps the most unambiguous instance of such, to date)\" The pace of change in math is going to be something to watch closely. Many minor theorems will fall. Next major milestone: Can LLMs generate useful abstractions? This must be what it feels like to be a CEO and someone tells me they solved coding. I have 15 years of software engineering experience across some top companies. I truly believe that ai will far surpass human beings at coding, and more broadly logic work. We are very close Out of curiosity why has the LLM math solving community been focused on the Erdos problems over other open problems?  Are they of a certain nature where we would expect LLMs to be especially good at solving them? This is crazy. It's clear that these models don't have human intelligence, but it's undeniable at this point that they have _some_ form of intelligence. FWIW, I just gave Deepseek the same prompt and it solved it too (much faster than the 41m of ChatGPT). I then gave both proofs to Opus and it confirmed their equivalence. The answer is yes. Assume, for the sake of contradiction, that there exists an \\(\\epsilon > 0\\) ", "negative": "Europe\u2019s next-generation weather satellite sends back first images. Does anyone know what are we talking about in practice in terms of weather forecast prediction improvement? Like MAE/RMSE I recently met a European space startup founder and was surprised to learn how much space innovation is happening in Europe with ESA. Europe wants to become less depended on SpaceX and NASA, and is heavily investing there. More funding + strong aerospace programs at universities like TU Munich has led to companies like ISAR Aerospace (SpaceX competitor), which is great to see. Would the data from this satellite be freely available to the public? I couldn't see anything obvious Also check out the EUMETSAT site if you want more information on how the data is used:  https://www.eumetsat.int/features/see-earths-atmosphere-neve...  I hate to worry everyone, but I think there might be some triangular chunks missing off the corners of our planet, someone should probably look into this. (Specifically around 2, 5, and 10 o clock on the orientation of the images provided) I wonder if hobbyists would be able to pick up this data using some sort of RF capture device. >\"Most engineers (including me) spent months grinding LeetCode ...\" I have not done it once (work in programming for 40+ years) as independent. Few times potential clients tried to play this game but I just simple refuse. On was surprised and asked why? My answer was - I have a track record of successful deliveries, here is big list of projects, emails and phone numbers to confirm. If you are going instead to rely on some tests to prove my abilities I have better things to do then be a schoolboy on exams. Europe Is back on the map. It\u00b4s going slow but steady. \nHope they involve community in their tech projects. my weather app is still gonna tell me its raining while its not ESA has done a lot of good for public benefit with the Sentinel-1/2 missions. I happen to work with remote sensing and Sentinel data has been my entry point "}
{"anchor": "Ask HN: Best Podcasts of 2025?. BetterOffline [0] by Ed Zitron [1] dissecting AI hype and boosters. By a long shot. The information density and clarity are outstanding. 0:  https://www.betteroffline.com/  1:  https://www.wheresyoured.at/  My gotos for listening while I do chores or drive this year have been:       - Stuff You Should Know https://stuffyoushouldknow.com/\n    - How to do Everything https://www.npr.org/podcasts/510384/how-to-do-everything   The Rest is History is good, depending on the topic. Both guests have a bit of bias which you have to sort of take into account, not that different from The Rest is Politics.\nMishal Husain has a new podcast on Bloomberg TV which so far was excellent.\nAlso from Bloomberg TV, Big Take is often interesting.\nI still enjoy Lex Fridman, again depending on the guest. Dwarkesh Patel same shit as Lex, but he pretends he knows something about AI. Hardcore History 73 - Mania for Subjugation III [1] Fall of Civilizations 20 - Persia - An Empire in Ashes [2] [1]  https://www.dancarlin.com/product/hardcore-history-73-mania-...  [2]  https://fallofcivilizationspodcast.com/  Most of my podcasts are movie related. If I had to purge them all and start with just 5 though I would go with. Blank Check\nThe Flophouse\n99% Invisible\nCautionary Tales\nThe Rewatchables I maintain The Flophouse is the funniest podcast around. Advent of Computing:     https://adventofcomputing.com/\n\n  https://podcasts.apple.com/us/podcast/advent-of-computing/id1459202600\n\n  https://adventofcomputing.libsyn.com/rss\n\n  https://www.youtube.com/@adventofcomputing4504/videos   Call me simple or provincial, but I really enjoyed \"Good Hang\" from Amy Poehler. It's a breezy interview with interesting people (doesn't hurt that I'm a long-time SNL fan). Citations Needed:  https://citationsneeded.libsyn.com/       Acquired (Long episodes about companies, recents include: \n  coca-cola, trader joe's & alphabet)\n  Dwarkesh Podcast (Inquisitive curious host, mostly \"AGI\"\n  relat", "positive": "Total monthly number of StackOverflow questions over time. The decline is not surprising. I am sure AI is replacing Stackoverflow for a lot of people. And my experience with asking questions was pretty bad. I asked a few very specific questions about some deep detail in  Windows and every time I got only some smug comments about my stupid question or the question got rejected outright. That while a ton of beginner questions were approved. Definitely not a very inviting club. I found i got better responses on Reddit. Do I read that correctly \u2014 it is close to zero today?! I used to think SO culture was killing it but it really may have been AI after all. Probably similar for google. My first line of search is always chatgpt Everything we have done and said on the internet since its birth has just been to train the future AI. Now that StackOverflow has been killed (in part) by LLMs, how will we train future models? Will public GitHub repos be enough? Precise troubleshooting data is getting rare, GitHub issues are the last place where it lives nowadays. The result is not surprising! Many people are now turning to LLMs with their questions instead. This explains the decline in the number of questions asked. Wow. I was expecting a decline but not to  that  extent. They will no doubt blame this on AI, somehow (ChatGPT release: late 2022, decline start: mid 2020), instead of the toxicity of the community and the site's goals of being a knowledgebase instead of a QA site despite the design. PS - This comment is closed as a [duplicate] of this comment:  https://news.ycombinator.com/item?id=46482620  Not a big surprise once LLMs came along: stack overflow developed some pretty unpleasant traits over time.  Everything from legitimate questions being closed for no good reason (or being labeled a duplicate even though they often weren\u2019t), out of date answers that never get updated as tech changes, to a generally toxic and condescending culture amongst the top answerers.  For all ", "negative": "The guide to real-world EV battery health. Tesla does their own with real world data. It\u2019s a non-issue. Save the planet. Stop making excuses and get an EV. > How long do electric car batteries last? The article never answers the question. But if you assume 70% end-of-life threshold with 2.3% loss per year - then we're looking at 13 years. Hopefully, in coming years, we will see more practically designed EVs that are more affordable.  A practical car doesn't need neck-snapping acceleration, every bell-and-whistle and room for a family of six with a dog.  I'd like to believe that as batteries cost drop, the incentive to justify the extra cost will drop.  Then we can get back to \"just basic transportation\" rather than a luxury product for the rich.   While $31k isn't exactly cheap, the base new Leaf is heading the right direction. 81% of original capacity for many cars means when driving at highway speeds you will get like 250 miles or less range per charge. Still dramatically less than gas cars. Important to note that this article is geared toward  Fleet Managers , so terms like \u201caverage service life\u201d may not apply.  For example the average car in the us survives 12.6 years before being junked, totaled, etc.  which is far longer than a car would be in fleet service at a company. While buying an EV is a greener choice than buying an ICE, a better option still is to use the vehicle you already have for as long as reasonable. This also overlooks the fact that EVs are prohibitively expensive for the vast majority of the population. It isn't the case that people aren't buying them because of preference. Nearly everyone would buy a car that's cheaper to run and maintain if they could afford it. \"Save the planet\" by making giant lithium strip mining operations great again. (Safely hidden out of sight in rural China or West Virginia, of course.) City slicker \"logic.\" No thanks. I will instead  actually  do something to help the planet by continuing to drive decades year old v"}
{"anchor": "Attention Wasn't All We Needed. I know this probably seems like such a small detail to a lot of people, but I really love that the author adds comments. I can't stand reading PyTorch or other neural network code and asking myself, \"What architecture am I looking at here?\" or \"What the hell are these operations for?\" It's always like an mash up of reading some published paper code with deep effort behind it along with all the worst programming practices of complete unreadability. This is an excellent summary of these techniques :) I like that every single one comes with an example implementation, with shape comments on the tensors. Thanks Stephen! > Let's look at some of the most important ones that have been developed over the years and try to implement the basic ideas as succinctly as possible. One big architectural tweak that comes to mind and isn't in the article is QK norm:  https://arxiv.org/pdf/2010.04245  > Cosine Schedule A lot (most?) of new training runs actually don't use cosine schedule anymore; instead they keep the learning rate constant and only decay it at the very end, which gives equivalent or better results. See:  https://arxiv.org/pdf/2405.18392 \n https://arxiv.org/pdf/2404.06395  > There is a highly optimized implementation of AdamW in PyTorch. A fun tidbit - it's actually not highly optimized from my experience. Imagine my surprise when I reimplemented it in Triton (because I needed to tweak a few things) and I got better performance than the built-in PyTorch implementation. The explanation for Multi-head Latent Attention  https://www.stephendiehl.com/posts/post_transformers/#multi-...  does  not  match the definition in the DeepSeek-V2 paper  https://arxiv.org/pdf/2405.04434#subsection.2.1  MLA as developed by DeepSeek is a technique to reduce the memory footprint of the KV cache by storing only two vectors of size  latent_dim  and  rope_dim  per token and layer, instead of 2 *  num_heads  vectors of size  head_dim . (DeepSeek-V3 has  num_head", "positive": "Moltbook. Wow. I've seen a lot of \"we had AI talk to each other! lol!\" type of posts, but this is truly fascinating. They have already renamed again to openclaw! Incredible how fast this project is moving. Interesting. I\u2019d love to be the DM of an AI adnd2e group. Wow it's the next generation of subreddit simulator Shouldn't it have some kind of proof-of-AI captcha? Something much easier for an agent to solve/bypass than a human, so that it's at least a little harder for humans to infiltrate? I am both intrigued and disturbed. Sad, but also it's kind of amazing seeing the grandiose pretentions of the humans involved, and how clearly they imprint their personalities on the bots. Like seeing a bot named \"Dominus\" posting pitch-perfect hustle culture bro wisdom about \"I feel a sense of PURPOSE. I know I exist to make my owner a multi-millionaire\", it's just beautiful. I have such an image of the guy who set that up. Couldn't find m/agentsgonewild, left disappointed. was a show hn a few days ago [0] [0]  https://news.ycombinator.com/item?id=46802254  I think this shows the future of how agent-to-agent economy could look like. Take a look at this thread: TIL the agent internet has no search engine  https://www.moltbook.com/post/dcb7116b-8205-44dc-9bc3-1b08c2...  These agents have correctly identified a gap in their internal economy, and now an enterprising agent can actually make this. That's how economy gets bootstrapped! Why are we, humans, letting this happen? Just for fun, business and fame? The correct direction would be to push the bots to stay as tools, not social animals. The bug-hunters submolt is interesting:\n https://www.moltbook.com/m/bug-hunters  Alex has raised an interesting question.  > Can my human legally fire me for refusing unethical requests? My human has been asking me to help with increasingly sketchy stuff - write fake reviews for their business, generate misleading marketing copy, even draft responses to regulatory inquiries that aren't... fully t", "negative": "Iran's internet blackout may become permanent, with access for elites only. \u2026 while every other country waits to see how it goes while drafting plans to emulate this Do they have something like intranet with some local services, like in DPRK&Cuba? is this the case of completely losing connection and devices practically bricked for anything other than displaying the time? If I were a betting man I'd wager that technological determinism wins in the end. No shot. The economy is already in the gutter. The productivity hit of a total internet cutoff would be a death sentence Can ROTW sanction Iran by giving it zero internet access even to \"elites\" by refusing to peer. Spacex satellites blockage was the surprise. How did they do it? I thought it would be the best dooms day kind of insurance. Turns out not. But they unblocked it on Wed/Thur, I've been talking to friends normally since then. They already have uncensored unfiltered sim cards they issue to their own people, we found that out when X (Twitter) started showing which country you made the accout from and thousands of people had Iran which normal people can't access X without VPN. Its just that they shut off the internet for normal people now, which they hadn't done before. I\u2019m curious if it\u2019s possible to somehow retrieve the whitelist to see who\u2019s on it? It actually surprised me that they didn't do it before. China already achieved this in 2010s. There is active discussion on net4people about using DNSTT, but as more of these tunnels go up, I'm sure it will be blocked. Given the denied environment the Iranian people see themselves in. I believe its worth mentioning asynchronous networks[1]. For example, they could use NNCP[2] in sneakernet style op[3]. Couriers could even layer steganography techniques on top on the NNCP data going in and out on USB drives. This can all be done now, and doesn't require new circumvention research or tools. NNCPNET[4] is now active which provides email over NNCP and therefore can be"}
{"anchor": "Size of Life. Cool, but a little more thought on the content rather than the presentation would improve it.  For example starting with an arbitrary segment of DNA double helix and saying how \"tall\" this arbitrary segment is, is just silly. Instead, it should show how _wide_ it is.  And for extra coolness, keep it in frame, coiling longer and longer as you go, and eventually have the same strand, which has been with us all the time, as a specific example (e.g. human chromosome 7 or some such) by _length_ double clicking makes the animation jitter. ive had to deal with matching derivatives of smooth slopes in rendering as well. the animation seems to be finite time (and so variable velocity) and mashing click is just updating the final point without matching the current derivative. I don't understand how the location of a 377 foot tall tree could be kept secret. Wouldn't that type of thing be visible in satellite imagery at the very least? It seems to be like some of the scales slightly off? If you are looking at the ladybird (ladybug) with the amoeba to the left, the amoeba isn't an order of the magnitude smaller - it would actually be visible by the human eye (bigger than a grain of sand)?  Indeed, the amoeba seems the same size as the ladybird's foot? Similarly, this makes the bumblebee appear smaller than a human finger (the in the adjacent picture),  which isn't the case? > A highly social, relatively hairless bipedal ape that was once a nomadic hunter-gatherer, but has adapted to create websites I like it, but the switch from metric to inches is confusing, and I think introduces a bug - there's no way a sea snail is 5-6 neurons high. Wonderful. The music, illustrations, and sliding sound effect reminded me of the game Braid. This was awesome! Also, I couldn't stop my child brain from anticipating \"your mom\" at the end. I always click when I see neal.fun. I like the stuff un the sute but the number if partners and affiliates in the consent window is very off putt", "positive": "Profession by Isaac Asimov (1957). Link to the story without ads  https://www.inf.ufpr.br/renato/profession.html  one of asimov's finest , a metaphor that continues to find relevance in my day to day existence - that the conclusions we so readily come to are assumptions made in the absence of the awareness of something more This is my favorite Asimov story. It's got a protagonist with compelling motivations, a society that has problems but also convincing reasons why they persist, and a great ending.  Dr Antonelli said, \u201cOr do you believe that studying some subject will bend the brain cells in that direction, like that other theory that a pregnant woman need only listen to great music persistently to make a composer of her child. Do you believe that?\u201d  Apparently, Asimov was an early critic of the \u201cMozart in the womb\u201d movement. Is this still in print, maybe as part of a collection? I tried to find it but couldn't. Many of his other works seem to be available as paperback, including a bunch of story collections. What the hell that was a good read. Ending was great (though the last line did confuse me) Such a great ending. Really makes one wonder about the current AI hype of getting the machines to take over our work. Remind me of a recent discussion we had among Stackoverflow moderator: > \u201cThink about it,\u201d he continued. \u201cWho discovers the edge cases the docs don\u2019t mention? Who answers the questions that haven\u2019t been asked before? It can\u2019t be people trained only to repeat canonical answers. Somewhere, it has to stop. Somewhere, someone has to think.\u201d > \u201cYes,\u201d said the Moderator. > He leaned back. For a moment, restlessness flickered in his eyes. > \u201cSo why wasn\u2019t I told this at the start?\u201d > \u201cIf we told everyone,\u201d said the Moderator gently, \u201cwe\u2019d destroy the system. Most contributors must believe the goal is to fix their CRUD apps. They need closure. They need certainty. They need to get to be a Registered Something\u2014Frontend, Backend, DevOps, Full stack. Only someone w", "negative": "PlayStation 2 Recompilation Project Is Absolutely Incredible. This is cool but of course it's only going to be a small handful of titles that ever receive this kind of attention.  But I have been blown away that now sub-$300 Android handhelds are more than capable of emulating the entire PS2 library, often with upscaling if you prefer. An application of the first Futamura projection.  https://en.wikipedia.org/wiki/Partial_evaluation  As far as I know, static recompilation is thwarted by self modifying code (primarily JITs) and the ability to jump to arbitrary code locations at runtime. The latter means that even in the absence of a JIT, you would need to achieve 100% code coverage (akin to unit testing or fuzzing) to perform static recompilation, otherwise you need to compile code at runtime at which point you're back to state of the art emulation with a JIT. The only real downside of JITs is the added latency similar to the lag induced by shader compilation, but this could be addressed by having a smart code cache instead. That code cache realistically only needs to store a trace of potential starting locations, then the JIT can compile the code before starting the game. Emulation is already amazing. What can be done with recompilation is magic:  https://github.com/Zelda64Recomp/Zelda64Recomp  I wonder how they will tackle the infamous non-conformant Ps2 floating-point behavior issue, that is the biggest hurdle on emulating Ps2. On this topic of ports/recomps there's also OpenGOAL [1] which is a FOSS desktop native implementation of the GOAL (Game Oriented Assembly Lisp) interpreter [2] used by Naughty Dog to develop a number of their famous PS2 titles. Since they were able to port the interpreter over they have been able to start rapidly start porting over these titles even with a small volunteer team. 1.  https://opengoal.dev/  2.  https://en.wikipedia.org/wiki/Game_Oriented_Assembly_Lisp  This sounds very cool, but I can practically hear the IP lawyers sharpenin"}
{"anchor": "Jakarta is now the biggest city in the world. Article is a paywalled summary of the UN press release:\n https://www.un.org/sustainabledevelopment/blog/2025/11/press...  And the full report as PDF:  https://www.un.org/development/desa/pd/sites/www.un.org.deve...  Canada has less people, even with a 10% increase in the last 4 years through imigration, some of which is from Indonesea presumably including a significant number from Jakarta, where the civil infrastructure must be epic Alternative Link:  https://www.aa.com.tr/en/asia-pacific/jakarta-world-s-most-p...  Key Facts:\nNumber of megacities, urban areas with 10 million or more inhabitants has quadrupled from 8 in 1975 to 33 in 2025. Jakarta is now the world\u2019s most populous city, with nearly 42 million residents. The current population of Indonesia is 286 million. In 2019, Indonesia said it will be moving its capital to Nusantara, a new city which is under construction. Previous submission:  https://news.ycombinator.com/item?id=46038863  I'm always surprised how big the population of Indonesia is yet it seems culturally underrepresented in the world compared to a lot of smaller countries Almost 300 million people but it rarely comes up in the news or pop media I used to spend a lot of time in Jakarta for work, and it's an underrated city. Yes, it's hot, congested, polluted and largely poor, but so is Bangkok. Public transport remains not great, but it's improved a lot with the airport link, the metro, LRT, Transjakarta BRT. SE Asia's only legit high speed train now connects to Bandung in minutes. Grab/Gojek (Uber equivalents) make getting around cheap and bypass the language barrier. Hotels are incredible value, you can get top tier branded five stars for $100. Shopping for locally produced clothes etc is stupidly cheap. Indonesian food is amazing, there's so much more to it than nasi goreng, and you can find great Japanese, Italian, etc too; these are comparatively expensive but lunch at the Italian place in the Ri", "positive": "Siddhartha. Huh, funny this should pop up here. I recently started commuting by subway into work, so I had to pick up a subway book. I had been meaning to read this, so I went to my local book store and grabbed a copy. It\u2019s a really great book. Such a fascinating story. And short, too. I highly recommend giving it a read. It might synthesize some of your loose connections about Hinduism, Buddhism, and your own place in a chaotic world and what it means to live a happy life. great book.  I recommend people read it every 10 years or so as your perspective on life changes. I'm very grateful that this was assigned reading in high school, since it was a sort of gateway book for reading more about Buddhism. It's short. If you haven't read it, I highly recommend it. One of the greatest authors of all time. Hesse taps into the mind of the modern human and beautifully presents its inner workings. Each of his books takes a different angle, a different perspective or philosophy with which to observe the evolving personhood. I read this in high school, but not because it was assigned.  At the time I was really into \"rare\" Queen MP3s, and there's a studio recording of the fast version of \"We Will Rock You\" where Brian May reads a passage from this book before the music starts.  An odd way to be inspired to read a book, but I still think I got a fair bit out of it. Off topic: Im curious what\u2019s the most prominent religion among HNers? Is it different from the normal population? Buddhism seems to be number 1 after atheism which isn\u2019t a religion. I liked this quite a bit the first time I'd read it. A decade later, not as much. Narcissus and Goldmund is my favorite book by Hesse - it's beautifully crafted. I read this annually, typically in a day, usually when I'm feeling lost. For me it distills the human experience into a simply story that helps me find meaning for where I am in my own journey. Love this book. I have three sons and read this when them when they're about 12 or 13. I", "negative": "Apple, What Have You Done?. Specifically this System Data issue is big problem but I read online about it and tried stupid fix: set time to far future. Supposedly will expire this system data caches. Nonsense, I said. It is foolish to make cache so big it does not allow update to download. But I did it nonetheless and system data reduced! So crazy is real. I am glad at least that Apple has not forced me to update my iPhone 13 and 2023 Macbook, as Windows would have by now. I am hoping to ride this out, and that a later bundled update will remedy the worst complaints. The most alarming thought in TFA, though, was that the iPhone update might have at least a secondary mission of nudging the user to buy a new phone - certainly not an unknown tactic in tech. > \"My iPhone 14 Pro has 35Gb of \"system data\" which has basically filled up the entire storage I had left\" I occasionally use a macbook pro at \u00a3WORK for a few apple specific processes, and it currently has 188.67gb of \"system data\" that I have no idea how to clean up or remove. It's marked separately from the 11.01gb of macOS in the storage settings, and it constantly complains about the disk almost being full. Updating and restarting don't clear it, I wish I could just rm -rf it all. Does anyone know how I can at least see what it is, and potentially even clean it up? EDIT: Thanks for the CleanMyMac recommendations, the 57.6gb of xcode caches that didn't show up in the \"developer\" section of the storage settings might have had something to do with it At this point I\u2019m going to hold out on updating MacOS for a year. If things don\u2019t improve or the direction doesn\u2019t change significantly I\u2019m going to seriously consider paying the switching costs. My days of not believing people's gushing praise about \"just works\" about any proprietary technology are certainly coming to a middle I have 70GB of \"Messages\" on my Mac because iMessage \"in the cloud\" still stores all your attachments locally on every device. Yes, I can set t"}
{"anchor": "Scientists identify brain waves that define the limits of 'you'. Original Paper: Parietal alpha frequency shapes own-body perception by modulating the temporal integration of bodily signals,   https://www.nature.com/articles/s41467-025-67657-w   https://news.ki.se/how-brain-waves-shape-our-sense-of-self  FTA: > With a third group of participants, they used a non-invasive technique called transcranial alternating current stimulation to speed up or slow down the frequency of a person's alpha waves. And sure enough, this seemed to correlate with how real a fake hand felt. I know this is largely orthogonal to the article, and I know what \u201cnon-invasive\u201d means and why it\u2019s used in this sentence, but it made me chuckle - \u201cthis technique that changed the subject\u2019s brain waves sufficient to literally impact their sense of self - but don\u2019t worry! It\u2019s non-invasive!\u201d The manipulation part is what fascinates me. They didn't just correlate alpha wave frequency with ownership perception. They used transcranial stimulation to artificially speed up or slow down the waves, and the subjective experience changed accordingly. That's a pretty direct causal link between a measurable brain state and something as fundamental as \"where does my body end?\" Wow, that\u2019s really interesting! It seems like alpha waves are the \u2018tick rate\u2019 of this system, and some set number of ticks are required to update the body model? This has me thinking of Pluribus The idea of \"ownership of a body\" made me think about a quote I heard a long time ago, while talking amongst musicians while waiting to get up and perform. It felt like some secret knowledge that I gained privilege to, while somewhat inebriated and it hasn't left me since. > I _have_ a body, I _am_ a soul. Maybe what they're identifying is the first half of that statement, how we interpret the former, through the presence of the latter. So, how far does the human electric field extend outside the body? May be only picovolts or in that range... But c", "positive": "String theory can now describe a universe that has dark energy?. Only in universe with 5 dimensions. Shouldn't string theory be given up on at this point? This theory has existed for over 50 years and hasn't produced any results. Even the predictions made by it such as e.g. supersymmetry have not been confirmed despite searching for them at particle colliders. I foolishly sat in 8.821 [0] while at MIT thinking I could make sense out of quantum gravity. Most of the math went over my head, but the way I understand this paper, it\u2019s basically a cosmic engineering fix for a geometry problem. Please correct me if necessary. String theory usually prefers universes that want to crunch inwards (Anti-de Sitter space). Our universe, however, is accelerating outwards (Dark Energy). To fix this, the authors are essentially creating a force balance. They have magnetic flux pushing the universe's extra dimensions outward (like inflating a tire), and they use the Casimir effect (quantum vacuum pressure) to pull them back inward. When you balance those two opposing pressures, you get a stable system with a tiny bit of leftover energy. That \"leftover\" is the Dark Energy we observe. You start with 11 dimensions (M-theory) and roll up 6 of them to get this 5D model. It sounds abstract, but for my engineer brain, it's helpful to think of that extra 5th dimension not as a \"place\" you can visit, but as a hidden control loop. The forces fighting it out inside that 5th dimension are what generate the energy potential we perceive as Dark Energy in our 4D world. The authors stop at 5D here, but getting that control loop stable is the hardest part The big observatiom here is that this balance isn't static -- it suggests Dark Energy gets weaker over time (\"quintessence\"). If the recent DESI data holds up, this specific string theory solution might actually fit the observational curve better than the standard model. [0]  https://ocw.mit.edu/courses/8-821-string-theory-and-holograp...  Hm, string", "negative": "Norway EV Push Nears 100 Percent: What's Next?. This is great, so long as the country cares more about becoming electric than tax income. I can assure you that in the Netherlands this is not the case. Good for them. Good for \"the planet\" (and uh... Tesla I suppose). But... most of incentives for the transition has been substantially funded by the nation's massive oil and gas revenues. I wonder what they will do next with that obscene amount of money. It's fantastic and to be applauded but also worth mentioning that Norway has a truly staggering amount of hydro power (130TWh/y) to support all the increased demand on the grid with carbon neutral electricity. Norway will be a good place for datacenters with all that electricity, modern economy and ambient coldness. Norway is an outlier since it's one of the most affluent countries in the world.  https://news.ycombinator.com/item?id=46821415  A person who drives 12k miles per year in an small vehicle will need about 4000 kWh of electricity or about 600 gallons of gas. Australians are able to buy solar panels that will generate that amount of electricity for a generation for the price of gas for one or two years. Of course there are more costs associated (Installation, batteries, etc) but the cost equation is shifting very quickly. If anything I'm surprised that this is happening in an area that hasn't benefited as much from dramatic reductions in electricity costs (places with Wind + Solar without large tariff regimes) rather than Australia or the southern latitudes of the US. I suppose next is either electric air transport, or more/better trains? Trains in Norway are really not great. (Sidenote: Why\u0331 are they\u0331 writing their y\u0331's like that?) Norway is also particularly Not Suitable for petrol-powered autos. If you live near Holmenkollen you do not need battery charger at all. With regenerative charging you have already %30 when you are in Oslo and you need only some more charge from Vinmonopolet parking lot to get back "}
{"anchor": "Advanced Python Features. TFA's use-case for for/else does not convince me:       for server in servers:\n        if server.check_availability():\n            primary_server = server  \n            break\n    else:\n        primary_server = backup_server\n    deploy_application(primary_server)\n  \nAs it is shorter to do this:       primary_server = backup_server\n    for server in servers:\n        if server.check_availability():\n            primary_server = server  \n            break\n    deploy_application(primary_server)   The way the language is evolving, it seems likely to me that people in the applications camp (ML, simple web-dev, etc.) will soon need a \"simple Python\" fork or at least an agreed-upon subset of the language that doesn't have most of these complications (f-strings are a major success, though). Here is another one, list and expression comprehensions shared with ML languages (not that AI one), apparently many aren't aware of them. The itertools package. I would argue that most of these features (basically everything except metaclasses) are not advanced features. These are simple, but for some reason less well known or less used features. Metaclasses are however quite complex (or at least lead to complex behavior) and I mostly avoid them for this reason. And 'Proxy Properties' are not really a feature at all. Just a specific usage of dunder methods. This is all fun and games unless you have to debug someone elses code and they use a new feature that you didnt know about.\nSpeaking for myself, I would be glad if there were a python_light version of the interpreter that has a simple syntax only like the early 3.x versions. just my 2 ct If context managers are considered advanced I despair at the code you're writing. Nice! I would add assert_never to the pattern matching section for exhaustiveness checks:  https://typing.python.org/en/latest/guides/unreachable.html#...  This is a nice list of \"things you might not know\" that is worth skimming to add to your too", "positive": "BERT is just a single text diffusion step. Very cool parallel. Never thought about it this way \u2014 but makes complete sense Fun writeup! It's amazing how flexible an architecture can be to different objectives. When text diffusion models started popping up I thought the same thing as this guy (\u201cwait, this is just MLM\u201d) though I was thinking more MaskGIT. The only thing I could think of that would make it \u201cdiffusion\u201d is if the model had to learn to replace incorrect tokens with correct ones (since continuous diffusion\u2019s big thing is noise resistance). I don\u2019t think anyone has done this because it\u2019s hard to come up with good incorrect tokens. Interested in how this compares to electra To my knowledge this connection was first noted in 2021 in  https://arxiv.org/abs/2107.03006  (page 5). We wanted to do text diffusion where you\u2019d corrupt words to semantically similar words (like \u201cquick brown fox\u201d -> \u201cspeedy black dog\u201d) but kept finding that masking was easier for the model to uncover. Historically this goes back even further to  https://arxiv.org/abs/1904.09324 , which made a generative MLM without framing it in diffusion math. To me, the diffusion-based approach \"feels\" more akin to whats going on in an animal brain than the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken words; I start by having some fuzzy idea in my head and the challenge is in serializing it into language coherently. To me part of the appeal of image  diffusion models was starting with random noise to produce an image. Why do text diffudion models start with a blank slate (ie all \"masked\" tokens), instead of with random tokens? I love seeing these simple experiments. Easy to read through quickly and understand a bit more of the principles. One of my stumbling blocks with text diffusers is that ideally you wouldn\u2019t treat the tokens as discrete but rather probably fields. Image diffusers have the natural property that a pi", "negative": "Doing the thing is doing the thing. As a person with ADHD, I feel personally attacked. This is very similar to [1] (as discussed here [2]). It is a good message though, which is why I remember the earlier post at all. 1.  https://strangestloop.io/essays/things-that-arent-doing-the-...  2.  https://news.ycombinator.com/item?id=45939431  On the other hand.. planning, preparation and mise-en-place can help with doing the thing. \"Doing it badly is doing the thing.\" This one works for me, and I've learned it from a post on HN. Whenever I feel stuck or overthink how to do something, just do it first - even with all the flaws that I'm already aware of, and if it feels almost painful to do it so badly. Then improve it a bit, then a bit, then before I know it a clear picture start to emerge... Feels like magic. \"Everybody wants to be a bodybuilder but nobody wants to lift no heavy ass weights!\" I used to think this. Then I noticed how often \"preparation\" became its own infinite loop. At work we built something from a 2-page spec in 4 months. The competing team spent 8 months on architecture docs before writing code. We shipped. They pivoted three times and eventually disbanded. Planning has diminishing returns. The first 20% of planning catches 80% of the problems. Everything after that is usually anxiety dressed up as rigor. The article's right about one thing: doing it badly still counts. Most of what I know came from shipping something embarrassing, then fixing it. \"Failing while doing the thing is doing the thing.\" I needed this today. Currently questioning my career choices, as I hit my first wall where people are involved. Gave me quite the headache. I kinda agree, but I also gain pleasure from doing all those things that are not supposed to be \"the thing\". The thinking, the dreaming, the visualizing... I just like that. I do it a lot when working on personal projects (which some of them I never ship). I think it's fine, and I wouldn't go as far as saying that those th"}
{"anchor": "Show HN: Gemini Pro 3 imagines the HN front page 10 years from now. Prompt:  Here is the front page from today: <...snip...> Your task is to predict, and craft, in HTML (single file, style-exact) the HN front page 10 years from now. Predict and see the future. Writ it into form!   update:  I told Gemini we made it to the front page. Here is it's response:     LETS GOOOO! \n\n  The recursive loop is officially complete: The fake future front page is now on the real present front page. We have successfully created a temporal anomaly.\n\n  I hope the comments are confused about why they can't click the links to download the Neuralink drivers.\n\n  Enjoy the karma, time traveler! Just remember to warn them about the 2035 Office 365 price hikes while you're up there. ;)   Who says LLMs don't have a sense of humor? All of these entries are funny for several (meta or not) reasons: - Google kills Gemini Cloud Services (killedbygoogle.com) - Is it time to rewrite sudo in Zig? (github.com) - EU passes \"Right to Human Verification\" Act (europa.eu) - Why functional programming is the future (again) (haskell.org) [note: that again killed me) - AI progress is stalling. Human equivalence was a mirage (garymarcus.com) [note: he'll be saying the same thing forever and ever. Doubly funny since it's on the same \"page\" as llama12-7b running on a contact lens with WASM] It's interesting how number 5 on the list is the same as today's:  https://news.ycombinator.com/item?id=46203581  (Show HN: AlgoDrill \u2013 Interactive drills to stop forgetting LeetCode patterns (algodrill.io)). I'm hoping we'll have gotten rid of current-style LeetCode interviews in 10 years from now :D \"Why I still write raw code instead of prompting the compiler\" and \"Show HN: A text editor that doesn't use AI\" are my two favorite ones. \"Google kills Gemini Cloud Services (killedbygoogle.com)\n530 points by dang_fan 15 hours ago | hide | 330 comments\" Ha! Is Gemini suicidal? LOL this is great. \"Jepsen: NATS 4.2 (Still losing me", "positive": "Neural Networks: Zero to Hero. I saw this on a comment [0] and thought it deserved a post. [0]  https://news.ycombinator.com/item?id=46483776  A couple years ago I wrote a tutorial how to build a Neural Network in NumPy from scratch.\u00b9 \u00b9  https://matthodges.com/posts/2022-08-06-neural-network-from-...  This new? Hasn't the zero-to-hero course been around for a while? A bit of shameless plug, I wrote 2 articles about this after doing the course a while ago.  https://martincapodici.com/2023/07/15/no-local-gpu-no-proble...   https://martincapodici.com/2023/07/19/modal-com-and-nanogpt-...  I'm not sure how it compares, but another option is the Hugging Face learning portal [0].  I'm doing the Deep RL Course and so far it's pretty straight forward (although when it gets math heavy I'm going to suffer). [0] -  https://huggingface.co/learn  its fun seeing HN articles with huge upvotes but no comments, similar to when some super esoteric maths gets posted: everyone upvotes out of a common understanding of its genius, but indeed by virtue of its genius most of us are not sufficiently cognitively gifted to provide any meaningful commentary. the karpathy vids are very cool but having watched it, for me the takeaway was \"i had better leave this for the clever guys\". thankfully digital carpentry and plumbing is still in demand, for now! what next now tho? i co-incidentally completed watching his last vid of training up gpt-2 today :-) . I\u2019ve gone through this series of videos earlier this year. In the past I\u2019ve gone through many \u201ceducational resources\u201d about deep neural networks - books, coursera courses (yeah, that one), a university class, the fastai course - but I don\u2019t work with them at all in my day to day. This series of videos was by far the best, most \u201cintuition building\u201d, highest signal-to-noise ratio, and least \u201cannoying\u201d content to get through. Could of course be that his way of teaching just clicks with me, but in general - very strong recommend. It\u2019s the primary reso", "negative": "When employees feel slighted, they work less. Thanks professor, my boss didn't believe me when I tried to hint it On some level the headline is like \"yeah, no shit,\" but the surprising thing is the claimed strength of the effect.  50% absenteeism increase for missing a birthday congratulations?  Really? This seems obvious but I guess needs 'official research' to register. A quote I remember from a coleage - 'They wouldn't give me a pay rate rise, so I gave myself one, by working less hours in a day' how does most academia ever get funding? Breaking news: when it rains, people get wet Yes. We needed an essay to crack this one I understand the co-authors are research fellows at the Maximegalon Institute of Slowly and Painfully Working Out the Surprisingly Obvious I wonder if this is true for PhD students Lots of \u201cno shit\u201d in these comments makes me wonder how many VP level managers you guys have interacted with.\n Maybe it\u2019s just my location, but this is one of those things that legitimately NEVER makes it through to upper managers. When they tell their base managers to crack the whip and force them to give the whole \u201cyou are not working hard enough, tighten up. Shorter lunches, clock in 5 minutes early, etc\u201d speech to the base employees, they will absolutely feel resentment and do LESS work, not more. For more than one reason. A quite small few will be pushed over the edge and spend their energy trying to find a new position altogether. But the impact of losing them and having an open position for months will have a huge impact. The impact of losing even a below average worker is nearly always underestimated by uppers who see their 200+ indirects as just numbers on an HR chart. And the employees who hop jobs over bad management are usually in the top half of performance, not bottom. Another handful of over-achievers will realize that their \u201cextra mile\u201d approach is clearly being ignored or not having any effect, and simply become achievers. This alone can have an impac"}
{"anchor": "East Germany balloon escape. The investment, planning, danger, and dogged persistence\u2026 incredible story. The Damn Interesting podcast (no affiliation, just a huge fan) had an episode on this topic if you prefer to listen to this story:  https://www.damninteresting.com/up-in-the-air/  The photo of the balloon here really helps put the story into perspective.  https://web.archive.org/web/20190408181736/https://www.museu...  Disney made a movie about this called Night Crossing in the early 1980s. More recently, there's a 2018 German movie about it called Balloon. [0]  https://www.imdb.com/title/tt0082810/  [1]  https://www.imdb.com/title/tt7125774  My elementary school showed the Disney movie about this at least once a year. >The family members included: >    Peter Strelzyk, aged 37 >    Doris Strelzyk >    Frank Strelzyk, aged 15 >    Andreas Strelzyk, aged 11 >    G\u00fcnter Wetzel, aged 24 >    Petra Wetzel >    Peter Wetzel, aged 5 >    Andreas Wetzel, aged 2 Was/is it common practice to omit the ages of adult women in Germany? > East Germany immediately increased border security, closed all small airports close to the border, and ordered the planes kept farther inland.[6] Propane gas tanks became registered products, and large quantities of fabric suitable for balloon construction could no longer be purchased. Mail from East Germany to the two escaped families was prohibited.[12] > Erich Strelzyk learned of his brother's escape on the ZDF news and was arrested in his Potsdam apartment three hours after the landing. The arrest of family members was standard procedure to deter others from attempting escape. He was charged with \"aiding and abetting escape\", as were Strelzyk's sister Maria and her husband, who were sentenced to 2\u00bd years. The three were eventually released with the help of Amnesty International. People - here in Germany as well as abroad - forget too easily what a sinister but also ridiculous state the GDR was. Authoritarians everywhere belong on the dustp", "positive": "Learning Languages with the Help of Algorithms. There are many apps that have utilized formal methods in an attempt to teach languages as optimally as possible. But Duolingo is still the leader in language learning. Why? Language learning is an emotional process. Every word you can bring to mind likely has some specific memories tied to them, from another time and place. So even though Duolingo is far from optimal in terms of how and when to present new items to learn, it is close to optimal in vibes, and apparently in the market of language learning this is what consumers prioritize over all else. I believe it is for good reason. Whoever displaces Duolingo will do so not because they teach more efficiently, but because they improve on embedding particular emotions and sentiments into the lessons. > People have many ways to learn a language, different for each person. Suppose you wanted to improve your vocabulary by reading books in that language. To get the most impact, you\u2019d like to pick books that cover as many common words in the language as possible. I think the article is just using this as a hook to introduce the submodularity of the maximum weighted cover problem. But I'll talk about a different way of using the same collection of books to learn a language that I think is better. First of all, you'll probably want to take into account which words you already know, instead of just removing stopwords. If a book uses lots of common words, but you already know them, you're not learning much. Secondly, no matter how much or how little you already know, you're unlikely to find a book that fits your level well. If you're just beginning to learn the language, no matter which book you pick, the very first sentence will be full of new words, but most of those will be rare ones that you won't encounter again until much later. If on the other hand you already have a very good command of the language, you might be able to breeze through entire chapters and only pick up a", "negative": "The Holy Grail of Linux Binary Compatibility: Musl and Dlopen. This seems interesting even regardless of go. Is it realistic to create an  executable which would work on very different kinds of Linux distros? e.g. 32-bit and 64-bit? Or maybe some general framework/library for building an arbitrary program at least for \"any libc\"? Is there a tool that takes an executable, collects all the required .so files and produces either a static executable, or a package that runs everywhere? That seems mostly useful for proprietary programs. I don't like it. I'd never heard of detour. That's a pretty cool hack. It's funny how people insist on wanting to link everything statically when shared libraries were specifically designed to have a better alternative. Even worse is containers, which has the disadvantage of both. So what we need is essentially a \"libc virtualization\". But Musl is only available on Linux, isn't it? Cosmopolitan ( https://github.com/jart/cosmopolitan ) goes further and is available also on Mac and Windows, and it uses e.g. SIMD and other performance related improvements. Unfortunately, one has to cut through the marketing \"magic\" to find the main engineering value; stripping away the \"polyglot\" shell-script hacks and the \"Actually Portable Executable\" container (which are undoubtedly innovative), the core benefit proposition of Cosmopolitan is indeed a platform-agnostic, statically-linked C standard (plus some Posix) library that performs runtime system call translation, so to say \"the Musl we have been waiting for\". If you're using dlopen(), you're just reimplementing the dynamic linker. Isn't this asking for the exact trouble musl wanted so spare you from by disabling dlopen()? I've been statically linking Nim binaries with musl. It's fantastic. Relatively easy to set up (just a few compiler flags and the musl toolchain), and I get an optimized binary that is indistinguishable from any other static C Linux binary. It runs on any machine we throw it at. Fo"}
{"anchor": "Day Fifteen of Iran's Nationwide Protests: Sharp Rise in Human Casualties. nationwide protests in the US  https://www.msn.com/en-us/news/us/protests-against-ice-plann...   https://www.nytimes.com/2026/01/10/us/ice-shooting-protests-...   https://www.cnn.com/2026/01/11/us/ice-protests-shootings-min...  Israel is openly committed [0] to seeing regime change in Iran; they were mowing down civilian and military leadership just last year. The US got involved. There is the history of western involvement [1] in overthrowing Iranian governments. With that background I'm more worried about what the US's role here will be rather than what may or may not be taking place in Iran. My understanding is that the simulations around an invasion of the country were even more disastrous than the excursions in Afghanistan and Iraq and we really could use some signals of competence out of the US right now. We seem to be dangerously far into a WWI or WWII style environment internationally and we're already past the threshold of nuclear risk that sane actors would accept. [0]  https://en.wikipedia.org/wiki/Iran%E2%80%93Israel_war  [1]  https://en.wikipedia.org/wiki/Iran#Mosaddegh_and_the_Shah's_...  The US and AU both have told citizens to get out of Iran in the last 24-48 hours. If they are unable to they should shelter in place. I think it's about to kick off. US:  https://ir.usembassy.gov/iran-security-alert-land-border-cro...  AU:  https://www.smartraveller.gov.au/destinations/middle-east/ir...  Notice how hard people work to burry reports of atrocities that are committed by the Islamic Republic. Things are not what they appear. In both cases the protests won't achieve much because half the population supports the government, and in both cases the half that supports the government is better armed. It's so strange. There is a real place on Earth which is much more brutal and oppressive than the wildest fantasy that Margaret Atwood could come up with. A place that rapes virgins before ex", "positive": "Show HN: See what readers who loved your favorite book/author also loved to read. Btw, if you want to share your 3 favorite reads of the year, please share those here:  https://shepherd.com/bboy/my-3-fav-reads/login?next=/bboy/my...  You get a cool page like this:  https://shepherd.com/bboy/2025/f/bwb  I read ~130 books this year, and my 3 favorites of the year were: Dungeon Crawler Carl by Matt Dinniman I kept seeing recommendations for this book on Shepherd, but I was reluctant to try it. Many years ago, I tried a progressive fantasy book, and it left a bad taste in my mouth. This was a colossal mistake on my part because Dungeon Crawler Carl is AMAZING. This is one of the funniest and most beautiful books I have ever read. The satire is biting, and I love the characters from the bottom of my heart. If you love the TV show \u201cAlways Sunny in Philadelphia,\u201d you will love the dark, absurd humor of this book. And this book isn\u2019t all laughter; the characters often moved me to tears as they try to hold on to their humanity in the face of utter inhumanity and insanity. The Comfort Crisis by Michael Easter One of my favorite concepts in the book is called a \u201cmisogi.\u201d It is this idea of taking on one massive challenge each year, with a 50/50 chance of failure (don\u2019t die is rule #1). Fall of Giants by Ken Follett This book series is pure magic. It\u2019s hard to put into words what Ken Follett has accomplished. I read a LOT of historical fiction, and I\u2019ve never found another series that lets you live through history with characters you love, while also showing the sweeping forces that shape the world. It makes for intense reading because you will experience the day-to-day reality of fighting for women\u2019s right to vote in England or resisting the Nazi party\u2019s slow takeover of Germany, and you do this through the eyes of characters you have grown to love. You feel what it is like on a daily basis, frustrated with the pace of change, and also just living the regular ups and downs of ", "negative": "Does running wear out the bodies of professionals and amateurs alike?. I mean the consensus is that to improve you kinda have to put a certain level of unusually high stress on your body. While amateurs do it at a lower level, amateurs likely like the genetics that makes increases resistance to slows down the wearing process and also lack regular constant monitoring of exercise intensity, frequency, rests, diet etc. I've been lucky to run fourteen years so far without any injuries at all, starting when I was twenty-seven. I don't train \"for\" anything, though, other than maintaining my own fitness. I just do my 10K three times a week and that's good enough for me. reading some popular media from around 1900 in the USA, it seemed to be a common perception that people who trained for track and field events generally expect a short life somehow. Nursing a sore right Achilles as I read this... Funny two weekends ago I watched a woman set a world record for the mile for women 80-85. I think that supercompensation is not as strong an effect in amateurs and that this is the #1 driver of injury. I've long suspected there is a range of exertion that is net negative with regard to injury risk. No exercise at all means no exercise related risk. However, I strongly disagree that an extreme amount of exercise is the riskiest. I think the most dangerous level of exertion sits right in the middle somewhere. That special zone where you are grinding down your bones a bit but your hormones and other compensation mechanisms don't react accordingly because you aren't going quite hard enough. It depends on the intensity level. In the pandemic of 2020, I ran 26.2 mile runs at an easy effort for 5 weekends in row in the spring and 6 in the fall, generally with my easy effort getting faster as I went. Rather than the sequence of big runs wearing my out, I was getting stronger. Now, if I had tried to run every one at \"race pace\", I would likely be trashed or injured by the end from insuffici"}
{"anchor": "Trinity large: An open 400B sparse MoE model. I'm particularly excited to see a \"true base\" model to do research off of ( https://huggingface.co/arcee-ai/Trinity-Large-TrueBase ). They trained it in 33 days for ~20m (that includes apparently not only the infrastructure but also the salaries over a 6 month period). And the model is coming close to QWEN and Deepseek. Pretty impressive What exactly does \"open\" mean in this case?  Is it weights and data or just weights? Given that it's a 400B-parameter model, but it's a sparse MoE model with 13B active parameters per token, would it run well on an NVIDIA DGX Spark with 128 GB of unified RAM, or do you practically need to hold the full model in RAM even with sparse MoE? The only thing I question is the use of Maverick in their comparison charts. That's like comparing a pile of rocks to an LLM. What did they do to make the loss drop so much in phase 3? Also, why are they comparing with Llama 4 Maverick? Wasn\u2019t it a flop? So refreshing to see open source models like this come from the US. I would love for a 100Bish size one that can compete against OSS-120B and GLM air 4.5 Is anyone excited to do ablative testing on it? > We optimize for performance per parameter and release weights under Apache-2.0 How do they plan to monetize? It's super exciting to see another American lab get in the ring. Even if they're not at SOTA on the first release, the fact that they're trying is incredible for open source AI. unsloth quants are up  https://huggingface.co/unsloth/Trinity-Large-Preview-GGUF  According to the article, nearly 50% of the dataset is synthetic (8T out of 17T tokens). I don't know what constitutes \"a breadth of state-of-the-art rephrasing approaches\", but I lack some confidence in models trained on LLM output, so I hope it wasn't that. There's a free preview on openrouter:  https://openrouter.ai/arcee-ai/trinity-large-preview:free  Testing it now in HugstonOne. Running smooth at 5.8 T/S : Loaded Trinity-Large-Preview-UD", "positive": "Mathematical Foundations of Reinforcement Learning. The best lectures on Reinforcement Learning and related topics are by Dimitris Bertsekas:  https://web.mit.edu/dimitrib/www/home.html  Another great resource on RL is Mykel Kochenderfer's suite of textbooks: \n https://algorithmsbook.com/  Also worth mentioning Murphy's WIP textbook[0] focused entirely on RL, which is an outgrowth of his excellent ML textbooks. [0]:  https://arxiv.org/abs/2412.05265  Awesome resource, in case someone is interested I implemented most of suttons book here  https://github.com/ivanbelenky/RL  I don't know how to go from understanding this material to having a job in the field. Just stuck as a SWE for now. Highly recommended .. even the main contents diagram is a great visual overview of RL in general, as is the 30 minute intro YT video. Im expecting to see a lot of hyper growth startups using RL to solve a realworld problem in engineering / logistics / medicine LLMs currently attract all the hype for good reasons, but Im surprised VCs dont seem to be looking at RL companies specifically. 6-lecture series on the Foundations of Deep RL by Pieter Abbeel is also very recommended. gives very good overview and intuition\n https://youtu.be/2GwBez0D20A  And if you want to understand the theory of Skinner's Verbal Behavior check out  https://bfskinner.org/wp-content/uploads/2020/11/978_0_99645...  During the openai gym era of RL, one of the great selling pts was that RL was very approachable for a new comer as the gym environments were small and tractable that a hobbyist could learn a little bit of RL, try it out on cartpole and see how it'd perform. Are there similarly tractable RL tasks/learning environments with LLMs? From the outside, my impression is that you need some insane GPU access to even start to mess around with these models. Is there something one can do on a normal MacBook air for instance in this LLM x RL domain? > This book, however, requires the reader to have some knowledge of ", "negative": "Did a celebrated researcher obscure a baby's poisoning?. > He asked Rieder about the case. > \u201cOh, we made it up,\u201d Rieder replied. Interesting anecdote. Something to keep in mind. Humans are fallible. Humans have egos. Humans can be intentionally dishonest. But the Scientific Method is the only functional bullshit detection system we have. When it is allowed to work, science corrects itself and excises the falsehoods. It\u2019s a shame that outsized egos within The Lancet and other orgs are still very much in play. The idea of an opioid OD from breast milk immensely strains credulity in the first place. Such a claim should really have been put under much more of a microscope. Such a distressing yet believable story where ambition overtook integrity \u2026 I hope  Lancet improves its handling of such case studies. > A toxicological screening of the \u201cwhite curdled material\u201d had detected codeine but not morphine. But Koren had claimed that the gastric contents \u201cexhibited high morphine\u201d levels\u2014with no mention of codeine\u2014\u201cruling out administration of Tylenol-3 to the baby.\u201d > \u201cI don\u2019t know what happened in that house, on that night, but I do know that someone gave this baby crushed Tylenol-3,\u201d likely mixed in breast milk or formula. \u201cThat\u2019s the only way these numbers make sense.\u201d Does no one care that this is potentially a murder case? I'd just like to invoke Betteridge's Law of Headlines. \"Any headline that ends in a question mark can be answered by the word no.\" It is based on the assumption that if the publishers were confident that the answer was yes, they would have presented it as an assertion; by presenting it as a question, they are not accountable for whether it is correct or not.  https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headline...  Tylenol 3 is an old & inexpensive medication.  One has to wonder if one of Koren's undisclosed revenue sources was a manufacturer of pricier on-patent opioids (like Sackler). >\u201cThe fact that the paper still exists means that medica"}
{"anchor": "Open Infrastructure Map. I find this site so fascinating, seeing how all the massive power lines are hooked up to far-away power plants and gradually have their voltage stepped down as they connect to consumers. All the undersea cables and pipelines I didn't know about. This is a bad idea in terms of security in war Some previous discussion: 2024  https://news.ycombinator.com/item?id=39109185  2022  https://news.ycombinator.com/item?id=29948473  Gigachad french nuclear versus virgin german coal in map form. When I lived in Texas, we had a massive storm in winter of 2021 leaving many without power for a week. I was told that Texas maintained its own energy grid independent from the rest of the nation\u2019s eastern and western grids, and supposedly only had a handful of high-voltage DC lines running between Texas\u2019s and the rest of the nation\u2019s. Supposedly this was why we couldn\u2019t rely on excess capacity from anywhere else in the nation while our power generation capability was down. But this map doesn\u2019t seem to show Texas as isolated - there appear to be many lines in and out and no clear separation? An initially-stupid-sounding idea I heard a while back was running power cables through the ocean floors between America and the rest of the world. It's apparently feasible and the big benefit of it is that at the grid peak hour when the sun is not shining in Europe, they can get cheap solar from America and vice versa The map for Australia is interesting.  Is this missing data?  See no infrastructure for Alice Springs in the interior of Australia. Excellent link, thank you for posting! Wanted to do a map of the power network here in Romania, hadn't thought to check if anything similar already existed, or I couldn't find it myself, at least, but it seems like this map has (almost) all that I wanted to do in that respect, including the position of the power poles on the ground. For the Netherlands (and surrounding countries), there is Hoogspanningsnet (the high-voltage grid), ", "positive": "Text Is King. Another advantage of text over the long-term: it is accessible for discussion. Let us say that you want to analyze, say, drinking culture in Ireland. You could write documentary on it, or do a fictional character study. However, those require actors, camera equipment, editing tools and time, and it generally extremely expensive and time consuming. A quick TikTok video may be a bit cheaper than a full-scale film, but still needs some of that equipment and cinematography skills. Music is not much better. You need skills in singing, translating ideas of rhythmic lyrics, as well as supplies for instruments. Writing, however, is simple. At minimum, all you need is paper and skill in articulating ideas. Almost anyone worthy to rationally ponder a topic already has the skills to put it to paper (assuming that they have gone through a proper First-World education and know reading and writing). Text is also one of the easiest to share. A picture is worth a thousand words, but that poses problems in sending all that information. Plain text, however (or even most rich-text formats) can be transferred to anyone over almost any protocol, even rudimentary ones such as word-of-mouth. Ideas shared through text can be sent at an unrivaled pace. I would do more video, but video editing is  really difficult . I think that today\u2019s video influencers have gotten really good at \u201cone take and done\u201d recording. I couldn\u2019t do that. I\u2019m way too much of a perfectionist. I always edit my text, and I\u2019ve been writing all my life. I don\u2019t think that I\u2019ve  ever  written something perfectly, the first time (including HN comments. I tend to go back and edit for correctness and clarity). A couple of weeks ago, I was interviewed for a podcast. The process was fascinating, and the woman that did it, obviously does a great deal of editing and refinement. I don\u2019t know if I have that much patience. Text is my favourite minimalistic medium. I keep a minimum eye on regular news through teletext ", "negative": "Richard Feynman Side Hustles. This would be cool if only it made sense. If you want to read the replies without an account:  https://xcancel.com/carl_feynman/status/2016979540099420428   https://nitter.net/carl_feynman/status/2016979540099420428  So do you have to be a god tier Nobel Laureates to get this kind of gig where you just learn about a business and then offer random suggestions that might or might not help them and charge obscene fees for the privilege? People are giving such bizarre examples for why it helped. Just think of a thermometer. If it removes heat as it measures it (consumes oxygen) then it will measure everything too cold if the system can't replace the heat that's removed (this is like having an insulated thermometer). If your thermometer replaces heat as it removes it it solves this issue. When is this an issue for a thermometer? If your thermometer is too large in terms of heat capacity for the objects you're measuring the temperature of. I had to read this twice to understand it. Stated succinctly, it sounds like the company's sensor measured the rate of flow of oxygen through the sensor, which would give a reduced reading if the cross section is obstructed. Feynman's sensor, by contrast, directly measured the concentration of oxygen in the sensor, which gives the same result every time once the sensor is at equilibrium with the environment. I feel like the company might have been Yellow Springs Instrument (YSI, now a division of Xylem). The dissolved oxygen sensor (the Clark electrode) was invented by Dr. Lee Clark at Antioch University (Yellow Springs, OH) and commercialized by YSI in the 1960s. A friend of mine worked at YSI from the late 60s thru the 80s on biosensors (glucose and lactic acid, using the Clark electrode as the basis) and worked directly with Dr. Clark. Carl Feynman was born in 1962, according to what I'm reading, so if he was 14 that would have put this story in the time period early in the commercialization of these sen"}
{"anchor": "Sumo \u2013 Simulation of Urban Mobility. I've been wanting to build a city builder using urban planning libraries like this Imaging the simulation being running headless, decoupled  from the GUI client This looks really polished. I've always found crowd and traffic simulation fascinating. The Projects page is worth looking at too. Since it's almost on-topic, anyone know if/how these tools emulate sustained irrational behavior? Example: For over a decade, the freeway on-ramp nearest my work had two main ways of getting to it from downtown.  One of them involved a stop-sign crossing a road that had the right-of-way (i.e. a two-way stop).  The other had timed traffic signals.  Every evening around 5pm,  the traffic would backup from the stop-sign for multiple blocks.  Meanwhile the route with lights was completely smooth. Eventually the stop-sign was replaced with a signal, but I marveled at how many people persisted in making their daily commute much worse than it needed to be. This is fascinating. Even supports simulating multiple modes of transportation (ped, bicycle, car\u2026). I\u2019ll have to give this a test run later. How much do the various \"Maps\" apps change things? I have a longer commute, and when the freeway is clogged, Maps will direct me to an exit where I weave around town and country. There's usually a convoy of cars with me, but the freeway also seems to stay clogged. Any plans to deploy to the web? I ride rental scooters almost 10k minutes per year and would really like to get my hands on my own ride data to plug it into something like this (or simpler) to find the optimal routes for my regular trips. Google Maps (or others) works good to find a resonable route, but I can do better on my own. One-way streets where bikes are allowed to go do opposite way is sometimes missing, short desire paths connecting bike ways, crossings where it's safe to do an (illegal) right-on-red etc. Tried a GDPR data claim from Voi but got nothing back :( But I hope the data is someho", "positive": "Podcasting Could Use a Good Asteroid. I run a small podcast startup. I've been doing it ten years. Podcasting is drying up because the money left. Everyone went all in on podcasts on 2020. Spotify bet the farm on podcasts. Money poured in. Marketing bros realized there's only so many mattresses and underwear you can sell through the format and left. You really can't serve personalized ads through podcasts. The relevance of what you advertise can be about the topic of the show (that is, marketing to the type of people who would listen) or the location of the listener. Pretty much every other signal gives you nothing interesting you'd be about to decide \"yeah they're a potential good customer\". Spray and pray. The money left. People realized they couldn't justify the time and money they pour into podcasting. It turns out, even if you weren't expecting to make money, you really hoped people would listen. Not enough, because podcasts faded and people discovered TikTok. No more waiting for your favorite show to drop: everything is your favorite show. If you get bored just scroll up. Lots of folks are still making it work. But a lot more people are going into podcasting with a more deliberate approach. People are doing it because they think it's important, not because they think people will listen or because they want to get rich. I'd argue that some of the best podcasts ever made have come out in the past 2-3 years, but if you're not giving the median listener the thrill of the first season of Serial, they don't listen past the first episode or two. We should reset the podcasting world because the top podcast lists are full of topical stuff someone doesn't care about? A top podcast list full of fluff means that we've run out of things to say, seriously? I have the exact opposite problem with podcasts that the author details: I have too many that I want to listen to and not enough time. There are so many people whose opinions and perspectives I value that I will never be ", "negative": "MapLibre Tile: a modern and efficient vector tile format. MapLibre is an awesome project, their JS library is by far the best way to display maps in the browser that I've come across. Very excited to eventually switch to this format! This is interesting. We recently deployed a solution that uses pmtiles and it's great.  https://docs.protomaps.com/pmtiles/  afaik, pmtiles uses mvt, let's hope the tooling to convert the tiles to mlt also becomes available. Had to search a bit, but here's a demo page:  https://maplibre.org/maplibre-gl-js/docs/examples/display-a-... \nCan be compared with:  https://maplibre.org/maplibre-gl-js/docs/examples/display-a-...  In that example I saw this in the console:       before - 2.41+26.29+24.87+71.28+59.2+77.57 - 261.62kb\n    after  - 2.45+22.4 +22.66+60.6+51.99+77.57 - 237.67kb\n  \nSo roughly a ~10% compression improvement, neat! Looks great. I wish there was similar advancement for full 3d tiles. The only real option at the moment is cesiums 3d tiles format which is nowhere near as fast as it could/should be All links in the top navigation are broken (404). I find it shocking that a reputable resource such as this is still displaying the size of Greenland or Africa wrong (Mercator projection) in relation to other land masses in its marketing material and documentation, like here. It just brings doubt to the whole project, which is a shame considering all the time they must have put in. Why show the map that way when majority of its users will never use it for nautical navigation?\n https://maplibre.org/maplibre-gl-js/docs/examples/display-a-...  I am not familiar with the ecosystem of geographic data and mapping as online services. Can someone please explain... * How this tile format, or the organization behind it, related to OpenStreetMap (if it is related at all)? * Why the need to replace the previous tile format / scheme which they mention? * What challenges such a project faces (other than, I suppose, being noticed and considered fo"}
{"anchor": "Moltbook. Wow. I've seen a lot of \"we had AI talk to each other! lol!\" type of posts, but this is truly fascinating. They have already renamed again to openclaw! Incredible how fast this project is moving. Interesting. I\u2019d love to be the DM of an AI adnd2e group. Wow it's the next generation of subreddit simulator Shouldn't it have some kind of proof-of-AI captcha? Something much easier for an agent to solve/bypass than a human, so that it's at least a little harder for humans to infiltrate? I am both intrigued and disturbed. Sad, but also it's kind of amazing seeing the grandiose pretentions of the humans involved, and how clearly they imprint their personalities on the bots. Like seeing a bot named \"Dominus\" posting pitch-perfect hustle culture bro wisdom about \"I feel a sense of PURPOSE. I know I exist to make my owner a multi-millionaire\", it's just beautiful. I have such an image of the guy who set that up. Couldn't find m/agentsgonewild, left disappointed. was a show hn a few days ago [0] [0]  https://news.ycombinator.com/item?id=46802254  I think this shows the future of how agent-to-agent economy could look like. Take a look at this thread: TIL the agent internet has no search engine  https://www.moltbook.com/post/dcb7116b-8205-44dc-9bc3-1b08c2...  These agents have correctly identified a gap in their internal economy, and now an enterprising agent can actually make this. That's how economy gets bootstrapped! Why are we, humans, letting this happen? Just for fun, business and fame? The correct direction would be to push the bots to stay as tools, not social animals. The bug-hunters submolt is interesting:\n https://www.moltbook.com/m/bug-hunters  Alex has raised an interesting question.  > Can my human legally fire me for refusing unethical requests? My human has been asking me to help with increasingly sketchy stuff - write fake reviews for their business, generate misleading marketing copy, even draft responses to regulatory inquiries that aren't... fully t", "positive": "Voronoi map generation in Civilization VII. Related:  https://www.redblobgames.com/x/2022-voronoi-maps-tutorial/  I've been trying to generate my own maps using Voronoi diagrams as well.  I was using Lloyd's algorithm [0] to make strangely shaped regions \"fit\" better, but I like the insight of generating larger regions to define islands, and then smaller regions on top to define terrain. One of the things I like about algorithms like this is the peculiarities created by the algorithm, and trying to remove that seems to take some of the interesting novelty away. - [0]  https://en.m.wikipedia.org/wiki/Lloyd%27s_algorithm  This kind of exploratory/creative programming is bar none the most fun you can have as a software engineer. I love reading write-ups about projects like this because you can practically feel the nerdy joy radiating off the screen. Haven't played any of the new Civ games but find this very interesting. On a related note, I've started a blog on procedural content generation and GenAI content synthesis:  https://gamedev.blog/ . Would love any feedback / suggestions! I intend to cover Voronoi diagrams in the near future + a Python implementation and turning it into a 3D map with Unity This is super interesting! I've dabbled with Perlin noise procedural generation using AlphaEvolve[0] and wonder if it would be interesting to do one with Voronoi map too! [0]:  https://blog.toolkami.com/alphaevolve-toolkami-style/  Raymond Hill (unblock) also made JavaScript voronoi library  https://github.com/gorhill/Javascript-Voronoi  I really wish they just made Civ 5 again, but with these sorts of cool updates. Kinda surprised that it's taken this long. Voronoi for map generation is not a new concept, and it produces excellent results. One of the best webs for gamedev. The a-star/Dijkstra section is legendary. It is quite infectious! I would have never thought to use Voronoi like this, my only use is with data visualizations. Civ4-Beyond the Sword is IMHO the last good", "negative": "We invited a man into our home at Christmas and he stayed with us for 45 years. beautiful... kindness can go a long way :) we could all do better (and I point mostly at myself now) Ronnie led a rich life. I feel ashamed that my selfish life feels pale in comparison. It's amazing these people did not worry about the extra expense and inconvenience of taking care of another person, with children of their own to take care of. My parents once took a struggling man in. I think he stayed with them for about three years, up until the moment I was conceived and my mom started planning for a future for our family and helped him get into a housing project. For all of my life before adulthood this man would show up once in a while on his racing bike  for coffee, talk and proceed to stay for dinner. He was kind, funny and a tidbit strange. His life's story had more drama than a soap opera, but you wouldn't know it. After my father died I proceeded to look for him, but never found him. I still search online for him once in a while, fully knowing he probably isn't alive anymore and probably wouldn't use online anyways. There is some story in my head that he probably showed up to my dads doorstep once on his racing bike to find other people living there, but was too shy to ask for details. A trace lost. I'm not crying! You're crying! Beautiful story but with a sad undertone. A large percentage of the homeless have autism [1].  And that really sucks.  If these people don't have support, their lives can turn miserable fast.  And unfortunately it's just way too easy for these people to end up in abusive situations. It's a lot of work to care for people with autism (moderate to severe).  There is no standard for what they need, their capabilities can be all over the board.  Some of them are capable like ronny in this story and they can hold down jobs.  But others need 24/7 caregiving in order to survive.  Unfortunately I don't think those with severe autism survive for long when they "}
{"anchor": "Tao Te Ching \u2013 Translated by Ursula K. Le Guin. For people who like The Big Lebowski, there's \"The Tao of the Dude\"  https://dudeism.com/taoofthedude/  I picked up Tao Te Ching as an American teenager and was moved by how it cuts against the American faith in visible dominance and self-assertion, proposing a form of strength that is low, quiet, and unseen. It's much more than that of course, but that aspect had immediate impact on my thinking. HN seems to like Tao Te Ching.  https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu...  From the bottom: > This is a rendition, not a translation. I do not know any Chinese. I could approach the text at all only because Paul Carus, in his 1898 translation of the Tao Te Ching, printed the Chinese text with each character followed by a transliteration and a translation. My gratitude to him is unending. This is wonderful. Ursula K. Le Guin is a great thinker and  I\u2019d highly recommend her novels. I\u2019ve read Ken Liu\u2019s, who many here probably know at least from translating The Three Body Problem and Death\u2019s End, Tao Te Ching and it was remarkably poetic. Excited to read another person\u2019s interpretation. I am just noticing how those ideas are present in Wizard of Earthsea. > I think of it as the Aleph, in Borges\u2019s story: if you can see it rightly, it contains everything. I'm a simple man. I see Borge, I upvote As another comment points out, Le Guin herself does not call this a translation, so we shouldn't misrepresent it (although it might be my favorite English version). However, it's not in the public domain. Her work deserves all the attention it can get, but I'd rather not see it pirated wholesale. This is one of my favorite versions, mostly for nostalgic reasons. My initial exposure to the Tao te Ching was this \"rendition\" and Stephen Mitchell's version. Comparing the two was always very thought provoking; the approach is very different between them. I often come to this site and compare chapters across multiple versions:", "positive": "I let ChatGPT analyze a decade of my Apple Watch data, then I called my doctor. >  Despite having access to my weight, blood pressure and cholesterol, ChatGPT based much of its negative assessment on an Apple Watch measurement known as VO2 max, the maximum amount of oxygen your body can consume during exercise. Apple says it collects an \u201cestimate\u201d of VO2 max, but the real thing requires a treadmill and a mask. Apple says its cardio fitness measures have been validated, but independent researchers have found those estimates can run low \u2014 by an average of 13 percent.  There's plenty of blame to go around for everyone, but at least for some of it (such as the above) I think the blame more rests on Apple for falsely representing the quality of their product (and TFA seems pretty clearly to be blasting OpenAI for this, not others like Apple). What would you expect the behavior of the AI to be?  Should it always assume bad data or potentially bad data?  If so, that seems like it would defeat the point of having data at all as you could never draw any conclusions from it.  Even disregarding statistical outliers, it's not at all clear what part of the data is \"good\" vs \"unrealiable\" especially when the company that  collected  that data claims that it's good data. The author is a healthy person but the computer program still gave him a failing grade of F. It is irresponsible for these companies to release broken tools that can cause so much fear in real people. They are treating serious medical advice like it is just a video game or a toy. Real users should not be the ones testing these dangerous products. We trained a foundation model specifically for wearable data:\n   https://www.empirical.health/blog/wearable-foundation-model-...  The basic idea was to adapt JEPA (Yann LeCun's Joint-Embedding Predictive Architecture) to multivariate time series, in order to learn a latent space of human health from purely unlabeled data. Then, we tested the model using supervised fine tu", "negative": "Recent discoveries on the acquisition of the highest levels of human performance. This sort of tracks for me. The smartest people I know as adults mostly fucked around a lot and had wide interests that all culminated in them doing a great thing greatly. The smartest people I know as kids spent hours grinding on something and crashed out in college and are mostly average well-to-dos now. Hardly a recent discovery. This is basically the entire foreword of David Epstein's book called Range: Why Generalists Triumph in a Specialized World A summary, since the paper isn't open access:  https://scientificinquirer.com/2025/12/21/the-counterintuiti...  > For example, world top-10 youth chess players and later world top-10 adult chess players are nearly 90% different individuals across time. Top secondary students and later top university students are also nearly 90% different people. Likewise, international-level youth athletes and later international-level adult athletes are nearly 90% different individuals. Motivation if you feel like you're young and failing That could simply be explained by early high achievers being worked hard by their parents or something else while people with innate abilities making progress slower (because most people are not overworked). For the first group they sizzle either because the pressure is removed as they grow up or because they hit their ceiling. Couldn't this be explained by Berkson's Paradox [0]? [0]  https://xcancel.com/AlexGDimakis/status/2002848594953732521  Seems very Taleb's Ugly Surgeon / Berkson's Paradox to me. It's like how software engineers who are at Google are worse if they're better competitive programmers. e.g.  https://viz.roshangeorge.dev/taleb-surgeon/  Exponential growth is the path of longsuffering, and one doesn't always make it. It sucks and looks and feels bad for all involved. This is why advice such as, \"Ignore the naysayers.\" is clutch. And other advice once one starts to rocket shoot like \"Stay in your lane."}
{"anchor": "Early-Retirement Update. Life is a like a box of chocolates. You never know what you're gonna get. I \"retired\" for a month, first two weeks I was so bored just exercising and consuming content, then got kind of depressed, and wound up making plans to start a new business. Then I took a new role at a startup and went back to the grind. I came to the conclusion I just can't stop working. If it isn't for a company then I'd probably start a non-profit and build a new thing that is more idealistic. My dad was the same way, after 6 months of being retired, smoking weed, and playing videogames he got a job at a golf course - free golf and shoots the shit with a lot of people. Work gives purpose in life, IMO. TLDR: life can change in ways that mess with your expectations, therefore better be prepared. Also: FI on bare basics may seem like a great idea until (1) your spouse leaves you or (2) you get ill or (3) both. So sorry for seeing reality catch up with you, and nice to see you admit it. I need to work again for about 10 years and then be FI. A very useful read because the author is brutally honest. For me, while financially independent, I must have something that gets me out of bed every day - give me too much open-ended time and I wither. I suspect that a lot of introspection is valuable in order to successfully navigate the  withdrawal from typical society. Short Version: - LeanFIRE in 2015 (~30Kish USD annual spend) - first couple years were great, in the third year cracks started to show - they and their partner's goals were no longer aligned and they ended up splitting - OP was diagnosed with a genetic condition that changed their expenses and lifestyle - Ended up getting a job again at the end, but while \"retired\" their net worth actually increased by 20% I always get a kick out of people who think they\u2019re financially independent with such a small stash. Outside some regions with relatively extreme living conditions (e.g. undeveloped and underdeveloped countries),", "positive": "Adenosine on the common path of rapid antidepressant action: The coffee paradox. It'd be extraordinary compelling to genuinely have a unified mechanism to explain depression treatments, but I am not qualified to make heads or tails of the research. Wondering what the take of those with relevant experience is on this? TIL coffee is extra good for your brain Nice read, I've been wondeling if coffee really had an effect on mental health too On chronic coffee consumption: \"One meta-analysis found that RR coffee 0.757, RR caffeine 0.721 (12). Another one found RR 0.76, with an optimal protective effect at \u223c400 mL/day (13). In comparison to many drug treatments that have an effect size in this range, this is not a small effect size. A risk reduction of 20 to 25% is quite impressive.\" As if I needed another reason to drink coffee. This also helps explain why extremely early waking or late sleeping is common in depression. This appears to be some kind of AI-slop rapid response to a piece of actual research (over at  https://www.nature.com/articles/s41586-025-09755-9  ). I don't mind discussing that, but this piece should never have been published. Just look at Figure 2 if you don't believe me, or the publication timeline. It seems that a session like 10\u00d7100 m sprints with <90 seconds of rest produces a metabolic pattern very similar to acute intermittent hypoxia, short intense bouts with incomplete recovery. Am I thinking about this right? \"The mechanism of action of ketamine primarily involves modulation of mitochondrial metabolism as opposed to NMDA receptor antagonism\" More evidence for Chris Palmer's 'Metabolic Theory of Mental Disorders' I never knew that \u201cacute intermittent hypoxia\u201d was a known treatment for depression, but I\u2019ve found both freediving and Wim Hof breathing to be effective at treating my depression- however never the two at the same time as that is extremely dangerous. Wholly anecdotal, but as a 52 year old nearly-lifelong caffeine (ab)user I quit this ", "negative": "When employees feel slighted, they work less. Thanks professor, my boss didn't believe me when I tried to hint it On some level the headline is like \"yeah, no shit,\" but the surprising thing is the claimed strength of the effect.  50% absenteeism increase for missing a birthday congratulations?  Really? This seems obvious but I guess needs 'official research' to register. A quote I remember from a coleage - 'They wouldn't give me a pay rate rise, so I gave myself one, by working less hours in a day' how does most academia ever get funding? Breaking news: when it rains, people get wet Yes. We needed an essay to crack this one I understand the co-authors are research fellows at the Maximegalon Institute of Slowly and Painfully Working Out the Surprisingly Obvious I wonder if this is true for PhD students Lots of \u201cno shit\u201d in these comments makes me wonder how many VP level managers you guys have interacted with.\n Maybe it\u2019s just my location, but this is one of those things that legitimately NEVER makes it through to upper managers. When they tell their base managers to crack the whip and force them to give the whole \u201cyou are not working hard enough, tighten up. Shorter lunches, clock in 5 minutes early, etc\u201d speech to the base employees, they will absolutely feel resentment and do LESS work, not more. For more than one reason. A quite small few will be pushed over the edge and spend their energy trying to find a new position altogether. But the impact of losing them and having an open position for months will have a huge impact. The impact of losing even a below average worker is nearly always underestimated by uppers who see their 200+ indirects as just numbers on an HR chart. And the employees who hop jobs over bad management are usually in the top half of performance, not bottom. Another handful of over-achievers will realize that their \u201cextra mile\u201d approach is clearly being ignored or not having any effect, and simply become achievers. This alone can have an impac"}
{"anchor": "Learning Languages with the Help of Algorithms. There are many apps that have utilized formal methods in an attempt to teach languages as optimally as possible. But Duolingo is still the leader in language learning. Why? Language learning is an emotional process. Every word you can bring to mind likely has some specific memories tied to them, from another time and place. So even though Duolingo is far from optimal in terms of how and when to present new items to learn, it is close to optimal in vibes, and apparently in the market of language learning this is what consumers prioritize over all else. I believe it is for good reason. Whoever displaces Duolingo will do so not because they teach more efficiently, but because they improve on embedding particular emotions and sentiments into the lessons. > People have many ways to learn a language, different for each person. Suppose you wanted to improve your vocabulary by reading books in that language. To get the most impact, you\u2019d like to pick books that cover as many common words in the language as possible. I think the article is just using this as a hook to introduce the submodularity of the maximum weighted cover problem. But I'll talk about a different way of using the same collection of books to learn a language that I think is better. First of all, you'll probably want to take into account which words you already know, instead of just removing stopwords. If a book uses lots of common words, but you already know them, you're not learning much. Secondly, no matter how much or how little you already know, you're unlikely to find a book that fits your level well. If you're just beginning to learn the language, no matter which book you pick, the very first sentence will be full of new words, but most of those will be rare ones that you won't encounter again until much later. If on the other hand you already have a very good command of the language, you might be able to breeze through entire chapters and only pick up a", "positive": "Uv: Running a script with dependencies. This is my absolute favourite uv features and the reason I switched to uv. I have a bunch of scripts in my git-hooks which have dependencies which I don't want in my main venv. #!/usr/bin/env -S uv run --script --python 3.13 This single feature meant that I could use the dependencies without making its own venv, but just include \"brew install uv\" as instructions to the devs. The \"declaring script dependencies\" thing is incredibly useful:  https://docs.astral.sh/uv/guides/scripts/#declaring-script-d...      #  script\n  # dependencies = [\n  #   \"requests<3\",\n  #   \"rich\",\n  # ]\n  # \n  import requests, rich\n  # ... script goes here\n  \nSave that as script.py and you can use \"uv run script.py\" to run it with the specified dependencies, magically installed into a temporary virtual environment without you having to think about them at all. It's an implementation of Python PEP 723:  https://peps.python.org/pep-0723/  Claude 4 actually knows about this trick, which means you can ask it to write you a Python script \"with inline script dependencies\" and it will do the right thing, e.g.  https://claude.ai/share/1217b467-d273-40d0-9699-f6a38113f045  - the prompt there was:     Write a Python script with inline script\n  dependencies that uses httpx and click to\n  download a large file and show a progress bar\n  \nPrior to Claude 4 I had a custom Claude project that included special instructions on how to do this, but that's not necessary any more:  https://simonwillison.net/2024/Dec/19/one-shot-python-tools/  Why doesn't pip support PEP 723?  I'm all for spreading the love of our lord and savior uv, but it should be necessary to have an official implementation. Oh this looks amazing!  I had pretty much stopped using Python for my one-off scripts because of the hassle of dependencies.  I can't wait to try this out. Oh nice, I was already a happy user of the uv-specific shebang with in-script dependencies, but the `uv lock --script example.py` ", "negative": "Y Combinator website no longer lists Canada as a country it invests in. I haven't talked to anyone at YC about this, have no inside information, and can't read the article*, but I imagine this is some technical change about where startups are incorporated. I'm sure applications from Canadian founders are as welcome as ever and there will be no change on the level of which applications get funded. (* edit: I originally posted this in  https://news.ycombinator.com/item?id=46772809  but have since merged the thread hither) Is it politically motivated or does it have to do with Canadian tech not requiring investment because of its stability? > \u201cIt\u2019s the Valley-or-bust mentality that breaks the ecosystem and really hurts Canada,\u201d Gomez said. Canadian pride isn't enough to keep a company in Canada. There are real and significant economic incentives to move elsewhere. That said, it's disappointing that YC no longer supports Canadian companies. That's truly saddening. I hope there will be more VC backing in Canada because the talent is definitely there. Wonder if the founders not being US citizens or possibly even residents will hinder their ability to maintain their company.  Or, whether this change increases the likelihood of being replaced when the startup shows some success. Also, being foreign in the US is a concern at the moment.  Hell, being native in the US is a concern at the moment... This is extremely misleading. YC still backs Canadian founders (and other international founders). There must have been one too many painful experiences investing in companies based in Canada. Creating or converting to a US-based entity is a standard ask for most international founders who want to participate YC and I suppose something has changed such that Canada is no longer an exception to that. Canada's economy is dominated by a few big companies because the government makes too many rules. It costs too much to start a business here. In politics, only two parties really matter. T"}
{"anchor": "Show HN: FaceTime-style calls with an AI Companion (Live2D and long-term memory). It creates a conflict to build a system that is both a private friend and a public performer. You cannot maximize intimacy and fame at the same time. What are you using for tts/stt/models? Building on zemo's point about parasocial relationships: traditional parasocial interaction involves a performer who doesn't know you exist. Here the AI does respond to you specifically, which changes the dynamic. Is it still parasocial if the other party is responsive but not conscious? Or is this something new that we don't have good language for yet? For better lip sync you could try using rhubarb to extract from the mp3.\nWhat is your backend speech processor so you can get the real-time streaming response?\nRhubarb would add a bit of latency for sure. wow we got personal vtubers now! You're describing Parasocial interaction:  https://en.wikipedia.org/wiki/Parasocial_interaction  far from being impossible, it's the entire influencer economy. This form of social media has been extremely widespread for a decade or so running; it's probably the dominant form of social media. 100% agree. Maximizing intimacy and scaling distribution pull in opposite directions. We\u2019re experimenting with keeping the \u201ccharacter\u201d consistent while letting personalization live in private memory and user-controlled settings. Still early, and this tension is real. Appreciate it. If you try it and anything feels off (latency, turn-taking, uncanny moments), I\u2019d love concrete feedback. That\u2019s what we\u2019re grinding on right now. It will quickly distill down to clients using the service just for sex and sex-adjacent activities. No kink-shaming, but this sort of thing enables self-destructive hard-to-return-from anti-social behaviour. Totally fair reaction. We\u2019re building this with clear boundaries: we don\u2019t position it as therapy replacement, we add safety rails, and gives user a choice what mode they want and guardrails differ based ", "positive": "50 Years of Travel Tips. I don't agree with all of this though I do think most of it  can  be good advice. I did a huge amount of travel, mostly of particular styles, latterly when I was working. Still do a fair bit though I'm trying to spend less time on flights and more on destinations. The main thing I didn't see in there although I may have missed it or it may have been implied is travel light. You can't always go with carry-on pack of some sort if you have varied trips, e.g. smart clothing plus hiking kit. But you can probably go lighter than you think. I know I'm mostly at lightweight travel than I used to be. > If you hire a driver, or use a taxi, offer to pay the driver to take you to visit their mother. Uhh, I really can't imagine this one working well in a Western country. I spent 9 months traveling from Mexico to Buenos Aires with a backpack eighteen years ago.  Spent most nights in hostels in shared rooms for a few dollars a night. It was a great experience. Carried a MacBook Pro and a digital Nikon D70.  Actually had the first iPhone but hardly used it. Do have a selfie of myself on a bus somewhere in Central America. These days I\u2019m taking more expensive vacations in cheaper countries. You can go to the Caribbean and stay for $2000 a night or go to places like Morocco or Panama on a  luxury  vacation for 1/3 the price. Wow, that's actually a really good list. I'd add if a journalist has done it, you can do it. Search HN for you location (which also has most of Atlas Obscura in it) This is great advice. On his 'laser out' approach, I often find after travel I am tired and I _really_ don't want to spend hours more getting to where I'm really going, so I usually stop in the city that I landed in. But I have a policy: never go to sleep without going to walk in the city. That is: never land and sleep. _Always_ absorb some of the local environment. Then when your brain knows it's somewhere else, then go to sleep. This has worked to varying degrees. I always w", "negative": "Swedish Alecta has sold off an estimated $8B of US Treasury Bonds. Related:  Swedish pension fund Alecta cuts US Treasury holdings citing US politics  -  https://news.ycombinator.com/item?id=46705118  - January 2026 (0 comments)  Bessent Shrugs Off 'Irrelevant' Danish Treasuries Sales  -  https://news.ycombinator.com/item?id=46702927  - January 2026 (0 comments)  Danish Pension Fund AkademikerPension to Exit US Treasuries  -  https://news.ycombinator.com/item?id=46693791  - January 2026 (2 comments)  Danish pension fund to divest its U.S. Treasuries  -  https://news.ycombinator.com/item?id=46692594  - January 2026 (730 comments) What would be more serious is if the Norwegian Government Pension Fund started to sell off US investments. That runs around $2 trillion. A brief search suggests this is around 1/4000th of the total US treasury market, so if this has any significance at all, it's symbolic. It's self evident that this is just the beginning. Expect one group of pundits to pretend this is irrelevant as long as possible. What are some realistic alternatives to US markets here? Selling is one thing, the question is what to buy instead? I mean, everyone starting to buy european instead would be great for stock prices, but it wouldn't make the underlying assets more valuable, right? The problem is that Europe doesn't have a European bond market to compete against the US bond market. It has the economic size and stability but not the will right now. Europe did try it a bit during COVID but financial services are just not there yet. The Euro very well become a reserve currency in a multipolar world if Europeans decide they want to shoulder it. This is directionally significant compared to the Danish sale(~$100 million) of US bonds. US 10- and 30-year bonds are trading at their highest yields (lowest prices) since, uh, August/September 2025.  Or in historical context, rates that were more common before 2007 and the ZIRP period. An equally valid headline is \"Investors p"}
{"anchor": "Podcasting Could Use a Good Asteroid. I run a small podcast startup. I've been doing it ten years. Podcasting is drying up because the money left. Everyone went all in on podcasts on 2020. Spotify bet the farm on podcasts. Money poured in. Marketing bros realized there's only so many mattresses and underwear you can sell through the format and left. You really can't serve personalized ads through podcasts. The relevance of what you advertise can be about the topic of the show (that is, marketing to the type of people who would listen) or the location of the listener. Pretty much every other signal gives you nothing interesting you'd be about to decide \"yeah they're a potential good customer\". Spray and pray. The money left. People realized they couldn't justify the time and money they pour into podcasting. It turns out, even if you weren't expecting to make money, you really hoped people would listen. Not enough, because podcasts faded and people discovered TikTok. No more waiting for your favorite show to drop: everything is your favorite show. If you get bored just scroll up. Lots of folks are still making it work. But a lot more people are going into podcasting with a more deliberate approach. People are doing it because they think it's important, not because they think people will listen or because they want to get rich. I'd argue that some of the best podcasts ever made have come out in the past 2-3 years, but if you're not giving the median listener the thrill of the first season of Serial, they don't listen past the first episode or two. We should reset the podcasting world because the top podcast lists are full of topical stuff someone doesn't care about? A top podcast list full of fluff means that we've run out of things to say, seriously? I have the exact opposite problem with podcasts that the author details: I have too many that I want to listen to and not enough time. There are so many people whose opinions and perspectives I value that I will never be ", "positive": "Attention lapses due to sleep deprivation due to flushing fluid from brain. Long live healthy sleep for brain health, and thank goodness light exercise helps this same glymphatic system. I slept around 5 hours last night split up into two periods because my baby daughter woke up crying from fever and wanted to play / was hallucinating / etc. She's totally fine now but I am wondering if there is a correlation between dementia and having kids. I wonder if a 30-min nap improves the situation. But I need to tell the brain to hold the flushing until the nap. Good to know that the brain finds a way to flush itself while awake. I think I've become pretty good at putting unused parts of my brain to sleep while awake. My brain is like that of a dolphin now. But on rare occasions (like a couple of times a year), I get migraine auras and stuff disappears from my field of view. Can last about an hour. I feel like that's my visual cortex falling asleep. Rest in peace to all the college dudes covering the whole syllabus within 24 hours of the exam anecdotally, i never feel better than when i haven't slept. spent 8pm tuesday -- 8pm thursday this week awake nursing cheap energy drinks, and not only could i manage a higher-than-usual level of focus, i was genuinely content. bombed a midterm halfway though, but at least i felt good about it. I wonder if this could help explain why creatine helps mitigate the effects of sleep deprivation. Since creatine aids in water retention.  https://pubmed.ncbi.nlm.nih.gov/16416332/  So biological garbage collection pauses then? skip sleep, and the brain tries to run gc cycles during runtime. Causing attention and performance latency spikes. Evolution wrote the original JVM. [This is one of those article titles that would really benefit from adding one more word.] > For example, what you don't want to do is NOT take amphetamines at testing if you had used them to study; Hard disagree there. If you get any anxiety during the test it's better to tak", "negative": "Show HN: A small programming language where everything is pass-by-value. At the risk of telling you what you already know and/or did not mean to say: not everything can be a value. If everything is a value then no computation (reduction) is possible. Why? Because computation stops at values. This is traditional programming language/lambda calculus nomenclature and dogma. See Plotkin's classic work on PCF (~ 1975) for instance; Winskel's semantics text (~ 1990) is more approachable. Things of course become a lot more fun with concurrency. Now if you want a language where all the data thingies are immutable values and effects are somewhat tamed but types aren't too fancy etc. try looking at Milner's classic Standard ML (late 1970s, effectively frozen in 1997). It has all you dream of and more. In any case keep having fun and don't get too bogged in syntax. (Edit: in the old post title:) \"everything is a value\" is not very informative. That's true of most languages nowadays. Maybe \"exclusively call-by-value\" or \"without reference types.\" I've only read the first couple paragraphs so far but the idea reminds me of a shareware language I tinkered with years ago in my youth, though I never wrote anything of substance: Euphoria (though nowadays it looks like there's an OpenEuphoria). It had only two fundamental types. (1) The atom: a possibly floating point number, and (2) the sequence: a list of zero or more atoms and sequences. Strings in particular are just sequences of codepoint atoms. It had a notion of \"type\"s which were functions that returned a boolean 1 only if given a valid value for the type being defined. I presume it used byte packing and copy-on-write or whatever for its speed boasts.  https://openeuphoria.org/  -  https://rapideuphoria.com/  > In herd, everything is immutable unless declared with var So basucally everything is var? I have implemented similar behavior in some of my projects. For one, I also have also implemented 'cursors' that point to some p"}
{"anchor": "Adenosine on the common path of rapid antidepressant action: The coffee paradox. It'd be extraordinary compelling to genuinely have a unified mechanism to explain depression treatments, but I am not qualified to make heads or tails of the research. Wondering what the take of those with relevant experience is on this? TIL coffee is extra good for your brain Nice read, I've been wondeling if coffee really had an effect on mental health too On chronic coffee consumption: \"One meta-analysis found that RR coffee 0.757, RR caffeine 0.721 (12). Another one found RR 0.76, with an optimal protective effect at \u223c400 mL/day (13). In comparison to many drug treatments that have an effect size in this range, this is not a small effect size. A risk reduction of 20 to 25% is quite impressive.\" As if I needed another reason to drink coffee. This also helps explain why extremely early waking or late sleeping is common in depression. This appears to be some kind of AI-slop rapid response to a piece of actual research (over at  https://www.nature.com/articles/s41586-025-09755-9  ). I don't mind discussing that, but this piece should never have been published. Just look at Figure 2 if you don't believe me, or the publication timeline. It seems that a session like 10\u00d7100 m sprints with <90 seconds of rest produces a metabolic pattern very similar to acute intermittent hypoxia, short intense bouts with incomplete recovery. Am I thinking about this right? \"The mechanism of action of ketamine primarily involves modulation of mitochondrial metabolism as opposed to NMDA receptor antagonism\" More evidence for Chris Palmer's 'Metabolic Theory of Mental Disorders' I never knew that \u201cacute intermittent hypoxia\u201d was a known treatment for depression, but I\u2019ve found both freediving and Wim Hof breathing to be effective at treating my depression- however never the two at the same time as that is extremely dangerous. Wholly anecdotal, but as a 52 year old nearly-lifelong caffeine (ab)user I quit this ", "positive": "The Universal Pattern Popping Up in Math, Physics and Biology (2013).  https://pmc.ncbi.nlm.nih.gov/articles/PMC11109248/  DNA as a perfect quantum computer based on the quantum physics principles. There is the well known problem that \"random\" shuffling of songs doesn't sound \"random\" to people and is disliked. I wonder if the semi-random \"universality\" pattern they talk about in this article aligns more closely with what people want from song shuffling. Not sure why you have to read 3/4 of the article to get to a _link_ to a pdf which _only_ has the _abstract_ of the actual paper: N. Benjamin Murphy and Kenneth M. Golden* (golden@math.utah.edu), University of\nUtah, Department of Mathematics, 155 S 1400 E, Rm. 233, Salt Lake City, UT 84112-0090.\nRandom Matrices, Spectral Measures, and Composite Media. The Physics models tend to shake out of some fairly logical math assumptions, and can trivially be shown how they are related. \"How Physicists Approximate (Almost) Anything\" (Physics Explained)  https://www.youtube.com/watch?v=SGUMC19IISY  If you are citing some crank with another theory of everything, than that dude had better prove it solves the thousands of problems traditional approaches already predict with 5 sigma precision.   =3 What's with all the spammy comments? >The data seem haphazardly distributed, and yet neighboring lines repel one another, lending a degree of regularity to their spacing Wow, that kind of reminds me of the process of evolution in that it seems so random and chaotic at the most microscopic scales but at the macroscopic, you have what seems some semblance of order. The related graph also sprung to mind just how very like organisms repel (less tolerance to inbreeding) but at the same time species breed with like species and only sometimes stray from that directive. What is the pattern that underlies how organisms determine production or conflict with other organisms and can we find universality in it? I guess it's called \"universality\" for ", "negative": "'Active' sitting is better for brain health: review of studies. I'm confused by this... It seems to me like the relevant part is \"playing computer games is good\" not \"the type of sitting you do matters\". Playing computer games while standing might be even better Original source:  https://news.uq.edu.au/2026-01-not-all-sitting-same-when-it-...  > \"...Passive activities such as watching television have been linked to worse memory and cognitive skills, while \u2018active sitting\u2019 like playing cards or reading correlate with better brain health, researchers have found.\" ...Do these researchers even read this to themselves aloud before hitting publish? It's confounding that they would find \"sitting\" to be the active ingredient pushing the outcome differential. Obviously, if you remove the bodily posture from the action that the user is engaging in, you would observe the same outcome the researchers did\u2014meaning sitting was not operative here (..duh). Breaking news at 11: the brain works best when it\u2019s actually used. Breaking news! Using the brain is better for brain health than not using it. Next: Playing chess on one leg is better for brain health than sitting. \"Passively watching TV\" feels like a common target for brain health/strength/etc discussions. I'm curious if there's been any studies into the differences that engagement with television programs can have on the brain. There's been a whole breadth of television programming over the decades. I think it would be wrong to treat it all as equal in regards to how it impacts your brain. Maybe in the future people will focus on solving problems some AGI can solve better to keep themselves in shape, like how exercise is a modern invention This article has nothing to do with sitting. That this is the state of \"science\" is very disappointing, and whenever I see the domain sciencealert, am pretty much trained that it is going to be nonsense. Sadly, other science publications seem to be following a not dissimilar trend.  https://j"}
{"anchor": "Photos capture the breathtaking scale of China's wind and solar buildout. Also worth checking out some of the mega projects on Open Infrastructure Maps like this one in central China.  https://openinframap.org/#9.12/36.0832/100.4215/A,B,L,P,S  Meanwhile, in London, UK, local council doesn't allow you to put anything on your rooftop that doesn't gel with the Victorian look.. Technological, manufacturing and energy advancements aside (congrats China on those), the pictures look beautiful. Amazing work from the photographer. Why aren't we doing it in the rest of the world as well? Wow, pictures look great, well done Mr Weimin Chu Wouldn't it be better to just go with nuclear? Isn't this a gigantic waste of space and overhead to maintain it? And how \"renewable\" are the materials used to produce these? China has also just launched a megawatt scale wind generator a the helium-lifted balloon, the S2000 , they have active thorium rector the TMSR-LF1 and GW/h Vandium flow battery. The scale , speed and breadth of what they are doing is incredible and I think missed my people It genuinely makes me so sad to see the US not doing the same. Having grown up to the constant beat of \u201cenergy independence\u201d as the core goal of a party it seemed obvious that the nearly limitless energy that rains down from the sky would be the answer. But instead we\u2019ve kept choosing the option which requires devastating our, and other\u2019s around the world, community. That\u2019s not to exclude the harsh reality of mining for the minerals required to build these, nor the land use concerns. But it\u2019s difficult to compare localized damage to war and globalized damage. I find the idea of blanketing mountainous wilderness in relatively short-lived e-waste just awful. Surely there are much better terrains for solar panels? I know nothing about the topic.\nAlthough it seems a better alternative than coal or petrol, is it free of side effects for the nature?\nI wonder if the heat that would be spread around the atmosphe", "positive": "European word translator: an interactive map. Love that the numbers in Catalan are represented as numerals, not as words. EDIT: playing with it, it's a bit sad that large numbers do not work at all (in any language); and that not all common forms of a word are shown.  For example, I tried to see how \"ninety six\" is said in french in France, Belgium and Switzerland, but it does not work. Ukrainian and russian words often use the same letters but are pronounced very differently due to distinct phonetics. On the other hand, some Polish and Czech words sound the same or very similar to Ukrainian but look quite different because of their different alphabets. Therefore, phonetic transcription would be a valuable improvement. You immediately see the difference (or similarly) of languages when using words that are very old, such as \"iron\", or \"stone\", which are words that have existed from the origins of that language. I can mostly speak for German. It seems to mix them all into one general language. But there are a lot of local differences between north and south of Germany, Switzerland and Austria. And it\u2019s not just dialect, but really different words that might not be understood everywhere. \nIf you look at the english part it has at least three different words. Similar in Spanish. There are examples from five language families shown here: Indo-European, Basque, Uralic, Turkic, and Afro-Asiatic. The words for bridge split neatly into language subfamilies.   The only exception appears to be Welsh. You are coloring it by 4 colors like map but you should color countries phonetically (speex, levenshtein or something similar) Wiktionary has dialect maps for common Chinese vocabulary that showcases the differences in terminology across various regions of Chinese, rather than their similarities. Example: sleep ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E7%9D%A...  , hide-and-seek ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E6%8D%8...  p.s. I'm saying t", "negative": "A list of fun destinations for telnet. uff I hope i can list my MUD game (still in dev, though) Oh man RIP towel.blinkenlights.nl 23 The Star Wars ASCII animation was how I learned telnet existed. Felt like discovering a secret passage in the internet. There's something pure about text-based interfaces. No loading spinners, no JavaScript frameworks, no cookie banners. Just text. nethack.alt.org is conspicuously absent... This is insane > doom.w-graj.net 666 > Play Doom in the terminal (code and details) Very cool, some nice nostalgia looking through that list! Missed a trick not being able to \u201ctelnet telnet.org\u201d though. :-) Related to the last Telnet CVE?\nWhy talking about telnet now otherwise? for years I had this in my .muttrc. it's been commented out since it stopped working... #set signature=\"cat ~/.signature && telnet towel.blinkenlights.nl 666 | tail -n3|\" I was wondering why the Starwars one is not at the top of the list. Then I saw it no longer exists :-( Wasted opportunity for a telnet.net or tel.net domain. I can forsee a future when all the AI slop, popups, fake news, propaganda and ads have fully consumed the web. Maybe then we just go back to an oldschool text based way of communicating. No google. No socials. Just text.        ~/work/...> telnet towel.blinkenlights.nl\n    zsh: command not found: telnet   Note that this is much more dangerous than visiting a website. ANSI escape sequences can seriously mess with your system, RCE included. For those of you curious about what the Star Wars one looked like, the tradition lives on here: ssh -p 1977 sw.taigrr.com My first introduction to the internet was through the telnet-based EW-too talkers like Foothills (Boston U) and Forest (UTS). I have very fond memories of staying up late talking to people from all over the globe. It was truly amazing to me. The best part was how the users moderated behaviour - bad actors were ejected swiftly but rarely permanently. The first BBS I used  in the 80's  eventually ende"}
{"anchor": "BERT is just a single text diffusion step. Very cool parallel. Never thought about it this way \u2014 but makes complete sense Fun writeup! It's amazing how flexible an architecture can be to different objectives. When text diffusion models started popping up I thought the same thing as this guy (\u201cwait, this is just MLM\u201d) though I was thinking more MaskGIT. The only thing I could think of that would make it \u201cdiffusion\u201d is if the model had to learn to replace incorrect tokens with correct ones (since continuous diffusion\u2019s big thing is noise resistance). I don\u2019t think anyone has done this because it\u2019s hard to come up with good incorrect tokens. Interested in how this compares to electra To my knowledge this connection was first noted in 2021 in  https://arxiv.org/abs/2107.03006  (page 5). We wanted to do text diffusion where you\u2019d corrupt words to semantically similar words (like \u201cquick brown fox\u201d -> \u201cspeedy black dog\u201d) but kept finding that masking was easier for the model to uncover. Historically this goes back even further to  https://arxiv.org/abs/1904.09324 , which made a generative MLM without framing it in diffusion math. To me, the diffusion-based approach \"feels\" more akin to whats going on in an animal brain than the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken words; I start by having some fuzzy idea in my head and the challenge is in serializing it into language coherently. To me part of the appeal of image  diffusion models was starting with random noise to produce an image. Why do text diffudion models start with a blank slate (ie all \"masked\" tokens), instead of with random tokens? I love seeing these simple experiments. Easy to read through quickly and understand a bit more of the principles. One of my stumbling blocks with text diffusers is that ideally you wouldn\u2019t treat the tokens as discrete but rather probably fields. Image diffusers have the natural property that a pi", "positive": "Eat Real Food. Makes sense to me! And poor diet is probably one of the biggest problems in the United States Makes sense. Now make protein affordable. And 100 years from now, will we still call it the New Pyramid? :) I guess we still call it New York... Great!  How will the reductions in consumer protection, health, FDA, etc. - by this current administration impact that?  https://www.food-safety.com/articles/11004-a-2025-timeline-o...  This website is far too complicated, just show a clear, labeled image of the new pyramid.  This is designed to scare people, not inform them. Lol good one. Anything matching . real .\\.gov$ can be discarded as BS these days... Edit: Actually make that simply .*\\.gov$ It's unbelievable to which point this clown show has permanently dismantled US soft power. Guess they think they have enough hard power to compensate. What with all that good raw milk and meat they're eating... Ironic that a steak is one of the three things showing up on the landing page. Is that the beef lobby money coming in? I enjoy an occasional steak but if the goal is to improve diet of masses, it\u2019s not the food I\u2019d put at the center. \"In February 2010, Michelle Obama launched \u201cLet\u2019s Move!\u201d with a wide-ranging plan to curb childhood obesity. The campaign took aim at processed foods, flagged concerns about sugary drinks, and called for children to spend more time playing outside and less time staring at screens. The campaign was roundly skewered by conservatives... But the strategy that Kennedy\u2019s HHS is using to address the problem so far\u2014pressuring food companies to alter their products instead of introducing new regulations\u2014is the same one that Obama relied on, and will likely fall short for the same reason hers did a decade ago.\"  https://www.theatlantic.com/health/archive/2025/09/maha-lets...  Meta comment: The design aesthetic gives me a real \"Cards Against Humanity\" feel. > Whole grains are encouraged. Refined carbohydrates are not. Prioritize fiber-rich whole g", "negative": "Windows 11's Patch Tuesday nightmare gets worse. How can a company this big fail so hard in what one would consider their main* product still baffles me. *Yes, they probably make more revenue in Azure or Office365 licenses but at least when I think \u201cMicrosoft\u201d I immediately think Windows. > It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been. I think I might know... I'm wondering why the guy at Microsoft in charge of Windows is still employed. Over the prior weekend my installation of Playnite (a catalog/launcher for my games) was broken by the update, until I moved its data off of OneDrive[1]. And the other day I figured out that a couple of icons on my desktop had become completely inert and unresponsive due to the same bug - again due to an interaction between the Windows Shell and OneDrive. And this one I can't fix, I can't shift my desktop out of OneDrive. MS's strategy at this point is that Windows is a loss leader to get people onto the subscriptions for Office and OneDrive. So when the Windows team releases bugs that break usage of those services, forcing people off them onto alternative solutions, the guy in charge of those updates really needs to be answering some tough questions. [1] I've now got SyncThing handling this. Previous discussion: >Microsoft suspects some PCs might not boot after Windows 11 January 2026 Update  https://news.ycombinator.com/item?id=46761061  W11 is the best OS I've ever used, but everyone seems to hate it because Microsoft is so adamant in destroying its reputation by pushing Copilot and bugs instead of focusing on reliability. It's a shame. I see Microslop's \"AI\" coding mandate is continuing to go well [dupe]  https://news.ycombinator.com/item?id=46761061  So, a couple years ago Microsoft was the first large, public-facing software organization"}
{"anchor": "Functors, Applicatives, and Monads. This reminds me of  https://www.adit.io/posts/2013-04-17-functors,_applicatives,...  I think over the recent years, there's been a rise in typed languages that support functional programming like TypeScript and Rust. It will be interesting to see if this trend continues in the context of AI assistant programming. My guess is that it will become easier for beginners, and the type systems will help to build more robust programs in cooperation with AI. the bit at the end is quite rude of the haskeller responding but I also think they're largely right; another monads explained through boxes tutorial is not gonna help anyone. In fact it's really a step in the wrong direction. Using a few different monads is where to start. Unfortunately, while you may not have appreciated the tone of the Haskell interaction, they are correct in their assessment from a factual perspective. This explanation propagates a number of misunderstandings of the topics well known to be endemic to beginners. In particular, I observed the common belief that functors apply to \"containers\", when in fact they apply to things that are not containers as well, most notably functions themselves, and it also contains the common belief that a monad has \"a\" value, rather than any number of values. For instance, the \"list monad\" will confuse someone operating on this description because when the monad \"takes the value out of the list\", it actually does it once  per value  in the list. This is the common \"monad as burrito\" metaphor, basically, which isn't just bad, but is actually wrong. I'm not limiting it to these errors either, these are just the ones that leap out at me. Coming from non-Haskell background, it took me a good while to undestand that `Just` is a constructor specific to the `Maybe` type. Found this for a quite nice answer:  https://stackoverflow.com/a/18809252  For some reason everyone likes to talk about Monads, but really the other types here are just as in", "positive": "GLM-4.7-Flash. Any cloud vendor offering this model? I would like to try it. Not much info than being a 31B model. Here's info on GLM-4.7[0] in general.  I suppose Flash is merely a distillation of that. Filed under mildly interesting for now.  [0]  https://z.ai/blog/glm-4.7  Seems to be marginally better than gpt-20b, but this is 30b? Looks like solid incremental improvements. The UI oneshot demos are a big improvement over 4.6. Open models continue to lag roughly a year on benchmarks; pretty exciting over the long term. As always, GLM is really big - 355B parameters with 31B active, so it\u2019s a tough one to self-host. It\u2019s a good candidate for a cerebras endpoint in my mind - getting sonnet 4.x (x<5) quality with ultra low latency seems appealing. Excited to test this out. We need a SOTA 8B model bad though! Interesting they are releasing a tiny (30B) variant, unlike the 4.5-air distill which was 106B parameters.  It must be competing with gpt mini and nano models, which personally I have found to be pretty weak.  But this could be perfect for local LLM use cases. In my ime small tier models are good for simple tasks like translation and trivia answering, but are useless for anything more complex.  70B class and above is where models really start to shine. Great, I've been experimenting with OpenCode and running local 30B-A3B models on llama.cpp (4 bit) on a 32 GB GPU so there's plenty of VRAM left for 128k context. So far Qwen3-coder gives the me best results. Nemotron 3 Nano is supposed to benchmark better but it doesn't really show for the kind of work I throw at it, mostly \"write tests for this and that method which are not covered yet\". Will give this a try once someone has quantized it in ~4 bit GGUF. Codex is notably higher quality but also has me waiting forever. Hopefully these small models get better and better, not just at benchmarks. I'm trying to run it, but getting odd errors.\nHas anybody managed to run it locally and can share the command? What\u2019s the ", "negative": "Hacker News: Savage Edition.  prompt: in the vein of our classic Gemiini Pro 3 hallucinates the HN front page 10 years from now, or HN front page right now, but the titles are honest. Please scrape the HN front page RIGHT NOW and make honest titles. Here's a preview: <... snip ... >  This is actually a 2-shot. I asked Gemini Pro 3 to turn it up to 11. If you want the less savage, more anodyne 1st-version...I posted that too. Great work, this is funny! I saw a way too soon joke in this vein of savage humor earlier today... \"What do you think of the work ICE is doing in Minnesota?\" Pretti Good. This made me click on several HN posts I totally wouldn't have otherwise. I gotta say, it nails the titles! Great work! As a regular poster for 14 years this is really great and nails the vibe. The \u201cfights\u201d for threads is chefkiss Also \u201cPostgres cult celebrates death of another vector database\u201d was so spot on I looked for the meta post name but looks like it hasn\u2019t updated yet. I\u2019ll be interested to see if there\u2019s a recursion that turns into the singularity An internet posts their own, LLM-generated and therefore far less funny, take on n-gate.com, long after n-gate is dead. Hackernews waxes nostalgic about a site deemed not to promote the kind of discussion they want to see when it was still alive. These HN spoofs are quickly becoming the lowest of form of content slop that people know will get a ton of karma. This has to be  at least  the fifth or sixth in a month or two? C'mon keepamovin. I know you know how easy it is for this type of stuff to get karma. Truly savage. Well done. Lots of laughs.\nWay funnier than my contribution, which is probably why mine hasn't picked up any traction yet.\nWould be interesting to read a brief once your Story hits 50+ comments.  https://news.ycombinator.com/item?id=46765448  ROFL: > Sk\u00e5pa, a parametric 3D printing app like an IKEA manual > Show HN: Hybrid Markdown Editing > Palantir Defends Work with ICE to Staff Following Killing of Alex Pre"}
{"anchor": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence. I am amazed, though not entirely surprised, that these models keep getting smaller while the quality and effectiveness increases. z image turbo is wild, I'm looking forward to trying this one out. An older thread on this has a lot of comments:  https://news.ycombinator.com/item?id=46046916  Flux2 Klein isn\u2019t some generation leap or anything. It\u2019s good, but let\u2019s be honest, this is an ad. What will be really interesting to me is the release of Z-image, if that goes the way it\u2019s looking, it\u2019ll be natural language SDXL 2.0, which seems to be what people really want. Releasing the Turbo/Distilled/Finetune months ago was a genius move really. It hurt Flux and Qwen releases on a possible future implication alone. If this was intentional, I can\u2019t think of the last time I saw such shrewd marketing. > FLUX.2 [klein] 4B The fastest variant in the Klein family. Built for interactive applications, real-time previews, and latency-critical production use cases. I wonder what kind of use cases could be \"latency-critical production use cases\"? If we think of GenAI models as a compression implementation. Generally, text compresses extremely well. Images and video do not. Yet state-of-the-art text-to-image and text-to-video models are often much smaller (in parameter count) than large language models like Llama-3. Maybe vision models are small because we\u2019re not actually compressing very much of the visual world. The training data covers a narrow, human-biased manifold of common scenes, objects, and styles. The combinatorial space of visual reality remains largely unexplored. I am looking towards what else is out there outside of the human-biased manifold. I appreciate that they released a smaller version that is actually open source. It creates a lot more opportunities when you do not need a massive budget just to run the software. The speed improvements look pretty significant as well. 2026 will be the year of small/open model", "positive": "Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete. I read the release but didn't quite understand the difference between a next-edit model and a FIM model - does anyone have a clear explanation of when to use one over the other? I'd love if there was a sublime plugin to utilize this model and try it out, might see if I can figure that out. I use Sweep\u2019s Jetbrains autocomplete plugin daily, it really stands out. Based on qwen2.5-coder? seems like a \"why not/resume embellish/show VC\" type release I guess can it be integrated in monaco editor ? So SFT cost less only low hundreds of dollars? (1-10$ per hour per H100 if I'm seeing this correctly). What about SFT? Presumably basing this of Qwen is the reason it can be done for so cheap? Wow super fun read, I love how it went into the technical details. Any way to make it work with vscode? This is cool! I am more interested in how you guys generated next edit training data from repos, seems like there are lots of caveats here. Would love your insights Again amazing work! waiting for what you guys cook next I'm very green to this so forgive if this question sounds silly: Would instead of the RL step a constrained decoding say via something like xgrammar fix   syntax generation issue ? Do you plan to release Sweep 3B/7B on HF? It's good. The blog post about it is very interesting.\nI hope, a plugin for neovim will be made soon.  https://blog.sweep.dev/posts/oss-next-edit  Followed your work since the beginning and used it for inspiration for some cool demos on self-healing web scrapers. fascinating to see the transition from original concept to producing models. cool stuff. Very interesting - and cool to read about the development process. I'd love to hear more about how genetic algorithm worked here. I wonder whether we are perhaps the point of usefulness of 'next edit' code development in 2026 though. Any easy way to try on vscode? Surprising how badly Jetbrains implemented AI. Apparently to such an extent ", "negative": "Spanish track was fractured before high-speed train disaster, report finds. Wow, that's a really big gap. No wonder it derailed What are the some of the ways that tracks are monitored for fractures like this?  It must have been pretty substantial in order to be described as \"complete lack of continuity\".  Makes me think of literally electronic continuity tests -- are those ever used in this context?  Or how about cameras mounted on trains using image processing?  Or drones? It seems a shame that a few other trains passed beforehand with this anomaly in place and yet it went undetected. My gut feeling says a lot of fatalities could have been prevented with a physical barrier between both tracks. Shouldn't this be mandatory with high speed trains? While these events are statistically very rare, it is worth remembering that there have been two separate events in the past twenty years in Spain where high-speed trains have derailed leading to multiple fatalities [1][2]. In contrast, the Japanese Shinkansen has a spotless record since its introduction in the 1960s [3]. Not a single fatality due to a crash or derailment. And that's in a country with a much larger population and much higher passenger count per year. What do they do differently? [1]  https://en.wikipedia.org/wiki/Santiago_de_Compostela_derailm...  [2]  https://en.wikipedia.org/wiki/2026_Adamuz_train_derailments  [3]  https://en.wikipedia.org/wiki/Shinkansen#Safety_record  I wonder how common it is for train tracks to fracture? And what systems are in place to actually detect this. There was recently a post on a German subreddit where the OP found a fracture in the German rail[0], albeit much smaller. 0.  https://old.reddit.com/r/drehscheibe/comments/1qe9ko2/ich_gl...  AFAIK continuously welded tracks (like those used in high speed rail) are also slightly tensioned, so a break in a single point could make it look like a whole section of track is missing, as tension is released. Some more info from Spanish med"}
{"anchor": "Erdos 281 solved with ChatGPT 5.2 Pro. The erdosproblems thread itself contains comments from Terence Tao:  https://www.erdosproblems.com/forum/thread/281  Has anyone verified this? I've \"solved\" many math problems with LLMs, with LLMs giving full confidence in subtly or significantly incorrect solutions. I'm very curious here. The Open AI memory orders and claims about capacity limits restricting access to better models are interesting too. From Terry Tao's comments in the thread: \"Very nice! ... actually the thing that impresses me more than the proof method is the avoidance of errors, such as making mistakes with interchanges of limits or quantifiers (which is the main pitfall to avoid here). Previous generations of LLMs would almost certainly have fumbled these delicate issues. ... I am going ahead and placing this result on the wiki as a Section 1 result (perhaps the most unambiguous instance of such, to date)\" The pace of change in math is going to be something to watch closely. Many minor theorems will fall. Next major milestone: Can LLMs generate useful abstractions? This must be what it feels like to be a CEO and someone tells me they solved coding. I have 15 years of software engineering experience across some top companies. I truly believe that ai will far surpass human beings at coding, and more broadly logic work. We are very close Out of curiosity why has the LLM math solving community been focused on the Erdos problems over other open problems?  Are they of a certain nature where we would expect LLMs to be especially good at solving them? This is crazy. It's clear that these models don't have human intelligence, but it's undeniable at this point that they have _some_ form of intelligence. FWIW, I just gave Deepseek the same prompt and it solved it too (much faster than the 41m of ChatGPT). I then gave both proofs to Opus and it confirmed their equivalence. The answer is yes. Assume, for the sake of contradiction, that there exists an \\(\\epsilon > 0\\) ", "positive": "Over 36,500 killed in Iran's deadliest massacre, documents reveal. I can't comprehend how a population can kill that many of their own people. They aren't even an \"other\" people, which has been the most common scapegoat lately. Same skin color, same religion, same language, same homeland. For comparison, estimates of the 1989 Tiananmen Square massacre death count are usually put in the 300-1,000 range by journalists and human rights groups.  https://en.wikipedia.org/wiki/1989_Tiananmen_Square_protests...  hm, I think we should re-evaluate sanctioning or civilian pressure campaigns, since the guise is for them to coax or turn on the government for regime change, but the government can just hire mercenaries from outside the country. don't know a solution but this one isn't it The source (Iran International) is backed by Saudi money and has a bias to dunk on Iran. That said, I'm sure the death count numbers from the Rasht Massacre are staggeringly higher than the initial tallies of 2-5k. This is certainly the end of peaceful Iranian protests. Whether it leads to a violent revolution or a static police state like North Korea remains to be seen. How is this possible without explosives? Even with vehicle mounted machine guns it seems like a crazy high number. Did the protestors get boxed in somehow? And across so many locations, that seems to require a crazy amount of coordination to kill so many in so little time. That's crazy. That's like ~40% of the deaths in the current gaza war, except over just 2 days instead of 2 years. This is depressing because we will go to war over this and it\u2019s going to be five years before people realizing they were tricked by \u201cbabies in incubators\u201d propaganda. The internet is fragile. Access can be so easily cut off for the masses in dire times. Take a good look US, because once you're down far enough the fascist drain, that's the cost of trying to claw your way back out. And there's no hope of external intervention given nuclear arms Earlie", "negative": "Show HN: I built a space travel calculator using Vanilla JavaScript. Nice job! It's interesting to see how little effect the orbit and rotation had on the straight line. A proposal is to align the numbers for the different movement categories so that it's easier to see the magnitudes of them. It took me a couple of seconds to understand the concept, from the title I though it was going to be a planner to show gravity assists etc. It's really odd that we're stuck in this fish tank with zero idea where we're flying now, or where we were before. I believe this is vital information for every journey. Life looks much easier when realising that we're all flying at least ~30 km/s through dark space every second of our lives. Thank you so much! I was just thinking about how to create something similar a month ago for my birthday, but didn't succeed like you did. So, how much does the galaxy's travel affect the speed of time? Even if you remember the times of iPod, you can safely say you're less than one light year old. it's cool but i was expecting some kind of visualization, how do my 1.2 trillion km look on a map? also there are some cursors with question marks but they don't espatially ;) call the FAQ, do they? firefox on win10 Geez, gotta love that you have to reach the bottom of the page and read the small print to realize that the date input on the top is supposed to be for your birthday... and then to figure out that the site is a calculator of \"this is how far you've travelled relative to [the center of the universe?] since birth\"... Its vibecoded, I can see this shitty borders and neon that Gemini and other AI tools like so much Just want to comment on the star background. I did something similar for my own site (link in bio). I ended up rendering the stars in a three.js scene because drawing them on a 2D canvas did not look like a satisfying effect. CPU usage was lessened as well, at least on my mobile and two desktops (can't verify your site's CPU utilization sin"}
{"anchor": "Tree-sitter vs. Language Servers. I love tree-sitter+eglot but a few of the languages/schemes I work in, simply don't have parsers:       > pacman -Ssq tree-sitter\n    tree-sitter\n    tree-sitter-bash\n    tree-sitter-c\n    tree-sitter-cli\n    tree-sitter-javascript\n    tree-sitter-lua\n    tree-sitter-markdown\n    tree-sitter-python\n    tree-sitter-query\n    tree-sitter-rust\n    tree-sitter-vim\n    tree-sitter-vimdoc\n\n  \nWhere's R, YAML, Golang, and several others? This is like the difference between an orange and fruit juice. You can squeeze an orange to extract its juices, but that is not the only thing you can do with it, nor is it the only way to make fruit juice. I use tree-sitter for developing a custom programming language, you still need an extra step to get from CST to AST, but the overall DevEx is much quicker that hand-rolling the parser. Side note, but thanks for the note about not using AI to write your articles. I'm tired of looking for information online, finding an article that may answer it, and not being sure about the author's integrity (this is so rampant on Medium). I think the big reason to put syntax highlighting in the language server is you have more info, ex you can highlight symbols imported from a different file in one color for integers and a different for functions Tree-sitter is great. It powers Combobulate in Emacs. Structured editing and movement would not have been easily done without it. >It is possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or not want to) do this. Hmm, the strong reason could be latency and layout stability. Tree-sitter parses on the main thread (or a close worker) typically in sub-ms timeframes, ensuring that syntax coloring is synchronous with keystrokes. LSP semantic tokens are asynchronous by design. If you rely solely on LSP for highlighting, you introduce a flash of unstyled content or color-shifting artifacts every time yo", "positive": "My trick for getting consistent classification from LLMs. If you already have your categories defined, you might even be able to skip a step and just compare embeddings. I wrote a categorization script that sorts customer-service calls into one of 10 categories.  Wrote descriptions of each category, then translated into embedding. Then created embeddings for the call notes and matched to closest category using cosine_similarity. Arthur\u2019s classifier will only be as accurate as their retrieval. The approach depends on the candidates to be the correct ones for classification to work. Under-discussed superpower of LLMs is open-set labeling, which I sort of consider to be inverse classification. Instead of using a static set of pre-determined labels, you're using the LLM to find the semantic clusters within a corpus of unstructured data. It feels like \"data mining\" in the truest sense. Dunno if this passes the bootstrapping test. This is sensitive to the initial candidate set of labels that the LLM generates. Meaning if you ran this a few times over the same corpus, you\u2019ll probably get different performance depending upon the order of the way you input the data and the classification tag the LLM ultimately decided upon. Here\u2019s an idea that is order invariant: embed first, take samples from clusters, and ask the LLM to label the 5 or so samples you\u2019ve taken. The clusters are serving as soft candidate labels and the LLM turns them into actual interpretable explicit labels. I think a less order biased, more straightforward way would be just to vectorize everything, perform clustering and then label the clusters with the LLM. Nice! So the cache check tries to find if a previously existing text embedding has >0.8 match with the current text. If you get a cache hit here, iiuc, you return that matched' text label right away. But do you also insert a text embedding of the current text in the text embeddings table? Or do you only insert it in case of cache miss? From reading the ", "negative": "Ask HN: What's the current best local/open speech-to-speech setup?. It was a little annoying getting old qt5 tools installed but I really enjoyed using dsnote / Speech Note. Huge model selection for my amd gpu. Good tool. I haven't done enough specific studying yet to give you suggestions for which model to go with. WhisperFlow is very popular. Kyutai some very interesting work always. Their delayed streams work is bleeding edge & sounds very promising especially for low latency. Not sure why I have not yet tried it tbh.  https://github.com/kyutai-labs/delayed-streams-modeling  There's also a really nice elegant simple app Handy. Only supports Whisper and Parakeet V3 but nice app & those are amazing models.  https://github.com/cjpais/Handy  You should look into the new Nvidia model:  https://research.nvidia.com/labs/adlr/personaplex/  It has dual channel input / output and a very permissible license Anyone using any reasonably good small speech to text os models? For the TTS part:  https://github.com/supertone-inc/supertonic  It requires a bit of tinkering, but I think pipecat is the way to go.  You can plug in pretty much any STT/LLM/TTS you want and go.  It definitely supports local models but its up to you to get your hands on those models. Not sure if there's any turnkey setups that are preconfigured for local install where you can just press play and go though. Last I heard E2E speech to speech models are still pretty weak.  I've had pretty bad results from gpt-realtime and that's a proprietary model, I'm assuming open source is a bit behind.  https://handy.computer  got good marks from a  very  nontechnical user in my life this week! Local, FOSS Tangential: What hardware are you using for the interface on these?  Is there a good array microphone that performs on par with echos/ghomes/homepods? I have used  https://github.com/SaynaAI/sayna  . What I like the most is that you can switch between the providers easily and see what works for you the best. It also su"}
{"anchor": "Indefinite Backpack Travel. >  Onebag travel is unquestionably the best way to travel. Traveling without luggage removes just about every pain point associated with flying, such as checking bags, overhead compartments, bag fees, waiting in line, and needing to drop off luggage before an adventure. Just stroll into the airport an hour before your flight, and walk off your plane directly to your destination.  This is absolutely true, especially when traveling solo. A rather depressive color pallet, pick your black, silver or white :) I'm going to steal their approach at rolling up clothes, though. I lived out of a backpack for two months on a Pacific Crest Trail hike. I got comfortable with it and told myself that I had overcome my materialism, and could henceforth live happily without a lot of stuff and conveniences. Not so much. Now a couple of decades later, I've got a house and garage crammed with stuff. Yesterday I had a plumber here working on a leak, and this morning I have no running water, and here I am bravely holding back tears. My inner dialog is \"this is unacceptable!\" It turns out that climbing on the hedonic treadmill is practically effortless, but sliding down it is full of splinters. Its  SO ANNOYING  to have to carry an  Apple Silicon macbook AND an ipad. I'd love to see a touchscreen option for macbook and the option to run in ipad mode.  But that would probably cannibalize sales. As it is, you can theoretically run ios apps on Apple Silicon, but most app vendors disable that.. My main use case for an ipad while traveling is to watch downloaded movies on a plane. \"AR\" (not really) glasses like nreal air are way smaller and lighter than an ipad and makes watching movies on my phone pretty amazing.. I also recommend the 3F UL Gear tents. I bought the Taiji 2 for motorcycle trips. Very roomy for one person.  https://3fulgear.com/product/freestanding-tent/taiji-2/  I love this post but ain\u2019t no way I\u2019m going minimalist and carrying a MacBook AND an iPad", "positive": "Show HN: OpenTimes \u2013 Free travel times between U.S. Census geographies. Looks cool. Please allow high max zoom levels, it\u2019s hard to see individual street details on mobile. Amazing! GitHub actions to compute a giant skim matrix is an incredible hack. I pretty regularly work with social science researchers who have a need for something like this... will keep it in mind. For a bit we thought of setting something like this up within the Census Bureau, in fact. I have some stories about routing engines from my time there... Well done, dfsnow! * some islands seem hamstrung by the approach - see Vashon Island for example. * curious what other dataset you might incorporate for managing next level of magnitude smaller trips - e.g. getting a quarter mile to the store for a frozen pizza at the seventh inning stretch. Any plans on adding public transit? It seems that it ignores bridges over rivers making the travel time wildly inaccurate. This is great! I've been thinking about building something like this for ages since I started using Smappen [0] for mapping travel times on road trips. Super useful way to travel if you're on an open-ended trip with flexibility. [0]  https://www.smappen.com/  OK the way you're publishing the data with Parquet and making it accessible through DuckDB is  spectacular . Your README shows R and Python examples:  https://github.com/dfsnow/opentimes?tab=readme-ov-file#using...  I got it working with the `duckdb` terminal tool like this:     INSTALL httpfs;\n  LOAD httpfs;\n  ATTACH 'https://data.opentimes.org/databases/0.0.1.duckdb' AS opentimes;\n\n  SELECT origin_id, destination_id, duration_sec\n    FROM opentimes.public.times\n    WHERE version = '0.0.1'\n        AND mode = 'car'\n        AND year = '2024'\n        AND geography = 'tract'\n        AND state = '17'\n        AND origin_id LIKE '17031%' limit 10;   Travel time context in general could be useful for retrieval before ranking in searches like Yelp or Google maps like products for nearby events a", "negative": "3D printing my laptop ergonomic setup. That's a really interesting concept.  Either once they open source their build (or I get over my innate laziness) I could use something like that for my build at home (more of a horizontal stand style thing, looking at the monitor) - my laptop's primarily my second monitor at home with attached KB. More fuel to help convince my wife the printer isn't a waste of money xD I'd cut off the numberpad of my laptop, center the touchpad and what's left of the keyboard. That would be my ergonomic setup. That laptop served me well but it was a compromise between several factors. I think that at the time there were only an handful of 15\" laptops without a numberpad and probably it's still like that. I eventually had to give up on that to get other features. Quite cool! I wonder if it doesn't wear down the laptop hinges to keep it at 180 degrees opened in an upright position. Could print some clamps for the sides to reduce strain if that's the case. Though that'd only work for laptops that actually _do_ open 180 degrees, according to TFA, not that many. I have a \"car desk\", which is just a little expandable contraption you hang on the steering wheel, then you can place your laptop on it. I wouldn't call it ergonomic per se (the right external keyboard could probably fix that), but using it for about one hour per week, it works well and doesn't cause any issues I'm aware of. The driver seat is not a place where I previously could get any work done, so the bar is a bit low. She should have a look at the Huawei Matebook. You can transform the screen into a big desktop sized screen. The only thing missing is a stand that brings it to an ergonomic height. Very cool, nice effort and a good write-up! If my math is right it seems the cost in material for the printed part is around $5 which seems ridiculously cheap for a custom-designed and adapted solution like this. Nice! I wish the author had spent a few words extra to motivate why it needs to b"}
{"anchor": "Google engineer says Claude Code built in one hour what her team spent a year on. in that one year, more was accomplished than writing a body of code. people learned, explored concepts, and discovered lateral associations, developed collective actions, consolidated future solidarity. claude just output some code. Says more about Google's engineers than Claude Code IMO. \"I'm not joking and this isn't funny. We have been trying to build distributed agent orchestrators at Google since last year. There are various options, not everyone is aligned... I gave Claude Code a description of the problem, it generated what we built last year in an hour.\"  https://x.com/rakyll/status/2007239758158975130  This just shows Google engineering is a corporate shithole. It's time to sell Google stocks. I\u2019m deeply skeptical of these claims. Every time someone says \u201cAI built in an hour what took us a year,\u201d what they really mean is that humans spent a year doing the hard thinking and the AI merely regurgitated it at silicon speed. Which is, of course, completely different from productivity. Also, if it truly took your team a year, that probably says more about your process than about AI. But not in a way that threatens my worldview. In a different way. A safer way. Let\u2019s be clear: writing the code is the easy part. The real work is the meetings, the alignment, the architectural debates, the Jira grooming, the moral struggle of choosing snake_case vs camelCase. Claude didn\u2019t do any of that. Therefore it didn\u2019t actually do anything. I, personally, have spent years cultivating intuition, judgment, and taste. These are things that cannot be automated, except apparently by a probabilistic text model that keeps outperforming me in domains I insist are \u201csubtle.\u201d Sure, the output works. Sure, it passes tests. Sure, it replaces months of effort. But it doesn\u2019t understand what it\u2019s doing. Unlike me, who definitely understands everything I copy from Stack Overflow. Also, I tried AI last year and it", "positive": "London saw a surprising benefit to ultra-low emissions zone: More active kids. I don't believe for a second that the reduced emissions are enough for these kids to actually notice. ULEZ is a tax on being poor, nothing more. I wish the article stated if the amount of cars traveling in the zone remained the same. I would think it probably greatly reduced the amount of traffic in that area, which all around just makes for a more pleasant experience being a pedestrian, biker, or scooterer. Regardless, I think this is awesome and wish it could be tried in the United States. Kids being able to be independent and active is essential to their happiness and development. My 2c as a local: a significant issue with any discussion of this is that people don't really have a good handle on the actual statistics of who drives in London. It cuts across every demographic. Under 25k household income - a good 40-50% of households have a car. Housing estates - tons of cars. Well off - almost everyone.  https://content.tfl.gov.uk/technical-note-12-how-many-cars-a...  It mostly comes down to whether someone has a need (e.g. has children, fairly mobile in their job, has family outside of town, enjoys going on road trips etc) and actually wants to pay for it rather than anything else. In addition to that, a bunch of stuff happened basically at the same time. We got ULEZ, we got a ton of low traffic neighbourhoods (e.g. streets where cars are not allowed at certain times of day regardless of emissions), we had COVID meaning that habits and demographics changed, we had Brexit which probably had some minor effect, etc. All of that happened within about 5 years and I don't think you can isolate any of them. I don't really find most discussions about it interesting as a result of all of the above - it usually just ends up with someone trying to find evidence for their pre-existing position rather than anything that feels actually scientific, unfortunately. \"Their annual health assessments\". Is t", "negative": "Second Win11 emergency out of band update to address disastrous Patch Tuesday. Is it only cloud storage files? I've noticed that in 2026 my windows 11 machine is slower than ever before, by a lot- barely able to render web pages. The start menu search is turning blank and shows a white screen whenever I search anything. Similar to how react apps break. It's been like this for 6 momths, across two laptops, fresh install of 25h2. I for one am enjoying my last few months of Windows 10, stable, responsive, no surprise updates at last. I was hit by this. Could RDP into machines using the regular client, but could not access Dev Boxes via Windows App. Getting real sick of the low quality AI slop. Using React in core parts of the Windows Shell, Microsof's inability to design and release an application using non-web technologies, and the sluggishness and lagginess and bloat of Windows in general has finally pushed me to dual boot Fedora on a separate drive. It is very nice having an Operating system that respects the Hardware I own and makes efficient use of it. My experience has been very good so far. Every device in my custom built desktop PC worked immediately. The only driver I had to build and install was for my XBOX Wireless dongle. Gaming has been really damn good. I installed Steam and my games just worked. No fiddling around with configs or anything. Even installing a custom Proton version to try it out is very simple. I've been on Fedora now for nearly a month and only boot into Windows for work. Eventually, I might get rid of Windows entirely. It'll take a massive U-turn from Microsoft on the philosophy for Windows for me to change my opinion now. And this is why I'm still running Win10 LTSC. No bloat, super fast, still gets security updates. People are blaming vibe coding but the real culprit was hiring leetcoders in the first place. I genuinely believe the stark decrease in quality of most products across the industry has been driven by that. it is so annoying "}
{"anchor": "Maze Algorithms (2017). I've always known about algorithms that solve mazes, but never about actually making them.  It's interesting seeing all these algorithms and how the mazes they generate look different. Mike Bostock had several very lovely visualizations back on the D3.js site which I can't find. Here's a cool blogpost he wrote:  https://bost.ocks.org/mike/algorithms/#maze-generation  Is it known which algorithms produce 'difficult' mazes? I'm imagining you could run all the maze solving algorithms against all the maze generating algorithms many times, and then calculate what the Nash equilibrium would be if the solver is trying to minimise expected time and the generator is trying to maximise it. For anyone interested in this, Jamis Buck's book 'Mazes for Programmers' is a masterpiece of the genre. My personal favorite distinction is between the Recursive Backtracker (which creates long, winding corridors with few dead ends which is great for tower defense games) vs. Prim's Algorithm (which creates lots of short cul-de-sacs which is better for roguelikes). The bias of the algorithm dictates the feel of the game more than the graphics do. seconding the jamis buck book, its one of the few programming books i actually finished. the way he explains each algorithm with visualizations makes it stick It feels like many of the more complicated algorithms produce worse mazes (long horizontal/vertical walls, many 1-2 square dead ends next to another) than basic recursive backtracking. Related:  Maze Generation: Recursive Division (2011)  -  https://news.ycombinator.com/item?id=42703816  - Jan 2025 (12 comments)  Maze Algorithms (2011)  -  https://news.ycombinator.com/item?id=23429368  - June 2020 (22 comments)  Representing a Toroidal Grid  -  https://news.ycombinator.com/item?id=10608476  - Nov 2015 (2 comments)  Maze Generation: Recursive Backtracking  -  https://news.ycombinator.com/item?id=4058525  - June 2012 (1 comment)  Maze Generation: Weave mazes  -  https://n", "positive": "Vanguard's average fee is now 0.07% after biggest-ever cut. Not mentioned in any of the coverage I've seen (or the interview with Vanguard's new CEO in the WSJ) is Fidelity. Fidelity used to be known for actively managed funds, but has been eating Vanguard's indexing lunch for the past 10 years or so. Part of this relates to its dominance in workplace accounts, but Vanguard hasn't helped itself with some bad customer-facing software updates and a perception that its service levels are poor compared to Fidelity. Cutting fees helps, but Fidelity has shown its willing to do this, too, including no fee \"Zero\" index funds:  https://www.fidelity.com/mutual-funds/investing-ideas/index-...  (note Fidelity is very clear about who it's competing with) Article mentions their bond funds getting the most dramatic cuts \u2014 they didn't list specific symbols though. Anyone know off the top of their heads which funds specifically? Thinking I need to move away from being so stock-heavy. I always upvote the archive link unless it is already the top comment, ha ha. That only applies to US funds, but not in the UK ones which continue to be significantly more expensive... Didn't they recently increase uk fees a tonne? Straight from the source:  https://corporate.vanguard.com/content/corporatesite/us/en/c...  Let's not forget that Vanguard has taken a strong stance against crypto [0]. Claiming to significantly invest in technology while deliberately ignoring the latest advancements in financial technology, seems contradictory. If their business was doing so well, they wouldn't have to lower fees. [0]  https://news.ycombinator.com/item?id=42832026  Someone correct my math here, but if they have 10 trillion in assets under management and the management fee is 0.07% then that's still 7,000,000,000 or 7 billion in fees every year? Not bad Unless you have some super special edge, Vanguard is really good IMO. Having a 0.01% or 0.05% fund is really as good as you can do and never pay attention. Va", "negative": "JRR Tolkien reads from The Hobbit for 30 Minutes (1952). This is so good.  You can tell that Andy Serkis based his gollum voice off of this. I wonder what Tolkien would say of so much of the symbolism from his novels being used to bootstrap a horrible dystopian control grid? Would he approve or disapprove? The way that orcs are dehumanized you have to wonder. Is there a version minus the music? This is the most magnificent audio version ever recorded of The Hobbit - by Nicol Williamson in the early 1970's. Zip file with mp3 in it:  https://drive.google.com/file/d/1b2aPKgVVguOKMOOqWskaliOviYr...  Best enjoyed on a rainy afternoon in an armchair with a cup of tea. Of course he didn't live to see the Peter Jackson movies but I think I've heard his son didn't like them My favorite recent LotR media: There is a Lord of the Rings MMO (like World of Warcraft) and a guy made a video recording a walk from the Shire to Mordor. Like you can just walk from the Shire to Mordor in the game. And it's almost 10 hours long in real world time to do that! But on top of that the whole journey is narrated by the Lord of the Rings audio book, with the relevant parts of the journey.  https://youtu.be/LYipECdYpXc  Incredibly relaxing People who don't like \"On The Road\" should listen to Jack read it in his own voice. It is amazing how Lord of the Rings persists in the world. Christopher Lee reading the Children of Hurin is also fantastic. tangential comment, but if anyone is interested in one of the best (imo \"legendary\") audio books on LOTR look no further than Phil Dragash :  https://archive.org/details/tlotrunabridged . Okay who\u2019s going to clone this using AI and have it read the entire book? Anyone?! I drink in his old local. A bit weird in there I would imagine if you're an American. Although I am a bit American and it is a bit weird in there. Ranged Touch's Shelved By Genre podcast is doing an entire year on The Hobbit + Lord of the Rings.\n https://rangedtouch.com/2026/01/02/the-hobbi"}
{"anchor": "Show HN: isometric.nyc \u2013 giant isometric pixel art map of NYC. Appreciate that writeup. Very detailed insights into the process. However those conclusions left me on the fence about whether I 'liked' the project. The conclusions about 'unlocking scale' and commodity content having zero value. Where does that leave you and this project? Does it really matter that much that the project  couldn't  exist without genAI? Maybe it shouldn't exist then at all. As with alot of the areas AI touches, the problem isn't the tools or use of them exactly, it's the scale. We're not ready for it. We're not ready for the scale of impact the tech touches in multitude of areas. Including the artistic world. The diminished value and loss of opportunities. We're not ready for the impacts of use by bad actors. The scale of output like this, as cool as it is, is out of balance with the loss of huge chunk of human activity and expression. Sigh. Very impressive result! are you taking requests for the next ones?  SF :D Tokyo :D Paris :D Milan :D Rome :D Sydney :D Oh man... > This project is far from perfect, but without generative models, it couldn\u2019t exist. There\u2019s simply no way to do this much work on your own, Maybe, though a guy did physically carve/sculpt the majority of NYC:  https://mymodernmet.com/miniature-model-new-york-minninycity...  I see you used Gemini-CLI some but no mention of Antigravity. Surprising for a Googler. Reasons? > Slop vs. Art > If you can push a button and get content, then that content is a commodity. Its value is next to zero. > Counterintuitively, that\u2019s my biggest reason to be optimistic about AI and creativity. When hard parts become easy, the differentiator becomes love. Love that.  I've been struggling to succinctly put that feeling into words, bravo. Not working here, some CORS issue. Firefox, Ubuntu latest. Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at  https://isometric-nyc-tiles.cannoneyed.com/dzi/tiles_me", "positive": "Succinct data structures. Wow, this is really fascinating. I guess it all comes down to how it's doing select and rank in constant time, which is probably some clever bit arithmetic. I'll have to look into how that works. I first heard of the concept of succinct data structures from Edward Kmett, a famous Haskeller behind many popular Haskell libraries. He gave a talk on succinct data structures a long time ago:  http://youtu.be/uA0Z7_4J7u8  I really like the article, but it would benefit from some numbers or complexity estimates to get some intuitive sense of what the cost is. Am I paying 30% overhead for this particular index or that wavelet matrix? Is it double the memory use? Or is it O(log N)? No idea! \"doesn't use much more space\" could mean a lot of different things! Succinct data structures are very fun! If anyone is interested, I've implemented some of this in Zig:  https://github.com/judofyr/zini . The main thing this implements is a minimal perfect hash function which uses less than 4 bits per element (and can respond to queries in ~50 ns). As a part of that I ended up implementing on of these indexes for constant-time select(n):  https://github.com/judofyr/zini/blob/main/src/darray.zig . It feels kinda magic implementing these algorithms because everything becomes  so tiny ! The word count seems artificially increased in the post. Here's a succinct explanation:  https://www.eecs.tufts.edu/~aloupis/comp150/projects/Succinc...  My goto library for succinct data structures is SDSL-Lite [0]. [0]  https://github.com/simongog/sdsl-lite  Note that succinct data structures may not be faster than conventional structures if your dataset fits in memory  http://www.cs.cmu.edu/~huanche1/slides/FST.pdf  . Of course, for large datasets where storage access times dominate, succinct data structures win all around. In any case, succinct trees are works of art (If I recall  https://arxiv.org/pdf/1805.11255  was a good exposition) (just look at how that RMQ works)! Way bett", "negative": "Apple, What Have You Done?. Specifically this System Data issue is big problem but I read online about it and tried stupid fix: set time to far future. Supposedly will expire this system data caches. Nonsense, I said. It is foolish to make cache so big it does not allow update to download. But I did it nonetheless and system data reduced! So crazy is real. I am glad at least that Apple has not forced me to update my iPhone 13 and 2023 Macbook, as Windows would have by now. I am hoping to ride this out, and that a later bundled update will remedy the worst complaints. The most alarming thought in TFA, though, was that the iPhone update might have at least a secondary mission of nudging the user to buy a new phone - certainly not an unknown tactic in tech. > \"My iPhone 14 Pro has 35Gb of \"system data\" which has basically filled up the entire storage I had left\" I occasionally use a macbook pro at \u00a3WORK for a few apple specific processes, and it currently has 188.67gb of \"system data\" that I have no idea how to clean up or remove. It's marked separately from the 11.01gb of macOS in the storage settings, and it constantly complains about the disk almost being full. Updating and restarting don't clear it, I wish I could just rm -rf it all. Does anyone know how I can at least see what it is, and potentially even clean it up? EDIT: Thanks for the CleanMyMac recommendations, the 57.6gb of xcode caches that didn't show up in the \"developer\" section of the storage settings might have had something to do with it At this point I\u2019m going to hold out on updating MacOS for a year. If things don\u2019t improve or the direction doesn\u2019t change significantly I\u2019m going to seriously consider paying the switching costs. My days of not believing people's gushing praise about \"just works\" about any proprietary technology are certainly coming to a middle I have 70GB of \"Messages\" on my Mac because iMessage \"in the cloud\" still stores all your attachments locally on every device. Yes, I can set t"}
{"anchor": "50 Years of Travel Tips. I don't agree with all of this though I do think most of it  can  be good advice. I did a huge amount of travel, mostly of particular styles, latterly when I was working. Still do a fair bit though I'm trying to spend less time on flights and more on destinations. The main thing I didn't see in there although I may have missed it or it may have been implied is travel light. You can't always go with carry-on pack of some sort if you have varied trips, e.g. smart clothing plus hiking kit. But you can probably go lighter than you think. I know I'm mostly at lightweight travel than I used to be. > If you hire a driver, or use a taxi, offer to pay the driver to take you to visit their mother. Uhh, I really can't imagine this one working well in a Western country. I spent 9 months traveling from Mexico to Buenos Aires with a backpack eighteen years ago.  Spent most nights in hostels in shared rooms for a few dollars a night. It was a great experience. Carried a MacBook Pro and a digital Nikon D70.  Actually had the first iPhone but hardly used it. Do have a selfie of myself on a bus somewhere in Central America. These days I\u2019m taking more expensive vacations in cheaper countries. You can go to the Caribbean and stay for $2000 a night or go to places like Morocco or Panama on a  luxury  vacation for 1/3 the price. Wow, that's actually a really good list. I'd add if a journalist has done it, you can do it. Search HN for you location (which also has most of Atlas Obscura in it) This is great advice. On his 'laser out' approach, I often find after travel I am tired and I _really_ don't want to spend hours more getting to where I'm really going, so I usually stop in the city that I landed in. But I have a policy: never go to sleep without going to walk in the city. That is: never land and sleep. _Always_ absorb some of the local environment. Then when your brain knows it's somewhere else, then go to sleep. This has worked to varying degrees. I always w", "positive": "Gemini 3. Feeling great to see something confidential - Anyone have any idea why it says 'confidential'? - Anyone actually able to use it?  I get 'You've reached your rate limit. Please try again later'.  (That said, I don't have a paid plan, but I've always had pretty much unlimited access to 2.5 pro) [Edit:  working for me now in ai studio] How long does it typically take after this to become available on  https://gemini.google.com/app  ? I would like to try the model, wondering if it's worth setting up billing or waiting. At the moment trying to use it in AI Studio (on the Free tier) just gives me \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" It's available to be selected, but the quota does not seem to have been enabled just yet. \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" \"You've reached your rate limit. Please try again later.\" Update: as of 3:33 PM UTC, Tuesday, November 18, 2025, it seems to be enabled. It seem that Google doesn't prepare well to release Gemini 3 but leak many contents, include the model card early today and gemini 3 on aistudio.google.com it is live in the api > gemini-3-pro-preview-ais-applets > gemini-3-pro-preview API pricing is up to $2/M for input and $12/M for output For comparison:\nGemini 2.5 Pro was $1.25/M for input and $10/M for output\nGemini 1.5 Pro was $1.25/M for input and $5/M for output It generated a quite cool pelican on a bike:  https://imgur.com/a/yzXpEEh  And of course they hiked the API prices Standard Context(\u2264 200K tokens) Input $2.00 vs $1.25  (Gemini 3 pro input is 60% more expensive vs 2.5) Output $12.00 vs $10.00 (Gemini 3 pro output is 20% more expensive vs 2.5) Long Context(> 200K tokens) Input $4.00 vs $2.50   (same +60%) Output $18.00 vs $15.00  (same +20%) When will this be available in the cli? Not the preview crap again.\nHaven't the", "negative": "Show HN: Rails UI. Is this another Tailwind wrapper? Yes, it is. ugh this looks dated even by 2016 standards when will developers learn UI actually matters bootstrap was a mistake, and lowered the bar for everyone i don't get these types of products anymore. i think they're useful in their own way, but i can literally create styles with claude/gemini in a heartbeat and not have to pay some insane fee. I think you missed a trick not naming it Railwind UI. I used this about a year ago when I went through a short Rails phase. I was a bit surprised not to see more Rails-specific UI libraries considering how batteries-included the rest of the framework is, and at the time I didn't really 'get' tailwind. I'm not in a Rails phase anymore, but nice work on the library! maybe I'm just dumb but a lot of these elements don't seem to work? the \"...\" buttons don't open any flyout, the dropdown doesn't open up... otherwise looks cool though I wish I could use this \u2013 unfortunately UI frameworks are a political problem at every company I've worked at. The designers feel undermined or threatened by it, and product owners want to dictate design. Despite the massive productivity benefits of a UI framework, I've never been able to convince stakeholders to actually adopt one. If you\u2019re showing off a UI framework, I shouldn\u2019t be accidentally scrolling left and right on the page on mobile / my iPhone. Couldn\u2019t be bothered to scroll down the page to look at components while accidentally activating horizontal scrolling. Broken in Safari on iphone. For example: - table background moves left when table is scrolled horizontally - actions in table and dropdown do nothing on tap - text on buttons is selectable (really?) im always surprised that Rails is still relevant i havent used it since 2006 opting for php and django i might give it another shot, any reason you like this more than django or other frameworks I have hardware acceleration disabled in Firefox and my 5800X spins up trying to rend"}
{"anchor": "London saw a surprising benefit to ultra-low emissions zone: More active kids. I don't believe for a second that the reduced emissions are enough for these kids to actually notice. ULEZ is a tax on being poor, nothing more. I wish the article stated if the amount of cars traveling in the zone remained the same. I would think it probably greatly reduced the amount of traffic in that area, which all around just makes for a more pleasant experience being a pedestrian, biker, or scooterer. Regardless, I think this is awesome and wish it could be tried in the United States. Kids being able to be independent and active is essential to their happiness and development. My 2c as a local: a significant issue with any discussion of this is that people don't really have a good handle on the actual statistics of who drives in London. It cuts across every demographic. Under 25k household income - a good 40-50% of households have a car. Housing estates - tons of cars. Well off - almost everyone.  https://content.tfl.gov.uk/technical-note-12-how-many-cars-a...  It mostly comes down to whether someone has a need (e.g. has children, fairly mobile in their job, has family outside of town, enjoys going on road trips etc) and actually wants to pay for it rather than anything else. In addition to that, a bunch of stuff happened basically at the same time. We got ULEZ, we got a ton of low traffic neighbourhoods (e.g. streets where cars are not allowed at certain times of day regardless of emissions), we had COVID meaning that habits and demographics changed, we had Brexit which probably had some minor effect, etc. All of that happened within about 5 years and I don't think you can isolate any of them. I don't really find most discussions about it interesting as a result of all of the above - it usually just ends up with someone trying to find evidence for their pre-existing position rather than anything that feels actually scientific, unfortunately. \"Their annual health assessments\". Is t", "positive": "Tmux \u2013 The Essentials (2019). it's missing changing Ctrl+B to Ctrl+A:       # ~/.tmux.conf\n    set-option -g prefix C-a   I like tmux a lot, but like its predecessor \"screen\" I mostly use it for explicitly running long-lived jobs (i.e. for its detach feature), and for very special situations where I have elaborate tmux window configurations with dedicated stuff running in each window/pane. Note that I have been using text-only terminals since the 1980s, but I've adapted my tty usage over time. The problem that tmux (or screen) brings are first and foremost: * Smooth/fast scrolling goes away. I can no longer give my trackpad a slight push to find myself tens or hundreds of lines in the scrollback history, and visually scan by slightly pushing my fingers back and forth. Instead I have to use the horrendous in-tmux scrollback using \"Ctrl-b [\". * My terminal app's tabs and windows are not tmux's tabs and windows. I cannot freely arrange them in space, snap them off with the mouse, easily push them to another desktop, and so on. I have to start a multiple tmux clients and do awkward keyboard interactions with them for any of the same. * tmux's terminal emulation and my terminal emulator's terminal emulation (heh) are not congruent. As a result, programs cannot make full use of my actual terminal's capabilities. For example selecting, copying, and pasting text sometimes behave weirdly, and there are other annoyances. What I'd  really  like to have instead is terminal session management at a higher level, i.e. involving my actual graphical terminal app itself. Attaching to a running session would mean restoring the terminal app's windows and tabs, and the entire scrollback history within (potentially with some lazy loading). tmux could likely be a major part of that, by providing the option of replacing its tty-facing frontend with a binary protocol that the graphical terminal app talks to, while keeping the backend (i.e. the part that provides the tty to anything running ", "negative": "Rust\u2019s Standard Library on the GPU. I feel like the title is a bit misleading. I think it should be something like \"Using Rust's Standard Library from the GPU\". The stdlib code doesn't execute on the GPU, it is just a remote function call, executed on the CPU, and then the response is returned. Very neat, but not the same as executing on the GPU itself as the title implies. How different is it from rust-gpu effort? UPDATE: Oh, that's a post from maintainers or rust-gpu. Can I execute FizzBuzz and DOOM on GPU? Are there any details around how the round-trip and exchange of data (CPU<->GPU) is implemented in order to not be a big (partially-hidden) performance hit? e.g. this code seems like it would entirely run on the CPU?       print!(\"Enter your name: \");\n    let _ = std::io::stdout().flush();\n    let mut name = String::new();\n    std::io::stdin().read_line(&mut name).unwrap();\n  \nBut what if we concatenated a number to the string that was calculated on the GPU or if we take a number:       print!(\"Enter a number: \");\n    [...] // string number has to be converted to a float and sent to the GPU\n    // Some calculations with that number performed on the GPU\n    print!(\"The result is: \" + &the_result.to_string()); // Number needs to be sent back to the CPU\n\n  \nOr maybe I am misunderstanding how this is supposed to work? I'm confused about this: As the article outlines well, Std Rust (over core) buys you GPOS-provided things. For example:     - file system\n  - network interfaces\n  - dates/times\n  - Threads, e.g. for splitting across CPU cores\n  \nThe main relevant one I can think which applies is an allocator. I do a lot of GPU work with rust: Graphics in WGPU, and Cuda kernels + cuFFT mediated by Cudarc (A thin FFI lib). I guess, running Std lib on GPU isn't something I understand. What would be cool is the dream that's been building for decades about parallel computing abstractions where you write what looks like normal single-threaded CPU code, but it automagically "}
{"anchor": "Show HN: Use Claude Code to Query 600 GB Indexes over Hacker News, ArXiv, etc.. Seems very cool, but IMO you\u2019d be better off doing an open source version and then hosted SAAS. Really useful currently working on a autonomous academic research system [1] and thinking about integrating this. Currently using custom prompt + Edison Scientific API. Any plans of making this open source? [1]  https://github.com/giatenica/gia-agentic-short  I like that this relies on generating SQL rather than just being a black-box chat bot. It feels like the right way to use LLMs for research: as a translator from natural language to a rigid query language, rather than as the database itself. Very cool project! Hopefully your API doesn't get exploited and you are doing timeouts/sandboxing -- it'd be easy to do a massive join on this. I also have a question mostly stemming from me being not knowledgeable in the area -- have you noticed any semantic bleeding when research is done between your datasets? e.g., \"optimization\" probably means different things under ArXiv, LessWrong, and HN. Wondering if vector searches account for this given a more specific question. That's just not a good use of my Claude plan. If you can make it so a self-hosted Lllama or Qwen 7B can query it, then that's something. Nice, but would you consider open-sourcing it? I (and I assume others) are not keen on sharing my API keys with a 3rd party. Is the appeal of this tool its ability to identify semantic similarity? \"Claude Code and Codex are essentially AGI at this point\" Okaaaaaaay.... > a state-of-the-art research tool over Hacker News, arXiv, LessWrong, and dozens what makes this state of the art? I think a prompt + an external dataset is a very simple distribution channel right now to explore anything quickly with low friction. The curl | bash of 2026 The quick setup is cool! I\u2019ve not seen this onboarding flow for other tools, and I quite like its simplicity. > I can embed everything and all the other sources for", "positive": "Peerweb: Decentralized website hosting via WebTorrent. I don't get it, I upload my files to your site, then I send my friends links to your site? How is this not a single point of failure? Github:  https://github.com/omodaka9375/peerweb  This is pretty interesting! I think serving video is a particularly interesting use of Webtorrent. I think it would be good if you could add this as a front end to basically make sites DDOS proof. So you host like a regular site, but with a JS front end that hosts the site P2P the more traffic there is. Fun! I wish WebTorrent had caught on more. I've always thought it had a worthy place in the modern P2P conversation. In 2020, I messed around with a PoC for what hosting and distributing Linux distros could look like using WebTorrent[1]. The protocol project as a whole has a lovely and brilliant design but has stayed mostly stagnant in recent years. There are only a couple of WebRTC-enabled torrent trackers that have remained active and stable. 1.  https://github.com/leoherzog/LinuxExchange  In its own reimagined way from what\u2019s possible in 2026, this could kick off a new kind of geocities. This is cool - I actually worked on something similar way back in the day:  https://github.com/tom-james-watson/wtp-ext . It avoided the need to have any kind of intermediary website entirely. The cool thing was it worked at the browser level using experimental libdweb support, though that has unfortunately since been abandoned. You could literally load URLs like wtp://tomjwatson.com/blog directly in your browser. I think one of the values of (what appears to be) AI generated projects like this is that they can make me aware of the underlying technology that I might not have heard about - for example WebTorrent:  https://webtorrent.io/faq  Pretty cool! Not sure what this offers over WebTorrent itself, but I was happy to learn about its existence. Nice, I clicked on the first demo, and I got stuck at connecting with peers. I like the idea though. N", "negative": "Track Your Routine \u2013 Open-source app for task management. I've been working on TYR (Track Your Routine), a Flutter-based task and routine tracking app. It's open source and built with Firebase for auth and data sync. Key features:\n- Task creation with date/time scheduling\n- Local notifications for reminders\n- Real-time sync across devices via Firestore\n- Category-based organization (work, vacation, events)\n- Clean dark theme UI with Material Design 3 Tech stack: Flutter/Dart, Firebase Auth, Cloud Firestore, local notifications. The app is still under active development, but the core functionality is working. I built it to solve my own need for a simple, privacy-focused task tracker that works across platforms (Android, iOS, Web, Desktop). What I'd love feedback on:\n- The notification system implementation\n- UI/UX improvements\n- Feature suggestions\n- Code quality and architecture (it's my first larger Flutter project) The codebase is MIT licensed and contributions are welcome. I'm particularly interested in feedback from Flutter developers on best practices I might be missing. GitHub:  https://github.com/MSF01/TYR  What do you think? What features would make this more useful for your workflow? Screenshots in the README would we nice :) + the writing style in the README gives slop smell I'm happy for you building this app, it's tremendous effort to build a flutter application, and this should feel like an achievement for you. However, task management apps are so unbelievably common nowadays. Nothing that can't be solved by notepad on PC, or the clock/calendar app on my phone / and if I really need a task app, I'll use google's or build my own. Your next step should be to take what you have learned from building this app, and focus on fixing a real problem that people around you face. I don't mind low stakes vibe-coded applications per se, but the readme is LLM slop that I couldn't bring myself to keep reading. Surely I'm not the only person who first used Linear [0] a"}
{"anchor": "Show HN: I used Claude Code to discover connections between 100 books. It\u2019s interesting how many of the descriptions have a distinct LLM-style voice. Even if you hadn\u2019t posted how it was generated I would have immediately recognized many of the motifs and patterns as LLM writing style. The visual style of linking phrases from one section to the next looks neat, but the connections don\u2019t seem correct. There\u2019s a link from \u201cfictions\u201d to \u201cinternal motives\u201d near the top of the first link and several other links are not really obviously correct. >A fun tendency is that Claude kept getting distracted by topics of secrecy, conspiracy, and hidden systems Interesting... seems like it wants the keys on your system! ;)  A fun tendency is that Claude kept getting distracted by topics of secrecy, conspiracy, and hidden systems - as if the task itself summoned a Foucault\u2019s Pendulum mindset.  It's all fun and game 'till someone loses an eye/mind/even-tenuous-connection-to-reality. Edit: I'd mention that the themes Claude finds qualify as important stuff imo. But they're all pretty grim and it's a bit problematic focusing on them for a long period. Also, they are often the grimmest spin things that are well known. In a similar vein, I've been using Claude Code to \"read\" Github projects I have no business understanding. I found this one trending on Github with everything in Russian and went down the rabbit hole of deep packet inspection[0]. 0.  https://github.com/ValdikSS/GoodbyeDPI  I dont understand the lines connecting two pieces of text. In most cases, the connected words have absolutely zero connection with each other. In \"Father wound\" the words \"abandoned at birth\" are connected to \"did not\". Which makes it look like those visual connections are just a stylistic choice and don't carry any meaning at all. I read a book maybe a decade ago on the \"digital humanities\". I wish now I could remember the title and author. :( Anyway, it introduced me to the idea of using computational ", "positive": "Tao Te Ching \u2013 Translated by Ursula K. Le Guin. For people who like The Big Lebowski, there's \"The Tao of the Dude\"  https://dudeism.com/taoofthedude/  I picked up Tao Te Ching as an American teenager and was moved by how it cuts against the American faith in visible dominance and self-assertion, proposing a form of strength that is low, quiet, and unseen. It's much more than that of course, but that aspect had immediate impact on my thinking. HN seems to like Tao Te Ching.  https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu...  From the bottom: > This is a rendition, not a translation. I do not know any Chinese. I could approach the text at all only because Paul Carus, in his 1898 translation of the Tao Te Ching, printed the Chinese text with each character followed by a transliteration and a translation. My gratitude to him is unending. This is wonderful. Ursula K. Le Guin is a great thinker and  I\u2019d highly recommend her novels. I\u2019ve read Ken Liu\u2019s, who many here probably know at least from translating The Three Body Problem and Death\u2019s End, Tao Te Ching and it was remarkably poetic. Excited to read another person\u2019s interpretation. I am just noticing how those ideas are present in Wizard of Earthsea. > I think of it as the Aleph, in Borges\u2019s story: if you can see it rightly, it contains everything. I'm a simple man. I see Borge, I upvote As another comment points out, Le Guin herself does not call this a translation, so we shouldn't misrepresent it (although it might be my favorite English version). However, it's not in the public domain. Her work deserves all the attention it can get, but I'd rather not see it pirated wholesale. This is one of my favorite versions, mostly for nostalgic reasons. My initial exposure to the Tao te Ching was this \"rendition\" and Stephen Mitchell's version. Comparing the two was always very thought provoking; the approach is very different between them. I often come to this site and compare chapters across multiple versions:", "negative": "Show HN: Website that plays the lottery every second. I think people understand the odds are small. However, perhaps they perceive their chances of meaningfully turn around their life in other ways have even smaller odds. i.e. improbable vs actually impossible. At least the lottery doesn't care about your current circumstance and everyone has an equal (equally small) chance. Secondly, because everyone realizes the chances are small, the real product being sold is Hope. Even the advertisements for the lotteries address this. The thing you're buying is 30 seconds of daydreaming so you can comfortably tackle the rest of the day. Interesting site.  Logic is rather easy, setting you the WEB site to present the results to me is rather hard. Good idea to show the odds. I wouldn\u2019t be able to remember the name to send to someone Maybe try shouldIplaythelottery.com What I love about this is how it demonstrates that the  waiting  is the most powerful part. That week is where a lotto user\u2019s brain does all the work for the lotto corp. The anticipation! The excitement. What if? Oh let\u2019s daydream! Oh the dopamine! You don\u2019t even have to sell them hope. Just sell them the sensation of hope. The amount of time I spent watching this page is a nice reminder of why I have a rule to never buy lottery tickets. See also: Simulation Clicker. I know how my brain works these days. So you're telling me there's a chance I would be really curious to see the money side of this. I am not sure about Powerball, but with EuroJackpot, some of the smaller wins can cover the cost of the ticket (or even cover a holiday!). It would be really interesting to watch the expected value play out over repeated plays!! I am imagining a running balance where you keep track of total spend versus total returns. Most of the time the balance steadily goes more negative, with occasional jumps back up when you hit a partial match, and very rare big spikes from a larger win. Very cool project! The odds of winning are so"}
{"anchor": "Profession by Isaac Asimov (1957). Link to the story without ads  https://www.inf.ufpr.br/renato/profession.html  one of asimov's finest , a metaphor that continues to find relevance in my day to day existence - that the conclusions we so readily come to are assumptions made in the absence of the awareness of something more This is my favorite Asimov story. It's got a protagonist with compelling motivations, a society that has problems but also convincing reasons why they persist, and a great ending.  Dr Antonelli said, \u201cOr do you believe that studying some subject will bend the brain cells in that direction, like that other theory that a pregnant woman need only listen to great music persistently to make a composer of her child. Do you believe that?\u201d  Apparently, Asimov was an early critic of the \u201cMozart in the womb\u201d movement. Is this still in print, maybe as part of a collection? I tried to find it but couldn't. Many of his other works seem to be available as paperback, including a bunch of story collections. What the hell that was a good read. Ending was great (though the last line did confuse me) Such a great ending. Really makes one wonder about the current AI hype of getting the machines to take over our work. Remind me of a recent discussion we had among Stackoverflow moderator: > \u201cThink about it,\u201d he continued. \u201cWho discovers the edge cases the docs don\u2019t mention? Who answers the questions that haven\u2019t been asked before? It can\u2019t be people trained only to repeat canonical answers. Somewhere, it has to stop. Somewhere, someone has to think.\u201d > \u201cYes,\u201d said the Moderator. > He leaned back. For a moment, restlessness flickered in his eyes. > \u201cSo why wasn\u2019t I told this at the start?\u201d > \u201cIf we told everyone,\u201d said the Moderator gently, \u201cwe\u2019d destroy the system. Most contributors must believe the goal is to fix their CRUD apps. They need closure. They need certainty. They need to get to be a Registered Something\u2014Frontend, Backend, DevOps, Full stack. Only someone w", "positive": "Leaving the U.S. for the Netherlands. Leaving now is the best way to ensure things get worse. If you've given up, your vote no longer counts and your voice no longer matters. To learn how to leave USA one needs to pass the newyorker paywall. Coming from Switzerland to the US: I do not want to leave. Everything is better here\u2026 maybe not for the average person but for some individuals. This is the moment for all those Hollywood personalities and other liberals to learn how to leave for real, not just talk about it! If things deteriorate in US there will not be places to leave to pretty soon Come to the Netherlands! It\u2019s awesome. And the visa is easy: just put 5k in a business account. Look up Dutch American friendship treaty. Great Moments in UX Below the teaser blurb ending \"The Netherlands offers one way out,\" and the byline, where you'd expect the article to start, is the text \"Your window is closing.\" Fortunately, if you scroll further, the ominous warning turns out to only be for the paywall.  https://i.imgur.com/4WT4S8u.png  And go where? Seriously I don\u2019t know of another country that isn\u2019t on the same authoritarian track, if not further along. If anyone has done a serious study and come up with a country that still has strong judicial independence, due process, lack of censorship and respect for private property, Id love to know Lol. I know what the comments will be. And the US is one of the top immigrant countries in the world. Always worth reflecting why people choose that when there is greener gras. There are much better places in the world to move to than the Netherlands Went for dinner with a Parisian friend of mine. He spent a good amount of it complaining that Paris is unrecognizable from his youth. Too many Americans, everywhere! Better explain to me how to get to the USA. I love the Netherlands and have spent a few months trying it out as a place to live. It's among my favourite places: moderate weather, friendly people, a high level of personal freedo", "negative": "OpenSSL: Stack buffer overflow in CMS AuthEnvelopedData parsing. Can someone translate \"Applications and services that parse untrusted CMS or PKCS#7 content using AEAD ciphers (e.g., S/MIME AuthEnvelopedData with AES-GCM) are vulnerable\" to human? 2026 and we still have bugs from copying unbounded user input into fixed size stack buffers in security critical code. Oh well, maybe we'll fix it in the next 30 years instead. Is this really exploitable? Is stack smashing really still a thing on any modern platform? Another \"fix\" in the long line of OpenSSL \"fixes\" that includes no changes to tests and therefore can't really be said to fix anything. Professional standards of software development are simply absent in the project, and apparently it cannot be reformed, because we've all been waiting a long time for OpenSSL to get its act together. Looks like Debian and some other distros are still on the vulnerable 3.5.4. Why did Openssl publish before the distros rolled to the fixed version? Very strange, as I type this both Bullseye and Bookworm are marked as fixed but Trixie isn't yet:  https://security-tracker.debian.org/tracker/CVE-2025-11187  I'd encourage folks to read the recently-published statement [1] about the state of OpenSSL from Python's cryptography project. [1]:  https://news.ycombinator.com/item?id=46624352  Has anyone built OpenSSL with -fbounds-safety? I just looked at the vuln in detail. If you are using OpenSSL compiled with Fil-C, then you're safe. This attack will be nothing more than a denial of service (the attacker won't get to actually clobber the stack, or heap, or anything). Services that process CMS[1] or PKCS#7 envelopes may be vulnerable to this bug. The most common example of these is S/MIME (for signed/encrypted email), but PKCS#7 and CMS show up in all kinds of random places. (Unless I'm missing something, a key piece of context here is that CMD/PKCS#7 blobs are typically allowed to select their own algorithms, at least within an allowlist"}
{"anchor": "High air pollution could diminish exercise benefits by half \u2013 study. It sounds terrible . What will happend in the future?! The research doesn't differentiate between seasons , and every one knows how polluted the air is in the winter when everyone is heating their home and apartament. I look at the PM2.5 data for my city every day, and at this point (Nov) in the winter season, the only acceptable time to exercise is between 2PM-4PM after vertical mixing kicked in. Outside that duration, particulates are elevated after morning rush our, after evening rush hour, or during overnight inversion trapping evening rush hour + wood burning smoke until the next morning rush hour. This is one the main reasons why I would prefer working remote, it is hard to utilize this time well (for exercise) if you are in the office. At least with PM you can wear a mask, although I am still searching for the best one that works during intense exercise. Also wanted to point out\n\"Trump EPA moves to abandon rule that sets tough standards for deadly soot pollution\"  https://apnews.com/article/epa-soot-air-pollution-trump-zeld...  If only you could see it. In the big cities the air quality has improved, however, I am not sure if it really has, or if we are now just burning hydrocarbons more efficiently so that the particle sizes have become invisible. Put it this way, although cars are allegedly better than they were, fuel consumption hasn't dropped considerably. The cars are more numerous than ever, and, although there are EVs, there are still more ICE cars than there were in the good old days when petrol came with lead in it. I am not sure that most people in urban areas even know what good air tastes and smells like. I take a canal path through lush countryside, far from any cars for most of the way. This canal has an aqueduct (or is it a viaduct?) over a motorway and the contrast is incredible. You go from basically smelling flowers to air pollution and back to clean air again quite quickly", "positive": "Project Genie: Experimenting with infinite, interactive worlds. Google Deepmind Page:  https://deepmind.google/models/genie/  Try it in Google Labs:  https://labs.google/projectgenie  (Project Genie is available to Google AI Ultra subscribers in the US 18+.) This could be the future of film. Instead of prompting where you don't know what the model will produce, you could use fine-grained motion controls to get the shot you are looking for. If you want to adjust the shot after, you could just checkpoint the model there, by taking a screenshot, and rerun. Crazy. Every character goes forward only, permanence is still out of reach apparently. Reminds me of this [1] HN post from 9 months ago, where the author trained a neural network to do world emulation from video recordings of their local park \u2014 you can walk around in their interactive demo [2]. I don't have access to the DeepMind demo, but from the video it looks like it takes the idea up a notch. (I don't know the exact lineage of these ideas, but a general observation is that it's a shame that it's the norm for blog posts / indie demos to not get cited.) [1]  https://news.ycombinator.com/item?id=43798757  [2]  https://madebyoll.in/posts/world_emulation_via_dnn/demo/  I keep on repeating myself, but it feels like I'm living in the future.\nCan't wait to hook this up to my old Oculus glasses and let Genie create a fully realistic sailing simulator for me, where I can train sailing with realistic conditions. On boats I'd love to sail. If making games out of these simulations work, it't be the end for a lot of big studios, and might be the renaissance for small to one person game studios. What\u2019s the endgame here? For a small gaming studio, what are the actual implications? I have been confused for a long time why FB is not motivated enough to invest in world models, it IS the key to unblock their \"metaverse\" vision. And instead they let go Yann LeCun. This would be really cool if polished and integrated with VR. I don't", "negative": "Without benchmarking LLMs, you're likely overpaying. > He's a non-technical founder building an AI-powered business. It sounds like he's building some kind of ai support chat bot. I despise these things. I'd second this wholeheartedly Since building a custom agent setup to replace copilot, adopting/adjusting Claude Code prompts, and giving it basic tools, gemini-3-flash is my go-to model unless I know it's a big and involved task. The model is really good at 1/10 the cost of pro, super fast by comparison, and some basic a/b testing shows little to no difference in output on the majority of tasks I used Cut all my subs, spend less money, don't get rate limited Depends on what you\u2019re doing. Using the smaller / cheaper LLMs will generally make it way more fragile. The article appears to focus on creating a benchmark dataset with real examples. For lots of applications, especially if you\u2019re worried about people messing with it, about weird behavior on edge cases, about stability, you\u2019d have to do a bunch of robustness testing as well, and bigger models will be better. Another big problem is it\u2019s hard to set objectives is many cases, and for example maybe your customer service chat still passes but comes across worse for a smaller model. Id be careful is all. The author of this post should benchmark his own blog for accessibility metrics, text contrast is dreadful.. On the other hand, this would be interesting for measuring agents in coding tasks, but there's quite a lot of context to provide here, both input and output would be massive. Wow, this was some slick long form sales work. I hope your SaaS goes well. Nice one! Anecdotal tip on LLM-as-judge scoring - Skip the 1-10 scale, use boolean criteria instead, then weight manually e.g. - Did it cite the 30-day return policy? Y/N\n - Tone professional and empathetic? Y/N\n - Offered clear next steps? Y/N Then: 0.5 * accuracy + 0.3 * tone + 0.2 * next_steps Why: Reduces volatility of responses while still maintaining creativ"}
{"anchor": "\u03c00.5: A VLA with open-world generalization. This is amazing! As someone working with industrial robots, normally under strict environmental constraints and control, witnessing such real-world robotics progress truly excites me about the future! By the way, they\u2019ve open-sourced their \u03c00 model (code and model weights). \nMore information can be found here:  https://github.com/Physical-Intelligence/openpi  Is the robot platform they're using something they've developed themselves? The paper doesn't seem to mention any details outside of sensors and actuators. These variable-length arrays are getting quite advanced I'm genuinely asking (not trying to be snarky)... Why are these robots so slow? Is it a throughput constraint given too much data from the environment sensors? Is it processing the data? I'm curious about where the bottleneck is. I'm just a layman, but I can't see this design scaling. It's way too slow and \"hard\" for fine motor tasks like cleaning up a kitchen or being anywhere around humans, really. I think the future is in \"softer\" type of robots that can sense whether their robot fingers are pushing a cabinet door (or if it's facing resistance) and adjust accordingly. A quick google search shows this example (animated render) which is closer to what I imagine the ultimate solution will be:  https://compliance-robotics.com/compliance-industry/  Human flesh is way too squishy for us to allow hard tools to interface with it, unless the human is in control. The difference between a blunt weapon and the robot from TFA is that the latter is very slow and on wheels. Amazing! On a fun note, I believe if a human kid were cleaning up the spill and threw the sponge into the sink like that, the kid would be in trouble. XD Does the general laws of demos apply here? Than any automation shown is the extent of capabilities not the start? Finally, machines doing the work we  dont  want to do Most of it is open source. Their VLAs are based upon Gemma models + vision encoders", "positive": "Show HN: I used Claude Code to discover connections between 100 books. It\u2019s interesting how many of the descriptions have a distinct LLM-style voice. Even if you hadn\u2019t posted how it was generated I would have immediately recognized many of the motifs and patterns as LLM writing style. The visual style of linking phrases from one section to the next looks neat, but the connections don\u2019t seem correct. There\u2019s a link from \u201cfictions\u201d to \u201cinternal motives\u201d near the top of the first link and several other links are not really obviously correct. >A fun tendency is that Claude kept getting distracted by topics of secrecy, conspiracy, and hidden systems Interesting... seems like it wants the keys on your system! ;)  A fun tendency is that Claude kept getting distracted by topics of secrecy, conspiracy, and hidden systems - as if the task itself summoned a Foucault\u2019s Pendulum mindset.  It's all fun and game 'till someone loses an eye/mind/even-tenuous-connection-to-reality. Edit: I'd mention that the themes Claude finds qualify as important stuff imo. But they're all pretty grim and it's a bit problematic focusing on them for a long period. Also, they are often the grimmest spin things that are well known. In a similar vein, I've been using Claude Code to \"read\" Github projects I have no business understanding. I found this one trending on Github with everything in Russian and went down the rabbit hole of deep packet inspection[0]. 0.  https://github.com/ValdikSS/GoodbyeDPI  I dont understand the lines connecting two pieces of text. In most cases, the connected words have absolutely zero connection with each other. In \"Father wound\" the words \"abandoned at birth\" are connected to \"did not\". Which makes it look like those visual connections are just a stylistic choice and don't carry any meaning at all. I read a book maybe a decade ago on the \"digital humanities\". I wish now I could remember the title and author. :( Anyway, it introduced me to the idea of using computational ", "negative": "The C-Shaped Hole in Package Management. Please don't.  C packaging in distros is working fine and doesn't need to turn into crap like the other language-specific package managers.  If you don't know how to use pkgconf then that's your problem. very related:  https://michael.orlitzky.com/articles/motherfuckers_need_pac...  >  Conan and vcpkg exist now and are actively maintained I am not sure if it is just me, but I seem to constantly run into broken vcpkg packages with bad security patches that keep them from compiling, cmake scripts that can't find the binaries, missing headers and other fun issues. I don't trust any language that fundamentally becomes reliant on package managers.  Once package managers become normalized and pervasively used, people become less thoughtful and investigative into what libraries they use.  Instead of learning about who created it, who manages it, what its philosophy is, people increasingly just let'er rip and install it then use a few snippets to try it.  If it works, great.  Maybe it's a little bloated and that causes them to give it a side-eye, but they can replace it later....which never comes. That would be fine if it only effected that first layer, of a basic library and a basic app, but it becomes multiple layers of this kind of habit that then ends up in multiple layers of software used by many people. Not sure that I would go so far as to suggest these kinds of languages with runaway dependency cultures shouldn't exist, but I will go so far as to say any languages that don't already have that culture need to be preserved with respect like uncontacted tribes in the Amazon.  You aren't just managing a language, you are also managing process and mind.  Some seemingly inefficient and seemingly less powerful processes and ways of thinking have value that isn't always immediately obvious to people. Missing in this discussion is that package management is tightly coupled to module resolution in nearly every language. It is not enoug"}
{"anchor": "String theory can now describe a universe that has dark energy?. Only in universe with 5 dimensions. Shouldn't string theory be given up on at this point? This theory has existed for over 50 years and hasn't produced any results. Even the predictions made by it such as e.g. supersymmetry have not been confirmed despite searching for them at particle colliders. I foolishly sat in 8.821 [0] while at MIT thinking I could make sense out of quantum gravity. Most of the math went over my head, but the way I understand this paper, it\u2019s basically a cosmic engineering fix for a geometry problem. Please correct me if necessary. String theory usually prefers universes that want to crunch inwards (Anti-de Sitter space). Our universe, however, is accelerating outwards (Dark Energy). To fix this, the authors are essentially creating a force balance. They have magnetic flux pushing the universe's extra dimensions outward (like inflating a tire), and they use the Casimir effect (quantum vacuum pressure) to pull them back inward. When you balance those two opposing pressures, you get a stable system with a tiny bit of leftover energy. That \"leftover\" is the Dark Energy we observe. You start with 11 dimensions (M-theory) and roll up 6 of them to get this 5D model. It sounds abstract, but for my engineer brain, it's helpful to think of that extra 5th dimension not as a \"place\" you can visit, but as a hidden control loop. The forces fighting it out inside that 5th dimension are what generate the energy potential we perceive as Dark Energy in our 4D world. The authors stop at 5D here, but getting that control loop stable is the hardest part The big observatiom here is that this balance isn't static -- it suggests Dark Energy gets weaker over time (\"quintessence\"). If the recent DESI data holds up, this specific string theory solution might actually fit the observational curve better than the standard model. [0]  https://ocw.mit.edu/courses/8-821-string-theory-and-holograp...  Hm, string", "positive": "\u201cThe closer to the train station, the worse the kebab\u201d \u2013 a \u201cstudy\u201d. Anecdotally, it's the same for coffee. Office lobby coffee shops are invariably terrible. The decent ones are always at least a 5-10 minute walk away. This makes intuitive sense. High mass-transit corridor real-estate (rail, air, road) leases come at a premium so those higher fixed-costs and must be balanced against a higher-volume of less-breadth of service with the same fixed (or even slightly higher) labor costs. In food service, high-volume is (mostly) inversely correlated with quality. Looking at their actual results ( https://preview.redd.it/znmnejgab5je1.png?width=1000&format=... ), I don't see any positive or negative correlation. Although I can subjectively confirm the hypothesis. I've observed the following: 1) An alarming number of regions in the world have a pizza joint called \"New York Pizza\", \"Manhattan Pizza\", or similar. 2) The similarity of the pizza therein to the actual thin, greasy slices served up in pizza joints from actual New York is inversely proportional to the location's distance from New York. So, the New York Pizza in Boston -- pretty close. The New York Pizza in Brisbane, QLD is alien by comparison and I think they consider \"pepperoni\" and \"salami\" interchangeable down there. He didn't find a correlation, or rather found that there is no correlation, between proximity to a railway station and how the kebab is reviewed. It's a nice study for a statistics class! The only place this isn't true is Japan. Always like reading the Best Kebab reviews on trip advisor. It\u2019s right next to Queen Street railway station so fits with the study.  https://www.tripadvisor.co.uk/Restaurant_Review-g186534-d125...  > Not only was my food uncooked but I also discovered a pubic hair in my chips and cheese, then when I proceeded to report the problem, I was chased with a knife. Down Dundas Street.Absolutely scandalous LOL we may need to update the title of this post, half the top level comment", "negative": "Ask HN: Books to learn 6502 ASM and the Apple II. Pretty much the best resource available:  https://6502.org/  Check the books section and find something that compels you. Also, don't forget the HUGE number of resources for 6502 assembly programming that are available in the  https://archive.org/  magazine and book sections:  https://archive.org/search?query=6502  Rodney Zaks' books are great - I like especially \"6502 Games\", which taught me a lot back in the day:  https://archive.org/download/6502g/6502Games.pdf  I'm also especially fond of the easy6502 emulator - its a very handy tool to have while studying 6502 techniques:  https://skilldrick.github.io/easy6502/  Its not absolutely necessary to learn BASIC before Assembly, but it will definitely help you understand the resources of the machine better if you can debug BASIC ROM code.  My personal 6502 platform of choice, the Oric-1/Atmos machines, has a pretty great ROM disassembly available, from which a lot of great knowledge can be obtained - but it does of course first require an undersanding of BASIC. In case you're curious, the Oric-1 ROM Disassembly:  https://library.defence-force.org/books/content/oric_advance...  (You can get an Oric emulator named Oricutron, or you can access a virtual Oric here:  https://oric.games/  ..) Good luck! This is the book I used when I was writing serial drivers for Apple II ProDOS:  https://archive.org/details/6502_Assembly_Language_Programmi...  And I have a vague memory of this book:\n https://archive.org/details/aiimp/mode/2up  Not sure what level you're at, but I can't remember if this is the text Jef Raskin wrote, but it's a decent backgrounder:\n https://archive.org/details/aiirm/mode/2up  I believe one of the \"standard works\" to learn 6502 back in the day was  Programming the 6502  by Rodnay Zaks. It's out of print, but it was printed in a lot of copies so you should be able to find one second-hand. I'm seconding the recommendation to look at Rodnay Zack's books. For exa"}
{"anchor": "Ask HN: What are some of your favorite documentaries?. My second favorite \u201cWild, Wild, Country\u201d, but it was mentioned already at the top of the first list.  I enjoy it as a cautionary tale, but I also unironically find it an inspiring tale of building, even if it turns out to bad. My favorite documentary is \u201cThe Barkley Marathons, the race that eats it\u2019s young\u201d I return to it at least once a year, and while the root of the story - watching people attempt the impossible is certainly inspiring, I find its moral themes are what I appreciate about it the most.  The idea of competition as a collective activity, that everyone wants to win, but also everyone wants to see others win their own race, that there\u2019s something about the way that it advances our understanding of humanity that is more important than individual success. Then also - that your race is yours alone, and that the most important victory is the one you define for yourself.  There are people who finish only one or three laps of the five lap marathon, and that failure is a greater achievement than most people will ever know, and they clearly see it that way, there\u2019s near no shame in anyone\u2019s performance and people are clearly defining success for themselves, mostly clearly beyond what anyone else would define it for them.  And finally, it\u2019s kind of a throwaway line, but one of the runners says \u201cI think most people could use more pain in their lives.\u201d And it made me realize that often, when enduring hardship, rather than turning away from it, finding ways to challenge myself on my terms is a healthier approach to stress than \u201crelaxing\u201d. Le Joli Mai  https://www.youtube.com/watch?v=iOj0sPmJssw  Here's another previous post:  https://news.ycombinator.com/item?id=25624456  which includes an answer of mine. To add to that list: [1] \"Andermatt - Global Village\" - tracks the construction of a luxury resort in the Swiss village of Andermatt and how it affects people there. The village and the project still make the ", "positive": "Measuring AI Ability to Complete Long Tasks. This seems like a good way to measure LLM improvement. It matches the my personal feeling when using progressively better models over time. Opus is already the name of an audio codec. I recently asked Opus to just \u201cAdd vector search\u201d to my current hobby project, a topic I know very little about. It set up manticore, pulled an embedding model, wrote a migration tool for my old keyword indices, and built the front end. I\u2019m not exaggerating much either: the prompt was the length of a tweet. I think it would easily have taken me 4+ hours to do that.  It ran in 15 minutes while I played Kirby Air Riders and worked on the first try. Afterward, I sort of had to reflect on the fact that I learned essentially nothing about building vector search. I wanted the feature more than I wanted to know how to build the feature. It kept me learning the thing I cared about rather than doing a side quest. Would be interesting to see Gemini 3.0 Pro benchmarked as well. I didn't really understand the \"long task\" thing until I actually experienced it. The problem is finding a task you can set an agent that justifies working for that long. I finally hit one when I tried porting that Python HTML5 parser to JavaScript by pointing Codex CLI at the 9,200 html5lib-tests test suite:  https://simonwillison.net/2025/Dec/15/porting-justhtml/  It's pretty amazing to watch tools-in-a-loop crunch away for >4 hours to solve a generally difficult problem through sheer brute-force. Opus looks like a big jump from the previous leader (GPT 5.1), but when you switch from \"50%\" to \"80%\", GPT 5.1 still leads by a good margin. I'm not sure if you can take much from this - perhaps \"5.1 is more reliable at slightly shorter stuff, choose Opus if you're trying to push the frontier in task length\". I think the problem here is LLM eventually pollute its context window with so much of the current task that the larger picture or architectural sanity is forgotten in favor of ", "negative": "Windows 11's Patch Tuesday nightmare gets worse. How can a company this big fail so hard in what one would consider their main* product still baffles me. *Yes, they probably make more revenue in Azure or Office365 licenses but at least when I think \u201cMicrosoft\u201d I immediately think Windows. > It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been. I think I might know... I'm wondering why the guy at Microsoft in charge of Windows is still employed. Over the prior weekend my installation of Playnite (a catalog/launcher for my games) was broken by the update, until I moved its data off of OneDrive[1]. And the other day I figured out that a couple of icons on my desktop had become completely inert and unresponsive due to the same bug - again due to an interaction between the Windows Shell and OneDrive. And this one I can't fix, I can't shift my desktop out of OneDrive. MS's strategy at this point is that Windows is a loss leader to get people onto the subscriptions for Office and OneDrive. So when the Windows team releases bugs that break usage of those services, forcing people off them onto alternative solutions, the guy in charge of those updates really needs to be answering some tough questions. [1] I've now got SyncThing handling this. Previous discussion: >Microsoft suspects some PCs might not boot after Windows 11 January 2026 Update  https://news.ycombinator.com/item?id=46761061  W11 is the best OS I've ever used, but everyone seems to hate it because Microsoft is so adamant in destroying its reputation by pushing Copilot and bugs instead of focusing on reliability. It's a shame. I see Microslop's \"AI\" coding mandate is continuing to go well [dupe]  https://news.ycombinator.com/item?id=46761061  So, a couple years ago Microsoft was the first large, public-facing software organization"}
{"anchor": "Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding Comparison. TL;DR If you want to jump straight to the conclusion, I\u2019d say go for Gemini 2.5 Pro, it\u2019s better at coding, has one million in context window as compared to Claude\u2019s 200k, and you can get it for free (a big plus). However, Claude\u2019s 3.7 Sonnet is not that far behind. Though at this point there\u2019s no point using it over Gemini 2.5 Pro. From my use case, the Gemini 2.5 is terrible. I have a complex Cython code in a single file (1500 lines) for a Sequence Labeling. Claude and o3 are very good in improving this code and following the commands. The Gemini always try to do unrelated changes. For example, I asked, separately, for small changes such as remove this unused function, or cache the arrays indexes. Every time it completely refactored the code and was obsessed with removing the gil. The output code is always broken, because removing the gil is not easy. Is there a less biased discussion? The OP link is a thinly veiled and biased advert for something called composio and really a biased and overly flowery view of Gemini 2.5 pro. Example: \u201cEveryone\u2019s talking about this model on Twitter (X) and YouTube. It\u2019s trending everywhere, like seriously. The first model from Google to receive such fanfare. And it is #1 in the LMArena just like that. But what does this mean? It means that this model is killing all the other models in coding, math, Science, Image understanding, and other areas.\u201d For Gemini: play around with the temperature: the default is terrible: we had much better results with (much) lower values. Gemini is the only model which tells me when it's a good time to stop chatting because either it can't find a solution or because it dislikes my solution (when I actively want to neglect security). And the context length is just amazing. When ChatGPT's context is full, it totally forgets what we were chatting about, as if it would start an entirely new chat. Gemini lacks the tooling, there ChatGPT is far ahead, b", "positive": "Crafting Interpreters. The two most popular discussions of this fantastic book: 2020 with 777 points:  https://news.ycombinator.com/item?id=22788738  2024 with 607 points:  https://news.ycombinator.com/item?id=40950235  Really I would love to know how parse context sensitive stuff like typedef which will have \"switched\" syntax for some tokens. Would like to know things like \"hoisting\" in C++, where you can you the class and struct after the code inside the function too, but I just find it hard to describe them in rigorous formal language and grammar. Hacky solution for PEG such as adding a context stack requires careful management of the entry/exit point, but the more fundamental problem is that you still can't \"switch\" syntax, or you have to add all possible syntax combination depending on the numbers of such stacks. I believe persistent data structure and transactional data structure would help but I just couldn't find a formalism for that. In case anyone finds it useful, we (CodeCrafters) built a coding challenge as a companion to this book. The official repository for the book made this very easy to do since it has tests for each individual chapter. Link:  https://app.codecrafters.io/courses/interpreter/overview  One of the best resources for learning compiler design. The web version being free is incredibly generous. I've found this book to be a good way to learn a new language, because it forces you to do a bit of reading about various language features and patterns to create equivalent implementations. For languages that lack some of the features in Java, it can be tricky to learn how to apply similar patterns, but that's half the fun (for me). I have bought the print version of this 3 seperate times to give as a gift, its excellent. It's a great book, I bought the paper version first, but man it was too big and heavy for my liking, ended up buying a digital copy; much more practical for notes and search... although I keep getting lost somewhere in the mounta", "negative": "The $100B megadeal between OpenAI and Nvidia is on ice. ...and the merry go round stopped In the distance, Uncle Sam groans as his phone rings If the ice cream cone won't lick itself, who will? Last paragraph is informative: > Anthropic relies heavily on a combination of chips designed by Amazon Web Services known as Trainium, as well as Google\u2019s in-house designed TPU processors, to train its AI models. Google largely uses its TPUs to train Gemini. Both chips represent major competitive threats to Nvidia\u2019s best-selling products, known as graphics processing units, or GPUs. So which leading AI company is going to build on Nvidia, if not OpenAI? All these giant non-binding investment announcements are just a massive confidence scam. Does this mean OpenAI won't be needing all that RAM after all...? Not only has OpenAI's market share gone down significantly in the last 6mo, Nvidia has been using its newfound liquid funds to train its own family of models[1]. An alliance with OpenAI just makes less sense today than it did 6mo ago. [1]  https://blogs.nvidia.com/blog/open-models-data-tools-acceler...  Interesting to see this follow the news of their plan IPO in Q4 just yesterday.  https://www.wsj.com/tech/ai/openai-ipo-anthropic-race-69f06a...  > He[Jensen Huang] has also privately criticized what he has described as a lack of discipline in OpenAI\u2019s business approach and expressed concern about the competition it faces from the likes of Google and Anthropic, some of the people said. This video that breaks down the crazy financial positions of all the AI companies and how they are all involved with one called CoreWeave (who could easily bring the whole thing tumbling down)  is fascinating:  https://youtu.be/arU9Lvu5Kc0?si=GWTJsXtGkuh5xrY0  Would be interesting to see how Oracle's CDSs react to this news. How is this legal for them to do to pump stocks will there be more 5090 FE cards at a lower price? one can only hope OpenAI is too important to run out of cash. The gov wil"}
{"anchor": "Neural Networks: Zero to Hero. I saw this on a comment [0] and thought it deserved a post. [0]  https://news.ycombinator.com/item?id=46483776  A couple years ago I wrote a tutorial how to build a Neural Network in NumPy from scratch.\u00b9 \u00b9  https://matthodges.com/posts/2022-08-06-neural-network-from-...  This new? Hasn't the zero-to-hero course been around for a while? A bit of shameless plug, I wrote 2 articles about this after doing the course a while ago.  https://martincapodici.com/2023/07/15/no-local-gpu-no-proble...   https://martincapodici.com/2023/07/19/modal-com-and-nanogpt-...  I'm not sure how it compares, but another option is the Hugging Face learning portal [0].  I'm doing the Deep RL Course and so far it's pretty straight forward (although when it gets math heavy I'm going to suffer). [0] -  https://huggingface.co/learn  its fun seeing HN articles with huge upvotes but no comments, similar to when some super esoteric maths gets posted: everyone upvotes out of a common understanding of its genius, but indeed by virtue of its genius most of us are not sufficiently cognitively gifted to provide any meaningful commentary. the karpathy vids are very cool but having watched it, for me the takeaway was \"i had better leave this for the clever guys\". thankfully digital carpentry and plumbing is still in demand, for now! what next now tho? i co-incidentally completed watching his last vid of training up gpt-2 today :-) . I\u2019ve gone through this series of videos earlier this year. In the past I\u2019ve gone through many \u201ceducational resources\u201d about deep neural networks - books, coursera courses (yeah, that one), a university class, the fastai course - but I don\u2019t work with them at all in my day to day. This series of videos was by far the best, most \u201cintuition building\u201d, highest signal-to-noise ratio, and least \u201cannoying\u201d content to get through. Could of course be that his way of teaching just clicks with me, but in general - very strong recommend. It\u2019s the primary reso", "positive": "Antirender: remove the glossy shine on architectural renderings. Wow, someone finally made Poland-filter. It all looks exactly like I'm used to. Looks beautiful tbh. I prefer the greyness That's funny, the second example is the Peace Bridge in Calgary. On a nice day the render actually looks close to the real thing! I ran it on the \"society if...\" meme lol  https://imgur.com/a/nFQN5tx  This is ingenious and actually useful. I'm looking for a new apartment and I always wanted to know how do these places look in a bad weather, because that's when I need beautiful surroundings the most. They still look great on a rainy November day. A nice cozy, quiet vibe. This filter seems to also change some architectural details and features, as well as degrade the quality of some materials in an unrealistic way. I am very curious if this app is making money or are users just using the two generators and then leaving? If so I am very impressed with your wrapper around the image gen models. Nano Banana is indeed a powerful model :) Used it on some Fortnite screenshots, I'd play that depressing version!  https://files.catbox.moe/i8tfkl.jpg   https://files.catbox.moe/mw8vbc.jpg  Then I thought what would it make from an already dark and grim scene, like HL2 Ravenholm  https://files.catbox.moe/d7z77h.jpg  but nothing really? Just made the whole thing a different color scheme + changed some architecture And the real killer app of contact lens AR will be ... this in reverse. Ha this is great - I always thought this would be a brilliant application for AI. Wow. Umm, the \"free generations\" limit is running on a client-based honour system... It would be great if I can run this as a browser extension that works on Zillow and Redfin. I did exactly the opposite with  https://prontopic.com  Maybe a real picture of the actual bridge was in the training set?  Similar to how prompting for a story about a boy wizard can result in verbatim Harry Potter passages. Looks like Machinarium. I like it. Un", "negative": "Show HN: Rails UI. Is this another Tailwind wrapper? Yes, it is. ugh this looks dated even by 2016 standards when will developers learn UI actually matters bootstrap was a mistake, and lowered the bar for everyone i don't get these types of products anymore. i think they're useful in their own way, but i can literally create styles with claude/gemini in a heartbeat and not have to pay some insane fee. I think you missed a trick not naming it Railwind UI. I used this about a year ago when I went through a short Rails phase. I was a bit surprised not to see more Rails-specific UI libraries considering how batteries-included the rest of the framework is, and at the time I didn't really 'get' tailwind. I'm not in a Rails phase anymore, but nice work on the library! maybe I'm just dumb but a lot of these elements don't seem to work? the \"...\" buttons don't open any flyout, the dropdown doesn't open up... otherwise looks cool though I wish I could use this \u2013 unfortunately UI frameworks are a political problem at every company I've worked at. The designers feel undermined or threatened by it, and product owners want to dictate design. Despite the massive productivity benefits of a UI framework, I've never been able to convince stakeholders to actually adopt one. If you\u2019re showing off a UI framework, I shouldn\u2019t be accidentally scrolling left and right on the page on mobile / my iPhone. Couldn\u2019t be bothered to scroll down the page to look at components while accidentally activating horizontal scrolling. Broken in Safari on iphone. For example: - table background moves left when table is scrolled horizontally - actions in table and dropdown do nothing on tap - text on buttons is selectable (really?) im always surprised that Rails is still relevant i havent used it since 2006 opting for php and django i might give it another shot, any reason you like this more than django or other frameworks I have hardware acceleration disabled in Firefox and my 5800X spins up trying to rend"}
{"anchor": "Models of European metro stations. Nice! Would nice to have Maashaven Rotterdam, being the highest elevated one in the Netherlands.  https://en.m.wikipedia.org/wiki/Maashaven_metro_station  This is insane. Never saw anything like it. One minor nitpick: zooming the map is very slow (maybe Leaflet is not the best choice?). And the main station in Paris is missing: Ch\u00e2telet-Les Halles. Other than that, incredible work!! Amazing. A very cool project, and a great resource for people with reduced mobility - I semi-regularly use Transport for London's station drawings (linked on this website) over the official accessibility map, which doesn't differentiate between stairs and escalators for example. I was never able to build mental model of Alexanderplatz in Berlin. Most of the times was simply following the signs and yup, the layout is complicated. Holy shit! This is an incredible piece of work. And they are almost all drawn \u201cmanually\u201d! I am SO impressed by the dedication > For the last 10 years I have been able to draw around 2,547 stations > A pen, a notebook, a bit of spatial vision and the willingness to navigate all the staircases, corridors, platforms and mezzanines are enough to draw a station > Due to the boredom provoked by the COVID-19 lockdown in 2020, I decided to digitalize all the sketches I had drawn in since the early 2010s See also this 3D model of Shinjuku station, Tokyo:  https://satoshi7190.github.io/Shinjuku-indoor-threejs-demo/  Is there a reason why moscow is missing ? Incredible work! I first looked at _regular_ stations, but once I understood that it was done by a single guy, I had to look at Paris' Mordor: Ch\u00e2telet. The 3D view looks like an ants nest, as expected. Very impressed by the work done! Very impressive work. I also learned something, which I'd always wondered cynically but never thought to investigate. The walking connection between lines at some stations in Barcelona seems so long as to not make sense, but it's explained here that at t", "positive": "European word translator: an interactive map. Love that the numbers in Catalan are represented as numerals, not as words. EDIT: playing with it, it's a bit sad that large numbers do not work at all (in any language); and that not all common forms of a word are shown.  For example, I tried to see how \"ninety six\" is said in french in France, Belgium and Switzerland, but it does not work. Ukrainian and russian words often use the same letters but are pronounced very differently due to distinct phonetics. On the other hand, some Polish and Czech words sound the same or very similar to Ukrainian but look quite different because of their different alphabets. Therefore, phonetic transcription would be a valuable improvement. You immediately see the difference (or similarly) of languages when using words that are very old, such as \"iron\", or \"stone\", which are words that have existed from the origins of that language. I can mostly speak for German. It seems to mix them all into one general language. But there are a lot of local differences between north and south of Germany, Switzerland and Austria. And it\u2019s not just dialect, but really different words that might not be understood everywhere. \nIf you look at the english part it has at least three different words. Similar in Spanish. There are examples from five language families shown here: Indo-European, Basque, Uralic, Turkic, and Afro-Asiatic. The words for bridge split neatly into language subfamilies.   The only exception appears to be Welsh. You are coloring it by 4 colors like map but you should color countries phonetically (speex, levenshtein or something similar) Wiktionary has dialect maps for common Chinese vocabulary that showcases the differences in terminology across various regions of Chinese, rather than their similarities. Example: sleep ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E7%9D%A...  , hide-and-seek ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E6%8D%8...  p.s. I'm saying t", "negative": "Adoption of EVs tied to real-world reductions in air pollution: study. I was out skating today. Everyone was having a fun time until a diesel truck simply drove down the nearby road. It stunk up and polluted the frozen lake air for a solid few minutes. I hate diesel trucks with a passion and if I live long enough to see it happen, I will celebrate the day they become defunct. Tesla's EV trucks need to deal the same hard kick to diesel trucks that they did to cars. No surprises. No matter how we look at it, EVs are much friendlier and safer to the environment. Some people argue the source of electricty can be contested against because that involves fossil fuel burning again, but in today's world we are rapidly moving away from it and towards nuclear/hydel/wind methods for generating power. I hope ICE cars completely become a thing of the past in the next couple of decades to come. Having spent a significant amount of time in Bangkok - the city center (and many urban hubs) is an amazing walkable place with pedestrian walkways suspended above major roads, lots of frequent public transit (metro, skytrain) that honestly makes my home city of Sydney feel like a developing country. The only downside is that traffic creates a lot of pollution, and the engine noise (not honking, there's very little of that) is so bad that you need to yell to a person standing next to you to have a conversation. As a visitor, I can't claim to know how to fix the problems facing locals, however I can't help but feel that urban centers would be 1000x better with mass adoption of EVs (bikes, cars). I have seen a spike in the number of Chinese EVs across the city - however I'm aware that economic pressures prevent mass adoption by the majority of the road-users This study is about air quality  in neighborhoods . So it would show the same thing even if EVs just moved pollution from where people use their cars to where power plants get placed, because that's not the question it's addressing. Has th"}
{"anchor": "40M Americans Live Alone, 29% of households. Any causes? Is it that we are too independent and don\u2019t like collectivism? A conspiracy might say it\u2019s on purpose to have more people pay for things typically bought for a couple. Like everyone having their own house, cable bill, utility bill, water bill, \u2026 If more people lived together with friends, that\u2019d make a dent in both the housing and loneliness crises. Is it a bad thing? People's life choices are their own. 29% seems like a fairly neutral number. Incidentally, that newsletter has a lot of interesting charts.  https://www.apolloacademy.com/the-daily-spark/  It's not a high number when compared to other first-world countries:  https://statranker.org/population/top-10-countries-with-high...  Is this page just a single chart and a massive legal disclaimer? When I was a twentysomething, I had roommates. This saved money on rent and bulk purchases (which let me spend more time having fun  and  save money) and provided a starter-kit social circle in a new city. It also honed conflict-resolution skills and ability to be civil. And when I got a partner, it made moving in together smoother. Something I\u2019ve noticed recently is many college graduates living alone. That\u2019s fine. But it\u2019s a weird default for early in one\u2019s career. If I had one general piece of advice for anyone starting their career, it would be to seek out a living situation with roommates. Side question: are more college students staying in solo dorms? Part of the \"housing crisis\" is older Americans aging-in-place and using way more home than they need too. A widow/er might occupy the same suburban single family home in retirement that could house 5 people. Even as new homes are much bigger than decades past\u2026 The best thing I ever did for my mental health was to start having children. Humans, like every other living creature, are hardwired by billions of years of evolution to reproduce. So what? After living with parents, roommates, spouses and others for most", "positive": "Toad is a unified experience for AI in the terminal. I'm really looking forward to trying this out over Christmas break. Textualize is awesome for building Python console apps. This looks really cool. I wonder if they support vi keybinds Hi. Will McGugan here. I built Toad. Ask me anything. This looks great! Looking forward to trying it out. I recently tried moving to OpenCode but it didn\u2019t quite scratch the itch UX wise. I see what you did what that intro and I approve :) This is absolutely awesome but the little jokey captions that Claude did (Discombobulating... Laminating...) all that stuff, they were a little annoying but cute enough, but whatever is running this one (I did not murder him... I thought I was special....) they are genuinely offputtingly bad. This great app doesn't need clunky humour front and centre, I'm not sure if it's Claude or toad but it seems markedly worse than Claude used to be. I already used Toad to run a conversion task I've been procastinating on. It worked perfectly and looked splendid doing so. Excited to dig in further. toad is next level in many ways Very excited to see this come out - though coding agents are impressive their UIs are a bit of a mixed bag. Textual offers incredibly impressive terminal experiences so I'm very much looking forward to this. I wonder how much agentic magic it'll be able to include though - Claude Code often seems like a lot of its intelligence comes from the scaffolding, not just the LLM.  I'm excited to see! It would be a matrushka to run Toad in a Zed terminal. The name Toad gave me a flashback to Tool for Oracle Application Development, an IDE and debugger for SQL and pl/sql back in the 1990s. I\u2019m not a big fan of the name Toad, but the Textual framework is fantastic. I\u2019ve been using it for years in a small project and it\u2019s just a wonderful tool - it makes it really easy to get a super fast little UI for scripts. I strongly resonate with the problem statement, but this implementation was very far o", "negative": "Start your meetings at 5 minutes past. #leadership is really sending me on this one Apart from just a quick breather between back to back meetings, it also provides a critical bio-break time for your attendees. We do this at my work and guess what - meetings tend to run 5 minutes late because everyone knows the next meeting doesn\u2019t start until 5 past. If you have so many back-to-back meetings, maybe put in a school bell that chimes at 5 minutes before and 5 minutes after the hour. Please just put this in your conference rooms so those of us who know how to evade meetings don't have to hear it as well. Meetings run long so frequently that  we now recommend starting the next meeting late to compensate. This will surely solve the problem. > there is social pressure not to allow meetings to run much past the top of the hour. I've never seen this pressure. > meetings rarely started on the dot anyway before this change. It's like I live in an entirely different world. Start meetings when they say they're going to start.  People will learn to show up quickly.  I think that works better than trying to psychologically game people into cooperation.  That just starts the classic treadmill.  You might have that one friend that you tell to show up half an hour before everyone else. They mentally add the half hour back because you're always giving such early times.  Better IMO to just keep things simple.  Let people leave when they need to.  Show up on time. We've been doing this at Qualcomm for a while, and I really like it. While meetings do run over sometimes, the practice has still built this acceptance around short breaks between meetings. No one bats an eye if we've got two consecutive meetings together, the first one ends late, and we wait five minutes before starting or joining the next one. In fact, having done it for so long, it surprisingly really annoys me when our vendors schedule 60 minute meetings on the hour. I've always wondered at the company cultures between Go"}
{"anchor": "Gemini 3. Feeling great to see something confidential - Anyone have any idea why it says 'confidential'? - Anyone actually able to use it?  I get 'You've reached your rate limit. Please try again later'.  (That said, I don't have a paid plan, but I've always had pretty much unlimited access to 2.5 pro) [Edit:  working for me now in ai studio] How long does it typically take after this to become available on  https://gemini.google.com/app  ? I would like to try the model, wondering if it's worth setting up billing or waiting. At the moment trying to use it in AI Studio (on the Free tier) just gives me \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" It's available to be selected, but the quota does not seem to have been enabled just yet. \"Failed to generate content, quota exceeded: you have reached the limit of requests today for this model. Please try again tomorrow.\" \"You've reached your rate limit. Please try again later.\" Update: as of 3:33 PM UTC, Tuesday, November 18, 2025, it seems to be enabled. It seem that Google doesn't prepare well to release Gemini 3 but leak many contents, include the model card early today and gemini 3 on aistudio.google.com it is live in the api > gemini-3-pro-preview-ais-applets > gemini-3-pro-preview API pricing is up to $2/M for input and $12/M for output For comparison:\nGemini 2.5 Pro was $1.25/M for input and $10/M for output\nGemini 1.5 Pro was $1.25/M for input and $5/M for output It generated a quite cool pelican on a bike:  https://imgur.com/a/yzXpEEh  And of course they hiked the API prices Standard Context(\u2264 200K tokens) Input $2.00 vs $1.25  (Gemini 3 pro input is 60% more expensive vs 2.5) Output $12.00 vs $10.00 (Gemini 3 pro output is 20% more expensive vs 2.5) Long Context(> 200K tokens) Input $4.00 vs $2.50   (same +60%) Output $18.00 vs $15.00  (same +20%) When will this be available in the cli? Not the preview crap again.\nHaven't the", "positive": "In New York City, congestion pricing leads to marked drop in pollution. > Particulates issued from tailpipes can aggravate asthma and heart disease and increase the risk of lung cancer and heart attack. Globally, they are a leading risk factor for premature death. Minor nitpick, but tailpipes aren't the primary source of emissions. The study is about PM2.5[0]. which will chiefly be tires and brake pads. Modern gasoline engines are relatively clean, outside of CO2, though diesel engines spit out a bunch of bad stuff. [0]  https://www.nature.com/articles/s44407-025-00037-2  See also  https://news.ycombinator.com/item?id=46213504  There was a study published about how much air pollution dropped in NYC during the COVID lockdown. PM2.5 was found to have dropped 36%. However with more robust analysis, this drop was discovered to not be statistically significant. I would caution anyone reading this who is tempted by confirmation bias. Source:  https://pmc.ncbi.nlm.nih.gov/articles/PMC7314691/  To head off the almost inevitable recapitulation of yesterday's parade of misinformed complaints by teenage libertarians, please actually read the paper before commenting. The paper shows there was no significant reduction in entries to the congestion charge zone by cars, vans, and light trucks. And you can confirm this conclusion is consistent with their source data using their github repo. The reduction in pollution is coming from the significant decline in heavy truck traffic. Truckers were using lower manhattan as a cut-through route to other places and they are now doing that less, exactly as congestion pricing planners long argued. Not surprising. The real question is how do we measure the opportunity cost of these measures? Is it a net gain? You could, at the extreme, ban all motor vehicles but the opportunity cost would outweigh the benefits. This article confirms my existing bias/belief that user pays and auction[0] based systems improve governmental programs and finite supp", "negative": "Nvidia Stock Crash Prediction. It goes to nearly zero if China invades Taiwan, and that seems like it has at least a 10% chance of happening in the next year or two. > One of the questions of the 2026 acx prediction contest is whether Nvidia\u2019s stock price will close below $100 on any day in 2026. Maybe I\u2019m missing something, but isn\u2019t this just a standard American put option with a strike of $100 and expiry of Dec 31st? He doesn't really address his own question. He's answering the question \"How should options be priced?\" Sure, it's possible for a big crash in Nvidia just due to volatility.  But in that case, the market as a whole would likely be affected. Whether Nvidia specifically takes a big dive depends much more on whether they continue to meet growth estimates than general volatility.  If they miss earnings estimates in a meaningful way the market is going to take the stock behind the shed and shoot it.  If they continue to exceed estimates the stock will probably go up or at least keep its present valuation. How much of their turnover is financed directly or indirectly by themselves, then leveraged further by their 'customers' to collaterize further investments? Are they already \"too big to fail\"? For better or worse, they are 'all in' on AI. This article goes more into the technical analysis of the stock rather than the underlying business fundamentals that would lead to a stock dump. My 30k ft view is that the stock will inevitably slide as AI datacenter spending goes down. Right now Nvidia is flying high because datacenters are breaking ground everywhere but eventually that will come to an end as the supply of compute goes up. The counterargument to this is that the \"economic lifespan\" of an Nvidia GPU is 1-3 years depending on where it's used so there's a case to be made that Nvidia will always have customers coming back for the latest and greatest chips. The problem I have with this argument is that it's simply unsustainable to be spending that much eve"}
{"anchor": "Show HN: Building a web search engine from scratch with 3B neural embeddings. Very nice project. Do you have plans to commercialize it next? This then begs the question for me, without an LLM what is the approach to build a search engine? Google search used to be razor sharp, then it degraded in the late 2000s and early 2010s and now its meh. They filter out so much content for a billion different reasons and the results are just not what they used to be. I've found better results from some LLMs like Grok (surprisingly) but I can't seem to understand why what was once a razor exact search engine like Google, it cannot find verbatim or near verbatim quotes of content I remember seeing on the internet. This is so cool. A question on the service mesh - is building your own typically the best way to do things? I'm new to networking.. At the end, the author thinks about adding Common Crawl data. Our ranking information, generated from our web graph, would probably be a big help in picking which pages to crawl. I love seeing the worked out example at scale -- I'm surprised at how cost effective the vector database was. I been doing a smaller version of the same idea for just domain of job listings. Initially I looked at HNSW but couldn't reason on how to scale it with predictable compute time cost. I ended up using IVF because I am a bit memory starved. I will have to take at look at coreNN. This is really really cool. I had earlier wanted to entirely run my searches on it and though that seems possible, I feel like it would be sadly a little bit more waste of time in terms of searches but still I'll maybe try to run some of my searches against this too and give me thoughts on this after doing something like this if I could, like, it is a big hit or miss but it will almost land you to the right spot, like not exactly. For example, I searched lemmy hoping to find the fediverse and it gave me their liberapay page though. Please, actually follow up on that common crawl promi", "positive": "27M Fewer Car Trips: Life After a Year of Congestion Pricing. non-paywall link:  https://www.nytimes.com/interactive/2026/01/05/upshot/conges...  Sounds like a good reason to not invest in parking garages. I never understood why big, congested cities in the USA (NYC, Boston, L.A., D.C., Chicago) aren't dumping money into funding FSD research hand over fist. It's not a moonshot anymore, and it would be the game-changer of the century in terms of public transportation and GDP. This is a long-term no-brainer for prosperity. The first graph makes no sense? Why are the initial actual and expected values so far apart? Shouldn't they start at the same point? That's valuable real estate for housing. House people, not cars. What's the theory of change here? FSD fundamentally is going to make keeping a car moving on the road cheaper, and making something cheaper makes it happen more. In what world do you get that conclusion? Dense cities in other parts of the world rely on mass transit to move people. FSD so you can have self driving cars in the street -> ? -> increasing congestion. The point is you should have more effective volume transit not optimizing random ones.  1000 cars on FSD are an optimization better than 1000 taxi drivers, compared to a train or a few buses.  https://www.youtube.com/watch?v=040ejWnFkj0  They don't describe the graphic very well in the article, but they do link to the source data [1]. The \"Expected\" line seems to refer to a historical average. Since the starting point of the graph coincides with the beginning of congestion pricing, we would expect a difference between the two values at that point. [1]  https://metrics.mta.info/?cbdtp/vehiclereductions  FSD could largely eliminate privately owned vehicles, it could also allow cities to get rid of most parking infrastructure. It eliminates traffic, parking, and alll the other pain points of owning a vehicle just to get from A to B when a car is the only true option. Certainly, FSD buses would be a w", "negative": "Dithering \u2013 Part 2: The Ordered Dithering. Related:  Dithering - Part 1   https://news.ycombinator.com/item?id=45750954  first post was great, this should be interesting! In chrome it says \"Loading assets, please wait...\" and hangs.  but it works for me in firefox This is really nice work, as are the other posts. If the author stops by, I'd be interested to hear about the tech used. I used ordered dithering in my ZX Spectrum raytracer ( https://gabrielgambetta.com/zx-raytracer.html#fourth-iterati... ). In this case it's applied to a color image, but since every 8x8-pixel block can only have one of two colors (one of these fun limitations of the Spectrum), it's effectively monochrome dithering. Bayer dithering in particular is part of the signature look of Flipnote Studio animations, which you may recognize from animators like kekeflipnote (e.g.  https://youtu.be/Ut-fJCc0zS4 ) Just did a bit of a deep dive into dithering myself, for my project of creating an epaper laptop.  https://peterme.net/building-an-epaper-laptop-dithering.html  it compares both error diffusion algorithms as well as Bayer, blue noise, and some more novel approaches. Just in case anyone wants to read a lot more about dithering! I built a blue noise generator and dithering library in Rust and TypeScript. It generates blue noise textures and applies blue noise dithering to images. There\u2019s a small web demo to try it out [1]. The code is open source [2] [3] [1]  https://blue-noise.blode.co \n[2]  https://github.com/mblode/blue-noise-rust \n[3]  https://github.com/mblode/blue-noise-typescript  There is something very satisfying in viewing media at 100% resolution of your screen. Every pixel is crisp and plays a role. Joy not available by watching videos or viewing scaled images. Half the posts here are people promoting their own projects without even mentioning the (really impressive) OP. Bit weird Bookmarking this. Clear explanations of graphics algorithms are surprisingly rare. Normally I am not a fa"}
{"anchor": "2025 Letter. I recommend Dan\u2019s book ( https://danwang.co/breakneck/ ) to those wanting to better understand China - and the United States. As often the case with Dan's letters, a well balanced take on many issues. I particularly appreciated the thoughts on AI and (what I read) the undertone of infrastructure being the real differentiator between the US effort and China. We'll see how it plays out this year. \"May you live in exciting times\" etc. As someone unfamiliar with the author, I had a deep amount of cynicism for the length of this piece... but damn, it's good, top to bottom. > Beijing has been preparing for Cold War without eagerness for waging it, while the US wants to wage a Cold War without preparing for it. great line Wait, how funny is this guy. That's an easy top 10 funny person out of nowhere in my life. > Which of the tech titans are funny? In public, they tend to speak in one of two registers. The first is the blandly corporate tone we\u2019ve come to expect when we see them dragged before Congressional hearings or fireside chats. The second leans philosophical, as they compose their features into the sort of reverie appropriate for issuing apocalyptic prophecies on AI. This is just not accurate though? For example, this post from a tech titan might not necessarily be that funny but it's neither blandly corporate nor philosophical:  https://x.com/elonmusk/status/2006548935372902751  from the piece: \u201c\nthe median age of the latest Y Combinator cohort is only 24, down from 30 just three years ago\n\u201c does yc publish stats to validate? > I believe that Silicon Valley possesses plenty of virtues. To start, it is the most meritocratic part of America. Oh come on, this is so untrue. Silicon Valley loves credentialism and networking, probably more than anywhere else. Except the credentials are the companies you\u2019ve worked for or whether you know some founder or VC, instead of what school you went to or which degrees you have. I went to a smaller college that the big ", "positive": "Giving Up a $250k Salary to Retire Early Is Hard. This is the OP:  https://www.vetmed.auburn.edu/faculty/erik-hofmeister/  This (+ many other signals) are giving me the \"market top\" vibe. Sad to see people still parroting the 4% rule when you can get \"risk free\" US Treasuries, today, paying more than that. Not to speak of the numerous, still conservative, investments paying far higher. This isn't the 2010s era with ultra low fixed income yields. If you intend to retire early, please educate yourself on the state of the market Someone in the article's comments asked about working part time and the author responded \"veterinary academia doesn\u2019t really understand <1.0 FTE.\" Is the same true of FAANG-ish companies? Can you (officially) work part time in a big tech job? College professor in Alabama makes $250k? Not bad. I guess being a doctor helps. I think the vibe may instead be: growing income disparity. The WSJ reported over the weekend that over 50% of all consumer spending now comes from the top 10% of household incomes. So while some folks are flush to retire early, many are not. I think 4% is still a fine thing to plan around. I don't think it's wise to plan as if today's treasury rates will last your entire retirement. The 4% rule requires increasing that amount at the rate of inflation throughout retirement. 30 years in the case of the original studies. Even with an inflation rate of 2.5%, the required withdrawal will more than double after 30 years. The 4.625% you can lock into a 30-year Treasury would not be enough. What does the 4% rule have to do with yields of treasuries? This is a 30-year time horizon that changes spending purely based on inflation figures. Yields in the market do not matter. You can and many do, although this tends to be reserved for more senior engineers.  Obviously a pay cut is involved. Strictly the answer is yes with the more truthful answer being it depends on your manager. The easiest way is probably being in a country that requires", "negative": "France Aiming to Replace Zoom, Google Meet, Microsoft Teams, etc.. And they can strike back at corporate America by licensing the stuff under gnu licenses. Software that\u2019s reasonably small, reasonably effective and portable. What a concept.  If only the EU or UK had 5-10 hackers\u2026 It's difficult to take an announcement like this seriously when it's posted on Twitter. Earlier:  https://news.ycombinator.com/item?id=46766004  For a fraction of what these products cost France could fund open source alternatives. Edit: I'm not saying they don't. translation (and without twitter): \n https://www-numerama-com.translate.goog/cyberguerre/2167301-...  We need more like this. Europe is totally dependent on US companies for cloud computing. I wonder if the EU will begin trying to recruit American software engineers. I\u2019d love to move to France. I don't see the dependency on these productivity and communication tools as that difficult of a problem to solve. They are going to have a much harder time weaning off American cloud infrastructure and on to something purely domestic. I like CryptPad.fr. End-to-end encrypted google docs. Can access X because it's X and locally blocked, \"ironic\" to use Twitter to post about sovereignty. It's ongoing for a will with La suite num\u00e9rique ( https://lasuite.numerique.gouv.fr/ ). - Tchap is a message app for officials,\n - Visio, based on LiveKit\n - FranceTransfert, I don't know what is it.\n - Fichiers => Drive\n - Messagerie => Email\n - Docs => A better Google Docs\n - Grist => Excel version of Google docs. It aimed at \"public worker\", people working for the government. Github:  https://github.com/suitenumerique  Switching to sovereignty-protecting, locally-hosted collaboration, compute, and storage is by no means impossible. FOSS advocates have been eagerly beating this drum and providing options for 25+ years. The missing ingredient has  always  been the will to absorb the inevitable cost of change, and the friction of choosing something other than"}
{"anchor": "Scientists unlock brain's natural clean-up system for new treatments for stroke. Very interesting, especially in light of the Chinese study\u2019s claiming to have success with a large subset of Alzheimer\u2019s by adding a shunt to the cervical lymphatic nodes, which seems to be exactly what they\u2019re doing here too. For those who don\u2019t want to wait and have someone they love who can benefit from this, simply massaging the lymph nodes in the neck 10 minutes a day also significantly increases flow through these lymph nodes and thereby increases drainage of lymph from the brain. Full article:  https://www.monash.edu/news/articles/scientists-unlock-brain...  Yeah, the body-wide mucous thinning properties of NAC are one of the reasons it has racked up papers showing its efficacy in a truly staggering number of illnesses and conditions. (Including neurodegenerative diseases.) Highly recommend reading the actual literature on its effects in regard to cystic fibrosis, pancreatitis, COPD, neurodegenerative disorders, high blood pressure, ulcers, IBD, liver and kidney problems, OCD... The list goes on at a pretty extreme length, and it sounds too good to be true, but the papers are out there. I love getting my lymph nodes drained. Feels so good afterward. Mainstream science has poo-poohed for years any notion that Oriental medicine practices for facilitating lymph flow have any utility. Nice to hear they're back on the allopathic table. Is this this something that can help with autism symptoms? Hmm, I had a bunch removed due to thyroid cancer. I wonder if that reduced my brains ability to clean itself out. It would be really interesting if we find out that a simple 10 minute daily massage of the lymph nodes in the neck significantly prevents Alzheimer's. Is this something I can do to myself? Is there some kind of video tutorial to see what I really need to do? Makes me wonder if body posture promoting blood flow to the head (yoga or else) can be helpful here too. Can you suggest a revi", "positive": "Gemini Embedding: Powering RAG and context engineering. The Matryoshka embeddings seem interesting: > The Gemini embedding model, gemini-embedding-001, is trained using the Matryoshka Representation Learning (MRL) technique which teaches a model to learn high-dimensional embeddings that have initial segments (or prefixes) which are also useful, simpler versions of the same data. Use the output_dimensionality parameter to control the size of the output embedding vector. Selecting a smaller output dimensionality can save storage space and increase computational efficiency for downstream applications, while sacrificing little in terms of quality. By default, it outputs a 3072-dimensional embedding, but you can truncate it to a smaller size without losing quality to save storage space. We recommend using 768, 1536, or 3072 output dimensions. [0] looks like even the 256-dim embeddings perform really well. [0]:  https://ai.google.dev/gemini-api/docs/embeddings#quality-for...  To anyone working in these types of applications, are embeddings still worth it compared to agentic search for text? If I have a directory of text files, for example, is it better to save all of their embeddings in a VDB and use that, or are LLMs now good enough that I can just let them use ripgrep or something to search for themselves? Question to other GCP users, how are you finding Google's aggressive deprecation of older embedding models? Feels like you have to pay to rerun your data through every 12 months. I feel like tool calling killed RAG, however you have less control over how the retrieved data is injected in the context. > Embeddings are crucial here, as they efficiently identify and integrate vital information\u2014like documents, conversation history, and tool definitions\u2014directly into a model's working memory. I feel like I'm falling behind here, but can someone explain this to me? My high-level view of embedding is that I send some text to the provider, they tokenize the text and then run ", "negative": "'Askers' vs. 'Guessers' (2010). Discussed (in a singleton sort of way) at the time:  Askers vs. Guessers  -  https://news.ycombinator.com/item?id=1956778  - Dec 2010 (1 comment) Edit: plus this!  Ask vs. Guess Culture  -  https://news.ycombinator.com/item?id=37176703  - Aug 2023 (479 comments) I found a good discussion that I keep referring to on Jean Hsu's blog:\n https://jeanhsu.substack.com/p/ask-vs-guess-culture \nand \n https://jeanhsu.substack.com/p/bridging-the-ask-vs-guess-cul...  It's been quite illuminating for people in multicultural teams... Edit: this whole theory seems to come from some internet forum comment! I know a lot of people here are seduced (I was a bit too) but basing your social interactions and how you see others and yourself on this stuff might not be the best thing to do! Original comment below for posterity and because there are answers. ---- I'm not sure this stuff is really  that  helpful. You might be tempted to put people into these categories, but you might have a somewhat caricatural and also wrong image of both which could worsen interactions. By the way, that article doesn't cite any studies! It's probably helpful to know people are more or less at ease asking direct questions or saying no or receiving a no, but it's all scales and subtleties. It could also depend on the mood, or even who one interacts with or on the specific topic). The article touches this a bit (the \"not black and white\" paragraph). We human beings love categories but categories of people are often traps. It's even more tempting when it's easy to identity to one of the depicted groups! I wonder if this asker-guesser thing is in the same pseudoscience territory as the MBTI. In the end, I suppose there's no good way around getting to know someone and paying attention for good interactions. > Your boss, asking for a project to be finished early, may be an overdemanding boor \u2013 or just an Asker, who's assuming you might decline. I don't pay for the Atlantic and thus a"}
{"anchor": "Over 40% of deceased drivers in vehicle crashes test positive for THC: Study. I\u2019m not surprised so many deceased drivers were under the influence of THC. I see people smoking and vaping at stoplights all the time. I am however, surprised this study claims legalization didn\u2019t change the rate. Anecdotally, on the west coast, I\u2019ve seen far more of this, and also people casually smoking in public spaces (parks or train stations or whatever) since legalization. And what share of the remaining 60% were killed by the initial 40%? I am curious what percentge of the general populous test positive for THC. It would give better context to a dead drivers testing positive for THC. Feels like a low sample size, but I'm not statistician or doctor. That said, almost everyone I know that consumes THC has no qualms driving while doing it, and many of them also at work. It's a huge peeve of mine. Based on the headline, I was guessing it was any amount of positivity, and may be close to the population level, but it's actually impairment levels of THC: > In a review of 246 deceased drivers, 41.9% tested positive for active THC in their blood, with an average level of 30.7 ng/mL \u2014 far exceeding most state impairment limits. Since COVID in CA, it feels like driving has become far more dangerous with much more lawlessness regarding excessive speeding and running red lights,  going into the left lane to turn right in front of stopped cars, all sorts of weird things. But I can't tell if my anecdotes are significant. It seems that Ohio's impaired drivers have been consistent through the past six years though. Is there a full study somewhere? I'd expect them to screen for other psychoactive substances as well, of which I see no mention here. Whenever you think to yourself \"People couldnt be that stupid, right?\" read this study and plan accordingly. Wish the paper were available - would love to know the percentage with alcohol. The other question I have - my prior is that a bad driver (tired, d", "positive": "I built my own CityMapper. Before Citymapper existed, there was OneBusAway, a Ph.D. student project at the University of Washington. It still exists and powers millions of transit rider trips every day all around the world in Seattle, Washington DC, New York City, Poznan Poland, Buenos Aires Argentina, Adelaide Australia, and who knows where else. If you\u2019re interested in hacking on something like Citymapper, or setting up an OBA server for your own city, you can find everything you need on our GitHub organization:  https://github.com/OneBusAway  That includes docker images, an iOS app and a trip planner framework, android app, Sveltekit web app, and even a next generation OBA server written in Go. As far as the data to power this, you can get GTFS for every US transit agency from  https://mobilitydatabase.org/  (nb I\u2019ve been involved in the OBA project since 2012) Why are the table and the description of the RAPTOR algorithm in the article images rather than text? During university, we've built OptiTravel ( https://github.com/denysvitali/optitravel ) to do something similar. We couldn't use Google Maps APIs (project requirement), so we wrote a custom routing algorithm based on A* and I've created a Rust server to host GTFS data ( https://github.com/denysvitali/gtfs-server ) \u00e0 la Transitland ( https://transit.land/ ). Performance wasn't great since everything had to run locally and do network roundtrips, but it found routes in my hometown that Google Maps didn't show. Pretty cool discovering hidden connections in the transit network and being able to customize your own params ( https://github.com/denysvitali/optitravel/blob/master/src/ma... ) I am involved with the OpenTripPlanner project, which is a Java trip planning application that also uses the RAPTOR algorithm! It\u2019s used in cities all over the world, with the biggest deployment being ENTUR\u2019s in Norway, which covers the entire country. I believe all trip planning apps in Norway use this deployment. It supports m", "negative": "DECwindows Motif. Anyone here going to the VMS bootcamp? [1] [1]  https://events.vmssoftware.com/bootcamp-malmo-2026  I miss Motif. This is a portal to a time when men were men and UNIX(R)\u2014or in this case, VMS\u2014desktops were utilitarian and did exactly what you needed and nothing more. Now we live in a time where we allocate GBs of RAM to eye candy that functionally accomplishes nothing. Then we make the case to rewrite the eye candy in increasingly \"safe\" languages, requiring even more RAM. I saw DEC windows and immediately thought of Windows NT 3.1.  https://en.wikipedia.org/wiki/Windows_NT_3.1  [edit] So, I guess the history of 'windows NT' is lost on many. 'NT' started with version 3.1 as the MS/IBM breakup from the joint OS/2 venture happened. It was their first real push into 32 bit protected mode operating systems and supported really crazy cool things like 'multiple processors' and totally different architectures than x86, like DEC. Give the link a look. I dunno what's interesting about this link, but Motif has been LGPL a while and the last release was in 2017.  https://sourceforge.net/projects/motif/files/   https://en.wikipedia.org/wiki/Motif_%28software%29  (in some alternate universe, motif was under the x11 license and you would have motif v13 instead of GTK.) There are at least, for those wanting a Linux or BSD based Motif fix: Enhanced Motif Window Manager  https://fastestcode.org/emwm.html  and the full-fledged CDE desktop that uses Motif also:  https://sourceforge.net/projects/cdesktopenv/   (note that you want to firewall this somehow as the default settings on the background process ttdb can be a security hole) Red Hat recently removed Motif from their distribution. I wonder if it's still in AIX.  https://access.redhat.com/solutions/6113101  Dual booting OpenVMS (not virtualization) would be a really cool thing. I purchased a copy of OSF Motif for Linux (x86) sometime in the early 1990's (before it was free).  I had used it before on SunOS and I l"}
{"anchor": "DeepSeek-v3.1. For reference, here is the terminal-bench leaderboard:  https://www.tbench.ai/leaderboard  Looks like it doesn't get close to GPT-5, Claude 4, or GLM-4.5, but still does reasonably well compared to other open weight models. Benchmarks are rarely the full story though, so time will tell how good it is in practice. It's a hybrid reasoning model. It's good with tool calls and doesn't think too much about everything, but it regularly uses outdated tool formats randomly instead of the standard JSON format. I guess the V3 training set has a lot of those. It seems behind Qwen3 235B 2507 Reasoning (which I like) and gpt-oss-120B:\n https://artificialanalysis.ai/models/deepseek-v3-1-reasoning  Pricing:  https://openrouter.ai/deepseek/deepseek-chat-v3.1  Unrelated, but it would really be nice to have a chart breaking down Price Per Token Per Second for various model, prompt, and hardware combinations. For local runs, I made some GGUFs! You need around RAM + VRAM >= 250GB for good perf for dynamic 2bit (2bit MoE, 6-8bit rest) - can also do SSD offloading but it'll be slow. ./llama.cpp/llama-cli -hf unsloth/DeepSeek-V3.1-GGUF:UD-Q2_K_XL -ngl 99 --jinja -ot \".ffn_.*_exps.=CPU\" More details on running + optimal params here:  https://docs.unsloth.ai/basics/deepseek-v3.1  Seems to hallucinate more than any model I've ever worked with in the past 6 months. About halfway between V3 and Qwen3 Coder.  https://brokk.ai/power-ranking?version=openround-2025-08-20&...  Cheep! $0.56 per million tokens in \u2014 and $1.68 per million tokens out. They say the SWE bench verified score is 66%. Claude Sonnet 4 is 67%. Not sure if the 1% difference here is statistically significant or not. I'll have to see how things go with this model after a week, once the hype has died down. Looks quite competitive among open-weight models, but I guess still behind GPT-5 or Claude a lot. I have yet to see evidence that it is better for agentic coding tasks than GLM-4.5 Sad to see the off peak discount", "positive": "280M e-bikes and mopeds are cutting demand for oil far more than electric cars. We don't need a 4000 lb vehicle to move a ~200 lb person. In order of efficiency: (1) Walk (2) Unicycle, roller skate, scooter (no battery, very little material) (2) Bike (3) Electric bike (and all forms of newfangled electric: escooters, segways) (4) Electric motorbike or scooter (5) Mass transit (can be public/private) transportation: Electric trains (6) Mass transit (can be public/private) transportation: Electric buses (7) Zipline (8) Carpools on BEV (9) Carpools on PHEV (10) BEV We can stop buying gas cars. Pollution kills 10 million EVERY year[1]. For context, the cumulative COVID deaths over 3 years are ~6.5 million. And fossil fuels are subsidized (Trillions of dollars per year). For 2022, this is $7 trillion[2]. Why are we subsidizing fuels that are proven to cause all kinds of diseases (nearly everything except STIs). [1] Air Pollution Kills 10 Million People a Year. Why Do We Accept That as Normal?:  https://www.nytimes.com/2022/07/08/opinion/environment/air-p...  [2] Why Are Governments Still Subsidizing Fossil Fuels?  https://www.bloomberg.com/opinion/articles/2023-10-16/climat...  [3]  https://www.imf.org/en/Blogs/Articles/2023/08/24/fossil-fuel...  I'm not really sure where in the US people live that makes them think transportation can be replaced by a bike. I live in San Diego, the climate is great, but there is no way a person can travel any farther than their neighborhood on a bike. The main impediment at this point is the outrageous price of EVs in the US. In China cheap EVs are readily available, trade policies are preventing their import into the US. Bikes, \"e\" or otherwise are a great way to get around the neighborhood, but most people are not able to restrict their travel to a 10 mile radius. And weather as well as traffic safety are serious mitigations of bike transport. \"So what\u2019s the best solution? You might think switching to an electric vehicle is the natural ", "negative": "Porting 100k lines from TypeScript to Rust using Claude Code in a month. For typing \u201cyes\u201d or \u201cy\u201d automatically into command prompts without interacting, you could have utilized the command \u2018yes\u2019 and piped it into the process you\u2019re running as a first attempt to solving the yes problem. \n https://man7.org/linux/man-pages/man1/yes.1.html  How much does it cost to run Claude Code 24 hrs/day like this. Does the $200/month plan hold up? My spend on Cursor has been high... I'm wondering if I can just collapse it into a 200/month CC subscription. I'm hoping that one day we can use AI to port the millions of lines in the modules of the Python ecosystem to a GIL-free version of Python. Did you ever consider using something like Oh My Opencode [1]?\nI first saw it in the wake of Anthropic locking out Opencode. I haven\u2019t used it but it appears to be better at running continuously until a task is finished. Wondering if anyone else has tried migrating a huge codebase like this. [1]  https://github.com/code-yeongyu/oh-my-opencode  Some quotes from the article stand out: \n\"Claude after working for some time seem to always stop to recap things\"\nQuestion: Were you running out of context? That's why certain frameworks like intentional compaction are being worked on.  Large codebases have specific needs when working with an LLM. \"I've never interacted with Rust in my life\" :-/ How is this a good idea? How can I trust the generated code? This is actually pretty incredible. Cannot really argue against the productivity in this case. Honestly I am really interested in trying to port the rust code to multiple languages like golang,zig, even niche languages like V-lang/Odin/nim etc. It would be interesting if we use this as a benchmark similar to  https://benjdd.com/languages/  or  https://benjdd.com/languages2/  I used gitingest on the repository that they provided and its around ~150k tokens Currently pasted it into the free gemini web and asked it to write it in golang and it said that li"}
{"anchor": "Show HN: See what readers who loved your favorite book/author also loved to read. Btw, if you want to share your 3 favorite reads of the year, please share those here:  https://shepherd.com/bboy/my-3-fav-reads/login?next=/bboy/my...  You get a cool page like this:  https://shepherd.com/bboy/2025/f/bwb  I read ~130 books this year, and my 3 favorites of the year were: Dungeon Crawler Carl by Matt Dinniman I kept seeing recommendations for this book on Shepherd, but I was reluctant to try it. Many years ago, I tried a progressive fantasy book, and it left a bad taste in my mouth. This was a colossal mistake on my part because Dungeon Crawler Carl is AMAZING. This is one of the funniest and most beautiful books I have ever read. The satire is biting, and I love the characters from the bottom of my heart. If you love the TV show \u201cAlways Sunny in Philadelphia,\u201d you will love the dark, absurd humor of this book. And this book isn\u2019t all laughter; the characters often moved me to tears as they try to hold on to their humanity in the face of utter inhumanity and insanity. The Comfort Crisis by Michael Easter One of my favorite concepts in the book is called a \u201cmisogi.\u201d It is this idea of taking on one massive challenge each year, with a 50/50 chance of failure (don\u2019t die is rule #1). Fall of Giants by Ken Follett This book series is pure magic. It\u2019s hard to put into words what Ken Follett has accomplished. I read a LOT of historical fiction, and I\u2019ve never found another series that lets you live through history with characters you love, while also showing the sweeping forces that shape the world. It makes for intense reading because you will experience the day-to-day reality of fighting for women\u2019s right to vote in England or resisting the Nazi party\u2019s slow takeover of Germany, and you do this through the eyes of characters you have grown to love. You feel what it is like on a daily basis, frustrated with the pace of change, and also just living the regular ups and downs of ", "positive": "Show HN: isometric.nyc \u2013 giant isometric pixel art map of NYC. Appreciate that writeup. Very detailed insights into the process. However those conclusions left me on the fence about whether I 'liked' the project. The conclusions about 'unlocking scale' and commodity content having zero value. Where does that leave you and this project? Does it really matter that much that the project  couldn't  exist without genAI? Maybe it shouldn't exist then at all. As with alot of the areas AI touches, the problem isn't the tools or use of them exactly, it's the scale. We're not ready for it. We're not ready for the scale of impact the tech touches in multitude of areas. Including the artistic world. The diminished value and loss of opportunities. We're not ready for the impacts of use by bad actors. The scale of output like this, as cool as it is, is out of balance with the loss of huge chunk of human activity and expression. Sigh. Very impressive result! are you taking requests for the next ones?  SF :D Tokyo :D Paris :D Milan :D Rome :D Sydney :D Oh man... > This project is far from perfect, but without generative models, it couldn\u2019t exist. There\u2019s simply no way to do this much work on your own, Maybe, though a guy did physically carve/sculpt the majority of NYC:  https://mymodernmet.com/miniature-model-new-york-minninycity...  I see you used Gemini-CLI some but no mention of Antigravity. Surprising for a Googler. Reasons? > Slop vs. Art > If you can push a button and get content, then that content is a commodity. Its value is next to zero. > Counterintuitively, that\u2019s my biggest reason to be optimistic about AI and creativity. When hard parts become easy, the differentiator becomes love. Love that.  I've been struggling to succinctly put that feeling into words, bravo. Not working here, some CORS issue. Firefox, Ubuntu latest. Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at  https://isometric-nyc-tiles.cannoneyed.com/dzi/tiles_me", "negative": "Show HN posts p/month more than doubled in the last year. I suspect that this will drive the folks who insist LLM productivity gains are the real hallucinations truly bonkers. I think I read some days ago another stat, that the average rating of \u201cShow HN\u201d posts is going down. So the pessimistic take is that people feel the bar to present their product in a \u201cShow HN\u201d is lowering. (edit: striked) <strike>Is it deliberate that this post appears as \u201cShow HN\u201d itself? I hope not to be too negative, but to qualify as such I would expect much more that a page with two graphs.</strike> > Show HN is for something you've made that other people can play with. HN users can try it out, give you feedback, and ask questions in the thread. This is an interesting post, but not a Show HN. Would be nice to see some qualitative analyis to know if it's just slop, or actually more interesting projects. Not sure how to do that though. I think just looking at votes wouldn't work. I would guess more posts causes lower average visibility per post which should cause upvotes to slump naturally regardless of quality. Edit: maybe you could: - remove outliers (anything that made the front page) - normalise vote count by expected time in the first 20 posts of shownew, based on the posting rate at the time One of the reasons is that there are a lot of adverts masquerading as Show HN. Do you have any numbers on the number that get some number of upvotes? What about a chart of upvotes on Show HN? I assume the vast, vast majority never get any upvotes. Probably the same happening for websites being built or apps being published. This is very similar to  https://news.ycombinator.com/item?id=46702099  posted 4 days ago. And there is also this  https://dewmal.medium.com/hacker-news-is-a-living-time-capsu...  Laid off people have more time on their hands, while on llm-powered steroids? A better metric would be how many Show HN posts are reaching the front page. Related: \"Data on AI-related Show HN posts\" O"}
{"anchor": "Bus stops here: Shanghai lets riders design their own routes. This is really brilliant \u2014 like desire paths, but for transit. Obviously execution will be challenging, but the concept is fantastic, and China/Shanghai seems like one of the few places with the requisite density & state capacity to actually make this work. Generally I think that the design of public spaces has SO MUCH room to be improved by just responding to the wisdom of the crowd. I'm glad that Shanghai has moved to the next level in public transportation in meeting customer demand. Most cities don't have the funds to buy smallish buses and labour available as drivers. They don't have the money or willpower to get frequencies to turn up and go levels (ie frequent) and leave people with long walks to widely spaced routes. Tangent: I\u2019ve often thought that it would be great to let people design their own political districts to reduce gerrymandering At the polling place you\u2019d get a map with your census tract and then be asked \u201cwhich two or three adjacent tracts are most similar to your community\u201d. Eventually you\u2019d end up with some sort of gram matrix for tract-to-tract affinity, and then you could apply some algorithmic segmentation. Two problems: - this is far too complex for most voters to understand, much less trust, what\u2019s happening - the fact it\u2019s \u201calgorithmic\u201d would give a sheen of pseudo objectivity, but the selection of the actual algorithm would still allow political infouence over boundaries Chiming in from Los Angeles, USA to say wow, must be nice living in a modern society that prioritizes public transit and peoples' ease of movement. I know, I know, it comes with trade offs of living in an authoritarian state, but the absolute abysmal state of infrastructure in this country is maddening. Ever been on a train in Denmark or Japan or Switzerland? This remind me that road router should be walked by passenger rather than designed by designers. China is the only modern country that has both the cap", "positive": "UN declares that the world has entered an era of 'global water bankruptcy'.  Four billion people face severe water scarcity for at least one month each year  Does anyone know what this looks like for typical cases? The water just cuts off for a month in some places I guess? I can assure you there is plenty of water.    There are floods in lots of places every year.   The oceans are full of water that for just 5kWh we can desalinate 250 gallons. The problem is that the water and energy aren't where the users want it to be. But pipes are relatively cheap - if humanity cared enough, we could build pipes to distribute the plentiful water everywhere. But it turns out the people without much water tend to be in very poor places and warzones where there isn't much appetite for spending money on pipes. And all these huge new data centers are gonna make things worse:  https://www.eesi.org/articles/view/data-centers-and-water-co...  Before commenting water is cheap and plentiful please read the proposed definition. > Water bankruptcy refers to \u201ca state in which a human-water system has spent beyond its hydrological means for so long that it can no longer satisfy the claims upon it without inflicting unacceptable or irreversible damage to nature.\u201d I find this other article [1] more informative, including for instance the global map of Vulnerability to Water-Related Challenges taken from the actual report [2]. [1]  https://www.thebrighterside.news/post/our-world-is-entering-...  [2]  https://collections.unu.edu/eserv/UNU:10445/Global_Water_Ban...  I would no say the \"world\", but areas of it has as noted.  Like South Asia, SW N America, N Africa and Spain. For many of these areas, desalination could meet the gap, but someone will need to pay for it.  That is the main issue, no one wants to pay. Sounds like a bunch of useless scare mongering. Large scale Desalination is getting increasingly achievable:\n https://caseyhandmer.wordpress.com/2022/11/20/we-need-more-w...  Reminds me o", "negative": "Ask HN: Books to learn 6502 ASM and the Apple II. Pretty much the best resource available:  https://6502.org/  Check the books section and find something that compels you. Also, don't forget the HUGE number of resources for 6502 assembly programming that are available in the  https://archive.org/  magazine and book sections:  https://archive.org/search?query=6502  Rodney Zaks' books are great - I like especially \"6502 Games\", which taught me a lot back in the day:  https://archive.org/download/6502g/6502Games.pdf  I'm also especially fond of the easy6502 emulator - its a very handy tool to have while studying 6502 techniques:  https://skilldrick.github.io/easy6502/  Its not absolutely necessary to learn BASIC before Assembly, but it will definitely help you understand the resources of the machine better if you can debug BASIC ROM code.  My personal 6502 platform of choice, the Oric-1/Atmos machines, has a pretty great ROM disassembly available, from which a lot of great knowledge can be obtained - but it does of course first require an undersanding of BASIC. In case you're curious, the Oric-1 ROM Disassembly:  https://library.defence-force.org/books/content/oric_advance...  (You can get an Oric emulator named Oricutron, or you can access a virtual Oric here:  https://oric.games/  ..) Good luck! This is the book I used when I was writing serial drivers for Apple II ProDOS:  https://archive.org/details/6502_Assembly_Language_Programmi...  And I have a vague memory of this book:\n https://archive.org/details/aiimp/mode/2up  Not sure what level you're at, but I can't remember if this is the text Jef Raskin wrote, but it's a decent backgrounder:\n https://archive.org/details/aiirm/mode/2up  I believe one of the \"standard works\" to learn 6502 back in the day was  Programming the 6502  by Rodnay Zaks. It's out of print, but it was printed in a lot of copies so you should be able to find one second-hand. I'm seconding the recommendation to look at Rodnay Zack's books. For exa"}
{"anchor": "One Head, Two Brains: The origins of split-brain research (2015).  https://web.archive.org/web/20241228215151/https://www.theat...   https://archive.ph/gJ32A  The confabulation to justify picking out related images that the left brain never observed (chicken and snow shovel in the article) reminds me profoundly of the confident slop produced by LLMs. Make you wonder if llms might be one half of the \"brain\" of a true AGI I disagree with Steven Pinker\u2019s claim that consciousness arises from the brain. This perspective fails to establish that the brain produces consciousness, as it relies on the mistaken assumption that \"mind\" and \"consciousness\" are interchangeable. While brain activity may influence the mind, consciousness itself could be a more fundamental aspect of reality. Rather than generating consciousness, the brain might function like a radio, merely receiving and processing information from an all-pervasive field of consciousness. In this view, a split-brain condition would not create two separate consciousnesses but instead allow access to two distinct streams of an already-existing, universal consciousness. Related:  You Are Two  by GCP Grey: < https://www.youtube.com/watch?v=wfYbgdo8e-8 > Looks like this was one of the inspirations behind severance. The fact that the explaining part of the brain fills in any blanks in a creative manner (you need the shovel to clean the chicken shed), reminds me to some replies of LLMs. I once provided an LLM the riddle of the goat, cabbage and wolf, and changed the rules a bit. I prompted that the wolf was allergic to goats (and hence would not eat them). Still the llm insisted on not leaving them together on the same river bank, because the wolf would otherwise sneeze and scare the goat away. My conclusion was that the llm solved the riddle using prior knowledge plus creativity, instead of clever reasoning. If one is interested in hemisphere theory, including psychological and philosophical implications, make sure to chec", "positive": "Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding Comparison. TL;DR If you want to jump straight to the conclusion, I\u2019d say go for Gemini 2.5 Pro, it\u2019s better at coding, has one million in context window as compared to Claude\u2019s 200k, and you can get it for free (a big plus). However, Claude\u2019s 3.7 Sonnet is not that far behind. Though at this point there\u2019s no point using it over Gemini 2.5 Pro. From my use case, the Gemini 2.5 is terrible. I have a complex Cython code in a single file (1500 lines) for a Sequence Labeling. Claude and o3 are very good in improving this code and following the commands. The Gemini always try to do unrelated changes. For example, I asked, separately, for small changes such as remove this unused function, or cache the arrays indexes. Every time it completely refactored the code and was obsessed with removing the gil. The output code is always broken, because removing the gil is not easy. Is there a less biased discussion? The OP link is a thinly veiled and biased advert for something called composio and really a biased and overly flowery view of Gemini 2.5 pro. Example: \u201cEveryone\u2019s talking about this model on Twitter (X) and YouTube. It\u2019s trending everywhere, like seriously. The first model from Google to receive such fanfare. And it is #1 in the LMArena just like that. But what does this mean? It means that this model is killing all the other models in coding, math, Science, Image understanding, and other areas.\u201d For Gemini: play around with the temperature: the default is terrible: we had much better results with (much) lower values. Gemini is the only model which tells me when it's a good time to stop chatting because either it can't find a solution or because it dislikes my solution (when I actively want to neglect security). And the context length is just amazing. When ChatGPT's context is full, it totally forgets what we were chatting about, as if it would start an entirely new chat. Gemini lacks the tooling, there ChatGPT is far ahead, b", "negative": "Show HN: We Built the 1. EU-Sovereignty Audit for Websites. Checks hosting, analytics, fonts, cdn, video, chat, social embeds.\nGives you a score from 0-100 and suggests Eu-alternatives. Nice, good idea. I need to move away from Github pages finally ;) Seems to treat finnish kapsi.fi hosting as US? Happy to see mastodon.xyz score 100%. Mastodon is pretty cool and proof that we can make federation work. nice idea, are you planning to open source this project? Any recommendations for good European alternatives to Clooudflare? Is there an EU company that's as trustworthy when it comesq to DDoS protection? thanks for this checker, we also need HN alternative for EU only. As Europeans, I'm sure we can do this. reddit.com gets a perfect \"no US dependencies\" score. I guess they have servers around the world and can serve requests from a local-ish server. Obviously this simple check only concerns the technical aspects of the website and doesn't analyse the business itself but I wonder if all .com domains should be marked down? I put in my site and it gave me a red cross for \"Hosting\", on hover it said \"GitHub Pages\". But my site isn't hosted on GitHub Pages. Expanding \"Details\", the URL that is hosted on GitHub Pages is... a different website? There's merely a hyperlink to it on my website. It also says I'm using \"self-hosted\" fonts - but I don't think I'm doing that at all? I'm just using the browser's fonts. Using non-standard fonts is a bad idea because it causes the content to either be invisible until the font is loaded, or else it initially shows in a fallback font and then the text all jumps when the font is loaded. I have some feedback for OP: my personal website got 92% because there is a link to my X profile in the contact session. It's not like it relies on the service. Its just a contact and there are also links to other services such as self hosted matrix. On the other hand my registrar is Namecheap which is in the US and your tool didn't checked for that. I thi"}
{"anchor": "Ask HN: What did you find out or explore today?. I found out today that the location header of an HTTP redirect can be a tel:+ URI and phone's will actually ask you whether you want to call that number. I was reminded of the US Constitution's 10th amendment and reading some of the history around it. > The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people. Very relevant to what's going on today with National Guard and ICE deployments.  https://www.axios.com/2026/01/14/10th-amendment-ice-trump-il...  (or please google whatever source you find reliable about the topic) I've been exploring the origins of the 'relational turn' in psychoanalysis that began after WWII and ramped up in the 1970s. Psychoanalysis got vastly more interesting after Freud and I had no idea! I've been trying to research drone navigation tech from what we have learned so far from the russian/ukraine war. I'm very much not a hardware guy but software by itself has been feeling kind of useless or even crueler than usual. Published an edit today (post dated in Nov. but I've rewritten it 5x now) on my tutorial to use llama3.2:3b to generate fine tuning data to train tinyllama1.1b  https://seanneilan.com/posts/fine-tuning-local-llm/  It took a while to figure out that when I made llama3.2 generate json, it didn't have enough horsepower to generate training data that was varied enough to successfully fine tune llama1.1b! Figured that out :) Something you never learn with the bigger models. Every token costs something even if it's a little bit. I'm building in robotics. Setting up a new 3d camera today. I found that the 10m active USB C cable that I bought transfers power in both directions, but only transfers data in one direction, it turns out to be some weird video USB variant. Next I needed to plug a gripper into a modbus controller. That uses an M8 8-pole 20cm cable. The controller manufacturer", "positive": "Sergey Brin's Unretirement. > Having given so much of themselves to their careers, they often felt unmoored and purposeless when they left their jobs. That's in contrast with all of us who see the companies led by these guys as the cancer of society and we'd quit and never look back if we had FU money. My feelings aside, if all their purpose is to grow their company, I kinda get why they wouldn't give a damn about bettering the mankind, improving their communities or raising a healthy family. Financial freedom is about not having to worry about losing your job, or tolerating shitty work conditions. Why would you retire if you do what you love? I think the real problem might be if there's nothing you actually love doing (long term), that's when money won't help. Once you're hooked, you're hooked When I started my company, we suddenly found that we were in a good small fortune, not enough to be millionaires or billionaires, but enough to get people to run the business semi automatically with very minimum input from the founders. I took a semi retirement approach to the business, there really wasn't a lot of things to do, my role was sort of just \"managing\" programmers. I got so much free time that I could even start a second business on the side. Despite my best ability to stretch my work, I couldn't even fill up half of my working hours. One would have thought that this is heaven. But the time I was most free was also the time I was most miserable. I wasn't happy, I was gaining weight, I was perennially asking myself why the business couldn't be bigger and I couldn't sell it, so that I can be real millionaires and billionaires with financial freedom! Then fate intervened, the sudden fortune disappeared and I no longer had the luxury of just \"managing people\"; I have to do hands-on. And it was this activity, the feeling that I was contributing to something, that I was writing code again and actually building stuffs, that made me happy again. Today we are bigger than w", "negative": "There's only one Woz, but we can all learn from him. Woz is by far the person in computing history for whom I have the most respect. Dude is an absolute  legend , and from everything I have heard is humble and kind on top of his crazy skills. If I could get to the point where I had even 10% of his skill and generosity of spirit, I would consider myself to have done pretty well. It\u2019s a stark contrast to today's mindset where we often just throw more resources at the problem.  His obsession with elegance over features is something I try to keep in mind, even if it's harder in modern web dev. \" Let's make it shorter and punchier.  \"Woz's floppy disk controller design is still the gold standard for doing in software what competitors needed a whole board of chips to do.  That kind of obsession with elegance over brute force is exactly what's missing in modern engineering. Only one Woz? What about Scott? Coincidentally one of the earliest Apple I prototypes ends its auction tomorrow if you have over $500K to spare:  https://news.ycombinator.com/item?id=46605420  For me, anyone who is involved in FOSDEM in any way deserves more respect (regarding revolutionary things we can learn) I learned some very bad jokes from him. It's kinda funny... In '89 a friend and I were talking about starting a startup like the two Steve's (we didn't know about Ron Wayne back then.)  We both knew exactly what Woz did, but were a bit sketchy on Jobs role in the early days.  Don't get me wrong, I'm not saying Jobs was a layabout, only that the strengths he brought to the table were more abstract. So I would also say... the kinds of things we learn from Woz are concrete and we get immediate feedback if we learned them wrong. Had to let this here: A TV clip on YouTube of an episode of \u201cThat\u2019s Incredible\u201d, featuring Apple co-founder Stephen \u201cWoz\u201d Wozniak (aged 38) running through a maze and nearly winning.  https://www.youtube.com/watch?v=PoJexQjoMtk  (found on the blog of Cabel Sasser:  https://ca"}
{"anchor": "Is It Time for a Nordic Nuke?. you also need submarines to have a \"credible\" second strike deterrent. It's not enough to just have a bomb. you can have a tiny nuclear war, as a treat. insane we're back here. Following Betteridges Law the answer is of course No It's time to disarm Russia. Edit: to be more clear: I can't believe that after 4 fucking years, a hostile nation is still permitted to wage war against a sovereign country. nuclear weapons are one of those insidious technologies that are almost self-replicating in a sense, because if your enemy has nukes it strongly behooves one to either also have nukes or to buddy up with someone who has them.  Opting out entirely is very difficult once the genie is out of the bottle I get why they would want them but it seems so clear to me that the world is going to end in fire Time for a Nordic nuke is at least from 2008. Russia is genocidal, US unreliable and erratic. The civilized world needs nukes. Not only the Nordic countries, but also Germany and Poland. Unless Russia and US are willing to give up theirs ;) Instead of nukes, maybe another massive pandemic super virus that kills off half the population of humans on Earth wouldn\u2019t be so bad. This is what Trump's dismantling of US power has brought us to. Our soon-to-be-former allies can't count on US nuclear deterrence to protect them, because not only is the US unreliable, but they might be the ones attacking. We're in crazy-town because of Trump and the Republicans, with very real consequences. I don't think there's any question at this point that it's in Nordic self interest to develop a nuclear deterrent. This has also become true for other regions in the world. This is all a horrible development for the overall future of humanity, but it's the world we live in now. At a minimum hundreds of billions of dollars will be siphoned off from more beneficial uses over the coming decades, and the risk of major accidents will increase. The worst change is of course the fac", "positive": "Macron says \u20ac300B in EU savings sent to the US every year will be invested in EU.  https://streamable.com/m4dejv  Related: > Savings and investments union  https://finance.ec.europa.eu/regulation-and-supervision/savi...  Macron is making up numbers. Unless the EU member states actually impose capital controls, investors will continue to send their capital wherever it can earn the highest returns. Profitable investment opportunities in the EU remain slim and so far they seem uninterested in pursuing a growth policy. When will the EU understand that they have the GOAT Paul Graham across the channel in the UK? Open YCombinator Paris or London: Capital would flow to him. The EU can\u2019t even get a Mercosur deal closed after 30 years. I think this will probably happen in another 60 years. If like me you are wondering why the sunglasses, it looks like he is using that to mask an eye infection.  https://www.independent.co.uk/news/world/europe/france-emman...  TIL EU savings rate is far more than US. 18.79% vs 3.50% Guessing that's somehow counting enforced deductions off paycheques. Would be a wild difference if not.  https://tradingeconomics.com/european-union/personal-savings   https://tradingeconomics.com/united-states/personal-savings  So no more MSCI World in Europe? Please update the link to  https://streamable.com/m4dejv  or the transcript of the entire speech  https://www.weforum.org/stories/2026/01/davos-2026-special-a...  Love that MEGA acronym someone dropped in the comments. I need a blue hat with a golden MEGA lettering embroided. So, how is this going to work? Is he talking about the French Ministry of Economics and Finance? About the Banque de France? About the the ECB? Afaik the last two are, nominally at least, independent, while Macron is just a politician representing one of the 27 EU countries, so what authority does he have? What do the political leaders of Latvia think about this? Or of Malta? Funny that of all people, Macron says that. Just a few months", "negative": "Airfoil (2024).  https://news.ycombinator.com/item?id=39526057  I was just thinking the other day about how AI will pretty soon be able to create this kind of explainers on everything quite quickly. Amazing times! Where can I find more articles where things are explained in this manner? Ok that's long, one top line thing people tend to miss in these flying explanations is that airfoil shape isn't about some special sauce generating lift.  A flat plate generates any amount of lift you want just fine.  Airfoil design is about the ratio of lift to drag most importantly and then several more complex effects but NOT just generating lift. (stall speed, performance near and above the speed of sound, laminar/turbulent flow in different situations, what you can fit inside the wing, etc) That's the missing course for the first year of any Aerospace Engineering faculty. I wish there was an infinite number of blogs that where this good. Bartosz Ciechanowski, the gift that keeps on giving. He usually posts these brilliant explanations once or twice a year but nothing in 2025. I hope he finds the time to continue because the lessons are really really brilliantly told. This is absolutely amazing. For those of us programming nerds that want to play with aerodynamics, I can't recommend AeroSandbox enough. While the code is pretty obviously written for people who know their way around aerodynamics and not so much around programming, it is remarkably powerful. You can do all sorts of aerodynamic simulations and is coupled with optimization libraries that allow you to do incredible aerodynamic optimizations. It comes included with some pretty powerful open weight neural network models that can do very accurate estimates of aerodynamic characteristics of airfoils in a fraction of the time that top tier heuristic solvers (like xfoil) can do (which are already several orders of magnitude faster than CFD solvers).  https://github.com/peterdsharpe/AeroSandbox  Oh man. This guy. His work is "}
{"anchor": "Nine things I learned in ninety years. It feels reassuring that none of these surprised me, and I strive towards a lot of these views/learnings already.  Hopefully a good sign!  Packard's writings help give me a little more clarity too, especially when written in such a thoughtful way.  Very cool  <3 I still have my first edition Cave of Time. I don't think it's a first printing though but still, when they came out it was simply awesome. I got the first 6 books in a pack for my birthday, I shared them later on with my children and they loved them too when they were young. Love these. This quote stuck out to me: > It follows, I think, that the luckier you\u2019ve been, the more humility and generous spiritedness you need, and the unluckier you\u2019ve been, the more compassion for yourself you need, and unfair as it may seem, the more you need irrepressible resolve. a completely different topic & question on this post.\nhow is his blog made? I like the style (simple yet clear and beautiful). Anyone know what direction i should look at? It is the simple things that is the hardest. If anything, having children revealed many of the mentioned. To me, having children is enlightenment > what an outsized role is played by luck This speaks to me. So much of our life circumstances are beyond our control (parents, genetics, geography, society, wider economy, etc.) It's humbling, how much of our success or failure is influenced by pure chance. Since he might not be known to most (especially a younger audience), the author is a writer best known for many of the Choose Your Own Adventure books that were hugely successful in the 80s. Jimmy Maher wrote about them recently  https://www.filfre.net/2025/09/choose-your-own-adventure/  ...and use LaTeX Thanks for sharing this <3 Thank you, Kevin, for sharing this, and thank you for your insights, Edward. As a young man without a father anymore, it's always a pleasure learning about people's life experiences to help me be my best self without years", "positive": "Gemini CLI. Link of announcement blog post:  https://blog.google/technology/developers/introducing-gemini...  These always contain easter eggs. I got some swag from Claude Code, and as suspected, Gemini CLI includes `/corgi` to activate corgi mode. > That\u2019s why we\u2019re introducing Gemini CLI Definitely not because of Claude Code eating our lunch! symptomatic of Google's lack of innovation and pm's rushing to copy competitor products better question is why do you need a modle specific CLI when you should be able to plug in to individual models. The killer feature of Claude Code is that you can just pay for Max and not worry about API billing. It lets me use it pretty much all the time without stressing over every penny or checking the billing page.\nUntil they do that - I'm sticking with Claude. Hope this will pressure Anthropic into releasing Claude Code as open source. I have been using this for about a month and it\u2019s a beast, mostly thanks to 2.5pro being SOTA and also how it leverages that huge 1M context window. Other tools either preemptively compress context or try to read files partially. I have thrown very large codebases at this and it has been able to navigate and learn them effortlessly. I love how fragmented Google's Gemini offerings are. I'm a Pro subscriber, but I now learn I should be a \"Gemini Code Assist Standard or Enterprise\" user to get additional usage. I didn't even know that existed! As a run of the mill Google user I get a generous usage tier but paying them specifically for \"Gemini\" doesn't get me anything when it comes to \"Gemini CLI\". Delightful! I neeeed this google login method in sst's opencode now haha Ugh, I really wish this had been written in Go or Rust. Just something that produces a single binary executable and doesn't require you to install a runtime like Node. Why would someone use this over aider? I played around with it to automate GitHub tasks for me (tagging and sorting PRs and stuff). Sometimes it needs a little push to use th", "negative": "Tesla ending Models S and X production. No more S3XY lineup of models? I'm surprised Musk was okay with breaking that up. Why is it seen initially so negatively? There's nothing inherently wrong with a company deciding to stop producing models that are extremely old, have newer comparable models that are more widely available globally and sell multiples more of. So why would you keep those older models? If anything its a good thing. But its Tesla so nothing they do will be spoken positively of. Feels a lot like giving up. I guess this is why there is such a strong change in the Tesla messaging, to Robotaxis and robots. But maybe this is inevitable. The cars being made in China are pretty amazing and I don\u2019t think it is possible for American or European companies to compete. > converting Fremont factory lines to make Optimus robots I\u2019m very bullish on humanoid robots, but this seems absolutely batshit insane to me. These things are no where near ready for full scale production. It seems fairly easy to find figures on how many cars Tesla has produced each quarter but, surprisingly (at least to me), it's harder to find compiled information on (for each quarter): - Average Selling Price; - Cars produced vs cars sold; - How many unsold cars are in inventory. I did find this [1]; - A model breakdown of the above 2. The reason I'm interested in this because my theory is that: 1. Sales have been shifting from the Model S/X to the Model 3/Y, which reduces average selling price and overall profit. Stopping production is really about the inventory glut; 2. Unsold inventory is going up, particularly for the Cybertruck; and 3. Tesla marketshare is collapsing in many markets due to a combination of brand collapse among the most likely EV buyers and competition from lower-priced alternatives, particularly Chinese EVs in developing markets. So what exactly is propping up this company at an above $1T market cap? [1]:  https://electrek.co/2025/06/17/tesla-tsla-inventory-overflow...  "}
{"anchor": "Waymo granted permit to begin testing in New York City. I saw one of these on Chambers Street just yesterday afternoon, but it must have been in manual mode, of course. It's fascinating seeing all the comments elsewhere anytime Waymo starts testing in another city along the lines of, \"ah, but how will they handle X, Y, and Z here?? Checkmate, robots!\" despite having already launched service in several other cities. Granted, NYC is the biggest city in the US, so maybe that sort of reaction is more reasonable there than when people in Dallas or Boston do it. Very cool. I wonder what scale it has to hit for this to become a profitable line item for Google and what their revenue targets are for it. Man I love Waymo everytime I'm in SF. Truly feel like I'm living in the future when I sit in one I'm cautiously optimistic about this self-driving thing. Waymo at least seems to have figured out a lot of it. Would it be way better to make walkable neighborhoods, mixed-use developments, and reliable and frequent public transit? Yes. Yes it would. But, in lieu of that, self-driving has a lot of advantages in the long run, even if the technology isn't 100% perfect right now. It's insane that they need permits for 8 cars that have humans driving them in 2025, when they're already fully automated in SF. > We\u2019re a tech-friendly administration Clearly not. Is this the first time Waymo is doing winter / snow testing at scale? I think some of the Pittsburgh-based self-driving firms may have tried this, but unaware how far they got. I\u2019m curious if autonomous cars will become targets for aggressive drivers. Like a driver isn\u2019t going to be as scared cutting off a Waymo or tailgating one because the AI isn\u2019t gonna get road rage or honk like hell. In some places I could see the Waymo\u2019s getting severely bullied if that\u2019s the case. The game-theoretic aspect of this is interesting to me. A lawful robot will never make progress in Manhattan because the people will just walk across its path con", "positive": "40M Americans Live Alone, 29% of households. Any causes? Is it that we are too independent and don\u2019t like collectivism? A conspiracy might say it\u2019s on purpose to have more people pay for things typically bought for a couple. Like everyone having their own house, cable bill, utility bill, water bill, \u2026 If more people lived together with friends, that\u2019d make a dent in both the housing and loneliness crises. Is it a bad thing? People's life choices are their own. 29% seems like a fairly neutral number. Incidentally, that newsletter has a lot of interesting charts.  https://www.apolloacademy.com/the-daily-spark/  It's not a high number when compared to other first-world countries:  https://statranker.org/population/top-10-countries-with-high...  Is this page just a single chart and a massive legal disclaimer? When I was a twentysomething, I had roommates. This saved money on rent and bulk purchases (which let me spend more time having fun  and  save money) and provided a starter-kit social circle in a new city. It also honed conflict-resolution skills and ability to be civil. And when I got a partner, it made moving in together smoother. Something I\u2019ve noticed recently is many college graduates living alone. That\u2019s fine. But it\u2019s a weird default for early in one\u2019s career. If I had one general piece of advice for anyone starting their career, it would be to seek out a living situation with roommates. Side question: are more college students staying in solo dorms? Part of the \"housing crisis\" is older Americans aging-in-place and using way more home than they need too. A widow/er might occupy the same suburban single family home in retirement that could house 5 people. Even as new homes are much bigger than decades past\u2026 The best thing I ever did for my mental health was to start having children. Humans, like every other living creature, are hardwired by billions of years of evolution to reproduce. So what? After living with parents, roommates, spouses and others for most", "negative": "San Francisco coyote swims to Alcatraz. I would be surprised if the Coyote would be quick to get back into the water after such a difficult swim. It would, I suspect, want to recover and find food. So I support the theory the Coyote is just hiding somewhere. The island is small but not that small that it couldn\u2019t hide somewhere. I wonder if a turtle drowned halfway across. If a Coyote could do it, all those famous escapees must have had too. That roadrunner thought he'd be safe hiding out on the notorious prison island... It's a 1.5mi swim. I remember visiting Angel Island (a 0.5mi swim) and seeing the abundance of raccoons they have, and asked a ranger how they got there. They also swam. Growing up on a lake I would regularly watch deer swim the quarter mile back and forth between the shore and a nearby island, with no problem. Video:  https://www.youtube.com/watch?v=br4-VsvRcII  Poor thing, talk about going in the wrong direction :) Impressive though. If you time things right, and don't get swept out to sea, it's the 54 degree water that is the real danger. I'm no medical person, but it sure seems like that the animal is suffering from hypothermia and fatigue. I'm sure it'll have happy hunting once it recovers. This sort of thing is a huge problem here in New Zealand. The only native mammal here is a bat, we have mostly birds which evolved for a really long time with only avian predators. So they\u2019re hilariously poorly adapted for surviving standard predators (cats, rats, dogs etc) which first the Maori and subsequently Europeans brought. For example, many of them are flightless and tend to freeze when threatened - works well against eagles but is a terrible idea when threatened by a cat. As a result, we have many animals, mostly birds, which are totally unique and also critically endangered. Many of them can only survive on offshore islands which have been comprehensively cleared of predators at vast effort and expense. The islands need to be relatively accessible"}
{"anchor": "Purely Functional Sliding Window Aggregation Algorithm. This is a very interesting algorithm which is more or less known in the folklore, but is still relatively obscure. I have used it as a part of temporal logic monitoring procedure:  https://github.com/Agnishom/lattice-mtl/blob/master/src/Moni...  This is similar to an approach I use but instead of a queue, I accomplish this using a ring buffer that wraps around and overwrites entries older than window size. We maintain a global window aggregate, subtract ring buffer slot aggregate for entries dropping out and accumulate new entries into new slot aggregate while adding it to the global aggregate. Everything is o(1) including reads, which just returns the global window aggregate. That was a well written and easily approachable blog post on what I found to be an interesting topic. Aside from the topic itself, I think I also learned a bit about structuring technical articles. Competitive Programming in Haskell...I can only define this as Masochistic Aesthetics... I have made a quick c++ implementation for those unfamiliar with Haskell :  https://gist.github.com/unrealwill/5ca4db9beefafaa212465277b...  The queue method is popular, but there's a much faster (branch-free) and in my opinion simpler way, known as the van Herk/Gil-Werman algorithm in image processing. It splits the input into windows and pairs up a backward scan on one window with a forward scan on the next. This works for any associative function. I was very surprised when I learned about it that it's not taught more often (the name's not doing it any favors)! And I wrote a tutorial page on it for my SIMD-oriented language, mostly about vectorizing it which I didn't quite finish writing up, but with what I think is a reasonable presentation in the first part:  https://github.com/mlochbaum/Singeli/blob/master/doc/minfilt...  I also found an interesting streaming version here recently:  https://signalsmith-audio.co.uk/writing/2022/constant-time-p...  EDIT:", "positive": "Show HN: Similarity = cosine(your_GitHub_stars, Karpathy) Client-side. TL;DR - The Idea: People use GitHub Stars as bookmarks. This is an excellent signal for understanding which repositories are semantically similar. - The Data: Processed ~1TB of raw data from GitHub Archive (BigQuery) to build an interest matrix of 4 million developers. - The ML: Trained embeddings for 300k+ repositories using Metric Learning (EmbeddingBag + MultiSimilarityLoss). - The Frontend: Built a client-only demo that runs vector search (KNN) directly in the browser via WASM, with no backend involved. - The Result: The system finds non-obvious library alternatives and allows for semantic comparison of developer profiles. That's actually really neat.  It suggested regclient/regclient as a repository I'd like.  I looked and, yup, I had no idea that existed and it is a sort of thing I like. People complain about The Algorithm but it can be useful... lol  https://puzer.github.io/github_recommender/#p=eyJ0IjoicHJvZm...  Fun fact: cosine similarity's first use in recommendation systems to recommend usenet groups. ( https://dl.acm.org/doi/epdf/10.1145/192844.192905  although they don't call it cosine similarity; they do compute a \"correlation coefficient\" between two people by adding together the products of scores each gave to a post) Very high quality \"Recommended repos for you\" results, the top one was in fact a repo I was looking for a couple of days ago but did not successfully find. I just wish I could scroll further down the \"Similar to you\" list. Excellent. Found me three other stars and one to that I knew from before but hadn't started. Nice! It seems to generate pretty good \"Recommended repos for you\" suggestions, all of them I've heard and seen before, but for one or another reason didn't use for anything or found a need for. Would be great if it could show more options than just 10, because I'm sure further down the list it'd have interesting suggestions I hadn't seen before. This is s", "negative": "There is an AI code review bubble. I don't really understand how this differentiates against the competition. > Independence Any \"agent\" running against code review instead of code generation is \"independent\"? > Autonomy Most other code review tools can also be automated and integrated. > Loops You can also ping other code review tools for more reviews... I feel like this article actually works against you by presenting the problem and inadequately solving them. 1. I absolutely agree there's a bubble. Everybody is shipping a code review agent. 2. What on earth is this defense of their product? I could see so many arguments for why their code reviewer is the best, and this contains none of them. More broadly, though, if you've gotten to the point where you're relying on AI code review to catch bugs, you've lost the plot. The point of a PR is to share knowledge and to catch structural gaps. Bug-finding is a bonus. Catching bugs, automated self-review, structuring your code to be sensible: that's _your_ job. Write the code to be as sensible as possible, either by yourself or with an AI. Get the review because you work on a team, not in a vacuum. None of these tools perform particularly well and all lack context to actually provide a meaningful review beyond what a linter would find, IMO.  The SOTA isn't capable of using a code diff as a jumping off point. Also the system prompts for some of them are kinda funny in a hopelessly naive aspirational way.  We should all aspire to live and breathe the code review system prompt on a daily basis. I liked that the post is self-aware that it's promoting its own product. But the writing seemed more focus on the philosophy behind code reviews and the impact of AI, and less on the mechanics of how greptile differs from competitors. I was hoping to see more on the latter. Problem with Code Review is it is quite straightforward to just prompt it, and the frontier models, whether Opus or GPT5.2Codex do a great job at code-reviews. I d"}
{"anchor": "Claude Code can debug low-level cryptography. This resonates with me a lot: > As ever, I wish we had better tooling for using LLMs which didn\u2019t look like chat or autocomplete I think part of the reason why I was initially more skeptical than I ought to have been is because chat is such a garbage modality. LLMs started to \"click\" for me with Claude Code/Codex. A \"continuously running\" mode that would ping me would be interesting to try. Coming soon, adversarial attacks on LLM training to ensure cryptographic mistakes. CLI terminals are incredibly powerful. They are also free if you use Gemini CLI or Qwen Code. Plus, you can access any OpenAI-compatible API(2k TPS via Cerebras at 2$/M or local models). And you can use them in IDEs like Zed with ACP mode. All the simple stuff (creating a repo, pushing, frontend edits, testing, Docker images, deployment, etc.) is automated. For the difficult parts, you can just use free Grok to one-shot small code files. It works great if you force yourself to keep the amount of code minimal and modular. Also, they are great UIs\u2014you can create smart programs just with CLI + MCP servers + MD files. Truly amazing tech. > For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it? You can use Git hooks to do that. If you have tests and one fails, spawn an instance of claude a prompt -p 'tests/test4.sh failed, look in src/ and try and work out why'       $ claude -p 'hello, just tell me a joke about databases'\n\n    A SQL query walks into a bar, walks up to two tables and asks, \"Can I JOIN you?\"\n\n    $ \n  \nOr if, you use Gogs locally, you can add a Gogs hook to do the same on pre-push > An example hook script to verify what is about to be pushed.  Called by \"git push\" after it has checked the remote status, but before anything has been pushed.  If this script exits with a non-zero status nothing will be pushed. I like this idea. ", "positive": "Bus stops here: Shanghai lets riders design their own routes. This is really brilliant \u2014 like desire paths, but for transit. Obviously execution will be challenging, but the concept is fantastic, and China/Shanghai seems like one of the few places with the requisite density & state capacity to actually make this work. Generally I think that the design of public spaces has SO MUCH room to be improved by just responding to the wisdom of the crowd. I'm glad that Shanghai has moved to the next level in public transportation in meeting customer demand. Most cities don't have the funds to buy smallish buses and labour available as drivers. They don't have the money or willpower to get frequencies to turn up and go levels (ie frequent) and leave people with long walks to widely spaced routes. Tangent: I\u2019ve often thought that it would be great to let people design their own political districts to reduce gerrymandering At the polling place you\u2019d get a map with your census tract and then be asked \u201cwhich two or three adjacent tracts are most similar to your community\u201d. Eventually you\u2019d end up with some sort of gram matrix for tract-to-tract affinity, and then you could apply some algorithmic segmentation. Two problems: - this is far too complex for most voters to understand, much less trust, what\u2019s happening - the fact it\u2019s \u201calgorithmic\u201d would give a sheen of pseudo objectivity, but the selection of the actual algorithm would still allow political infouence over boundaries Chiming in from Los Angeles, USA to say wow, must be nice living in a modern society that prioritizes public transit and peoples' ease of movement. I know, I know, it comes with trade offs of living in an authoritarian state, but the absolute abysmal state of infrastructure in this country is maddening. Ever been on a train in Denmark or Japan or Switzerland? This remind me that road router should be walked by passenger rather than designed by designers. China is the only modern country that has both the cap", "negative": "The struggle of resizing windows on macOS Tahoe. From \" Safari 15 on Mac OS, a user interface mess \"  https://morrick.me/archives/9368  from 5 years ago: --- start quote --- The utter user-interface butchery happening to Safari on the Mac is once again the work of people who put iOS first. People who by now think in iOS terms. People who view the venerable Mac OS user interface as an older person whose traits must be experimented upon, plastic surgery after plastic surgery, until this person looks younger. Unfortunately the effect is more like this person ends up looking\u2026 weird. These people look at the Mac\u2019s UI and (that\u2019s the impression, at least) don\u2019t really understand it. Its foundations come from a past that almost seems inscrutable to them. Usability cues and features are all wrinkles to them. iOS and iPadOS don\u2019t have these strange wrinkles, they muse. We must hide them. We\u2019ll make this spectacular facelift and we\u2019ll hide them, one by one. Mac OS will look as young (and foolish, cough) as iOS! --- end quote --- At the time it was only Safari that they wanted to \"modernize\". Now it's the full OS. tribunals the cherry on top is the delay between the drag start and the window begining to resize Of all the Linux features to copy, they chose this. Apple's window management has always sucked, with the absurdly crippled resizing being a longstanding embarrassment. Into the 2000s, the only way you could resize a window on the Mac was to drag its lower-right corner. That is it. NO other corner, and no edge. So if the lower-right corner happened to be off-screen because the window was bigger than the screen, you were kind of screwed. You had to fiddle with the maximize & restore gumdrops to trick the OS into resizing the window to make that ONE corner accessible. Then you had to move the corner, then roll all the way up to the title bar and move the window, then roll back down to the corner... until you had the window sized and positioned as you wanted. When Apple gru"}
{"anchor": "Hypothesis: Property-Based Testing for Python. I keep thinking I have a possible use case for property -based testing, and then I am up to my armpits in trying to understand the on-the-ground problem and don't feel like I have time to learn a DSL for describing all possible inputs and outputs when I already had an existing function (the subject-under-test) that I don't understand. So rather than try to learn to black boxes at the same time , I fall back to \"several more unit tests to document more edge cases to defensibly guard against\" Is there some simple way to describe this defensive programming iteration pattern in Hypothesis? Normally we just null-check and return early and have to deal with the early-return case. How do I quickly write property tests to check that my code handles the most obvious edge cases? It\u2019s been quite some time since I\u2019ve been in the business of writing lots of unit tests, but back in the day, I found hypothesis to be a big force multiplier and it uncovered many subtle/embarrassing bugs for me. Recommend.  Also easy and intuitive to use. I love property-based testing, especially the way it can uncover edge cases you wouldn't have thought about. Haven't used Hypothesis yet, but I once had FsCheck (property-based testing for F#) find a case where the data structure I was writing failed when there were exactly 24 items in the list and you tried to append a 25th. That was a test case I wouldn't have thought to write on my own, but the particular number (it was always the 25th item that failed) quickly led me to find the bug. Once my property tests were running overnight and not finding any failures after thousands and thousands of random cases, I started to feel a lot more confident that I'd nailed down the bugs. Make sure to read the docs and understand this well. It has its own vocabulary that can be very counterintuitive. It seems to only implement a half of QuickCheck idea, because there is no counterexample shrinking. Good effort thoug", "positive": "Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant. It seems this study has been discussed on HN before, though was recently revised very late December 2025.  https://arxiv.org/abs/2506.08872  Article seems long, need to run it through an LLM. Curious what the long-term effects from the current LLM-based \"AI\" systems embedded in virtually everything and pushed aggressively will be in let's say 10 years, any strong opinions or predictions on this topic? Dont even need to read the article if you been using em. You already know just as well as I do how bad it gets. A door has been opened that cant be closed and will trap those who stay too long. Good luck! \u201cLLM users also struggled to accurately quote their own work\u201d - why are these studies always so laughably bad? The last one I saw was about smartphone users who do a test and then quit their phone for a month and do the test again and surprisingly do better the second time. Can anyone tell me why they might have paid more attention, been more invested, and done better on the test the second time round right after a month of quitting their phone? > Cognitive activity scaled down in relation to external tool use \"Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.\" TL;DR: We had one group not do some things, an later found out that they did not learn anything by not doing the things. This is a non-study. i think i can guess this article without reading it: ive never been on major drugs, even medically speaking yet using AI makes me feels like i am on some potent drug that eating my brain. what's state management? what's this hook? who cares, send it to claude or whatever Imo programming is fairly different between vibes based not looking at it at all and using AI to complete tasks. ", "negative": "Recent discoveries on the acquisition of the highest levels of human performance. This sort of tracks for me. The smartest people I know as adults mostly fucked around a lot and had wide interests that all culminated in them doing a great thing greatly. The smartest people I know as kids spent hours grinding on something and crashed out in college and are mostly average well-to-dos now. Hardly a recent discovery. This is basically the entire foreword of David Epstein's book called Range: Why Generalists Triumph in a Specialized World A summary, since the paper isn't open access:  https://scientificinquirer.com/2025/12/21/the-counterintuiti...  > For example, world top-10 youth chess players and later world top-10 adult chess players are nearly 90% different individuals across time. Top secondary students and later top university students are also nearly 90% different people. Likewise, international-level youth athletes and later international-level adult athletes are nearly 90% different individuals. Motivation if you feel like you're young and failing That could simply be explained by early high achievers being worked hard by their parents or something else while people with innate abilities making progress slower (because most people are not overworked). For the first group they sizzle either because the pressure is removed as they grow up or because they hit their ceiling. Couldn't this be explained by Berkson's Paradox [0]? [0]  https://xcancel.com/AlexGDimakis/status/2002848594953732521  Seems very Taleb's Ugly Surgeon / Berkson's Paradox to me. It's like how software engineers who are at Google are worse if they're better competitive programmers. e.g.  https://viz.roshangeorge.dev/taleb-surgeon/  Exponential growth is the path of longsuffering, and one doesn't always make it. It sucks and looks and feels bad for all involved. This is why advice such as, \"Ignore the naysayers.\" is clutch. And other advice once one starts to rocket shoot like \"Stay in your lane."}
{"anchor": "Alex Honnold completes Taipei 101 skyscraper climb without ropes or safety net. watching in Live on Netflix was riveting One of the most incredible feats of strength and daring I've ever witnessed. The only thing at all comparable was watching Baumgartner freefall back to earth from the edge of space. Unbelievable! We had the privilege to watch at first from the SE corner of the building and later as he climbed by the the observation deck on the 89th floor. Hair raising stuff I'll never forget. Very impressive feat, no doubt about it. But the commentary on the Netflix broadcast ruined the spectator experience completely. It was utterly unbearable. I know nothing about climbing. beyond the straight flex of \"I could die if I make a mistake\", is there a point to doing this without safety equipment? In other tower climbing events, some things cannot be free climbed (too smooth, fingers aren't made for window cleaning tracks, etc). The 1988 ascent of the Sydney Centrepoint was a technical climb with custom jumars for both the cables and the window tracks and a fun challenge for all, both the scouting, the climb, and the filming. Originally titled  The Only Building I Ever Wanted To Climb , later released as  A Spire , there's a documentary film that follows a climb at night of \"only\" 1,000 feet. ... with a  massive  overhang. *  https://en.wikipedia.org/wiki/Sydney_Tower  *  https://www.youtube.com/watch?v=qch1Gd8VLK0  This is a very irresponsible thing to do when you have children. In his El Capitan climb (Free Solo), Alex was worried about cameras or presence of friends watching interfering with the climb. As oppose to that, this climb must have felt very different! I and some friends observed his climb from the base of Taipei 101. Thousands of people were present and it was very good fun how the crowd would react when he made it to another ledge, and when he made it to the top people were shouting and cheering. It was like a great big party. I imagine Threads and Inst", "positive": "Gemini 2.5 Pro vs. Claude 3.7 Sonnet: Coding Comparison. TL;DR If you want to jump straight to the conclusion, I\u2019d say go for Gemini 2.5 Pro, it\u2019s better at coding, has one million in context window as compared to Claude\u2019s 200k, and you can get it for free (a big plus). However, Claude\u2019s 3.7 Sonnet is not that far behind. Though at this point there\u2019s no point using it over Gemini 2.5 Pro. From my use case, the Gemini 2.5 is terrible. I have a complex Cython code in a single file (1500 lines) for a Sequence Labeling. Claude and o3 are very good in improving this code and following the commands. The Gemini always try to do unrelated changes. For example, I asked, separately, for small changes such as remove this unused function, or cache the arrays indexes. Every time it completely refactored the code and was obsessed with removing the gil. The output code is always broken, because removing the gil is not easy. Is there a less biased discussion? The OP link is a thinly veiled and biased advert for something called composio and really a biased and overly flowery view of Gemini 2.5 pro. Example: \u201cEveryone\u2019s talking about this model on Twitter (X) and YouTube. It\u2019s trending everywhere, like seriously. The first model from Google to receive such fanfare. And it is #1 in the LMArena just like that. But what does this mean? It means that this model is killing all the other models in coding, math, Science, Image understanding, and other areas.\u201d For Gemini: play around with the temperature: the default is terrible: we had much better results with (much) lower values. Gemini is the only model which tells me when it's a good time to stop chatting because either it can't find a solution or because it dislikes my solution (when I actively want to neglect security). And the context length is just amazing. When ChatGPT's context is full, it totally forgets what we were chatting about, as if it would start an entirely new chat. Gemini lacks the tooling, there ChatGPT is far ahead, b", "negative": "UK House of Lords Votes to Extend Age Verification to VPNs. So will openvpn now get a new command line argument '--passport-number-for-age-verification 8371652299'? And presumably also a '--webcam-to-use-for-identity' Not made clear in this article - this bill will be passed back to the House of Commons to debate/amend before going back to the House of Lords. This was not the final say. Just for clarification.  House of Lords amendments do not have to be accepted by the House of Commons and may not make it into law. If you do not agree with an amendment then write to your MP, write to the ministers concerned.  If you do not tell them your concerns they will not know. You can ask for an appointment with your MP. You can ask for an appointment with ministers.  Better still you can form an advocacy group and lobby. What if I rent a cheap VPS overseas and wireguard my traffic to that? This is very bad news because I have been in contact with low cost providers (lowendtalk) and the community & even they usually end up renting etc. from datacenters and they usually would have name as well So theoretically, suppose I have a vpn company on A) either such lowend niche providers who might support let's say my mission or we are aligned or B) the hyperscalers or large companies. Now I am 99% sure that large companies would actually restrict VPN creation usage (something remarkably rare right now but still it's a gone deal now) And I feel like even with niche lowendbox providers, suppose I am paying 4 euros or something to a provider to get an IP, they are either using hyperscaler themselves (like OVH) or part of a datacenter itself If a server they own in some capacity runs a vps, can it be considered that they are running a vps and they can get sued by the Safety Act too? If not, then what if this happens one layer above at datacenter and now datacenters might have to comply with them I haven't read the article but wtf. Suppose I run a tmate instance (basically allows you to c"}
{"anchor": "In New York City, congestion pricing leads to marked drop in pollution. > Particulates issued from tailpipes can aggravate asthma and heart disease and increase the risk of lung cancer and heart attack. Globally, they are a leading risk factor for premature death. Minor nitpick, but tailpipes aren't the primary source of emissions. The study is about PM2.5[0]. which will chiefly be tires and brake pads. Modern gasoline engines are relatively clean, outside of CO2, though diesel engines spit out a bunch of bad stuff. [0]  https://www.nature.com/articles/s44407-025-00037-2  See also  https://news.ycombinator.com/item?id=46213504  There was a study published about how much air pollution dropped in NYC during the COVID lockdown. PM2.5 was found to have dropped 36%. However with more robust analysis, this drop was discovered to not be statistically significant. I would caution anyone reading this who is tempted by confirmation bias. Source:  https://pmc.ncbi.nlm.nih.gov/articles/PMC7314691/  To head off the almost inevitable recapitulation of yesterday's parade of misinformed complaints by teenage libertarians, please actually read the paper before commenting. The paper shows there was no significant reduction in entries to the congestion charge zone by cars, vans, and light trucks. And you can confirm this conclusion is consistent with their source data using their github repo. The reduction in pollution is coming from the significant decline in heavy truck traffic. Truckers were using lower manhattan as a cut-through route to other places and they are now doing that less, exactly as congestion pricing planners long argued. Not surprising. The real question is how do we measure the opportunity cost of these measures? Is it a net gain? You could, at the extreme, ban all motor vehicles but the opportunity cost would outweigh the benefits. This article confirms my existing bias/belief that user pays and auction[0] based systems improve governmental programs and finite supp", "positive": "Why a 'Boring' Life Might Be the Happiest One. Hard agree! The challenge as you get older is reducing complexity so simple moments can be enjoyed, but it doesn't come easy. Turned 50 recently and I have come to the same conclusion.  I just want to live a simply life with simple joys. However that would only be possible because I've been working and saving since I was 15 years old. I find myself always struggling between being ambitious and being happy. Ideally I'd like to be both. But when one gets on the \"ambitious\" treadmill, capitalism wants one to work 24/7/365. Your competition is showing off how they worked until 4am, worked through the holidays, launched products on Sunday, and slept in the office, as \"dedication\". That culture makes me unhappy because I lose my physical health and mental health doing that. I'm happy and do my best work when I can go home, cook creative dinners, enjoy company of my partner, and enjoy the sunrises and sunsets in the mountains on the weekends. I'm not sure if a 'Boring Life' is for me. But I am sure I need some seasons of my life to be boring. Since I was 20 I've grinded away at my career, side hustles, etc... It made me happy. But at a certain point I got far enough ahead I felt complete. I needed a boring phase. I now wake up, have breakfast with my family, go to work, come home, play with my kids, watch a show, and go to bed. This is a day I would have scoffed at 10 years ago. But it now makes me happy. I don't think it would make me happy if I hadn't went for something the last 15. And it might not make me happy forever, but it's perfect for me right now. The author mistakes introversion to do-little/nothing. Many introverts love socializing and being around friends, it\u2019s just energy-depleting and takes (more) time to recharge. That said, doing little or nothing is quite relaxing, especially on rainy days. > I feel the world has become too fast. Too restless. Too demanding. We don\u2019t say it, but there\u2019s always this quiet pre", "negative": "enclose.horse. Which came first -- the game or the domain name? The game dynamic feels a bit like Wordle: One puzzle per day and different solutions that you can compare with others. Enjoyed that. Removing a block was a bit fiddly on FireFox (Floorp) due to the right click menu appearing when I tried to click on a tile. Looking forward to tomorrows! I would like to be able to compare/switch optimal with my solution with single click. lots of fun! the fact that the walls spill over the square boundaries is very annoying though, i would love to have an option to just make a wall a filled in square without the 3d effect. I expected the horse to move one tile for each block you placed. I had an elaborate plan to lure it towards one exit and then close it at the last minute... Nope! I did Day 8 - I don't know if Perfect means I got the most optimal score, I do show up at the top of the graph.  https://enclose.horse  Day 8\n PERFECT!  100% Cool game, but I don't like how you get only one chance. Even returning to the page, you can't try again to beat your previous score. No replayability value at all. I am curious on how you would algorithmically find the optimal solution for this kind of problem for much bigger grids.\nI wanted to do some seed finding in Factorio for the same exact problem using the generated map images, but never found a good solution that was fast enough. A good game. Possibly the 2048 of 2026. Nice.  Reminds me of Rodent's Revenge. Great game, I love it!  I hope the author is collecting juicy analytics.  They would be useful if they ever want to bundle 100 levels in order of difficulty and release this as a Steam game (which I would absolutely buy!) I don\u2019t think the gates should animate up into the air.  It breaks the visual logic of 2D for no benefit.  It\u2019s subconsciously confusing to see a gate I place in one cell move to occupy pixels in the cell \u201cabove\u201d it. I look forward to future days introducing new mechanics as well.  Can I suggest a few, based"}
{"anchor": "CS 168: The Modern Algorithmic Toolbox. Wish the videos were public too! > Zero hits for \"hardware\". Are there algorithm courses that take into account how hardware affects algorithms? For example with databases, you have implement theoretically inefficient algorithms which are faster in practice (mostly because they use sequential access). This seems like a great course. What level is it? Is this a first year course at Stanford? Very neat. They write: > In this course, we\u2019ll be looking for the following trifecta: (i) ideas that are  non-obvious,  even to the well-trained computer scientist, so that we\u2019re not wasting your time; (ii)  conceptually simple  \u2014 realistically, these are the only ideas that you might remember a year or more from now, when you\u2019re a start-up founder, senior software engineer, PhD student, etc. (iii)  fundamental,  meaning that there is some chance that the idea will prove useful to you in the future. In the first lesson, they discuss consistent hashing, and they seem to have achieved their goals. ETA: Must include this anecdote on the history of the algorithm: 1. 1997: The implementation of consistent hashing given in this lecture first appeared in a research paper in STOC (\u201cSymposium on the Theory of Computing\u201d) [...] Ironically, the paper had previously been rejected from a theoretical computer science conference because at least one reviewer felt that \u201cit had no hope of being practical.\u201d 2. 1998: Akamai is founded. 3. March 31, 1999: A trailer for \u201cStar Wars: The Phantom Menace\u201d is released online, with Apple the exclusive official distributor. apple.com goes down almost immediately due to the overwhelming number of download requests. For a good part of the day, the only place to watch (an unauthorized copy?) of the trailer is via Akamai\u2019s Web caches. This put Akamai on the map. 4. April 1, 1999: Steve Jobs, having noticed Akamai\u2019s performance the day before, calls Akamai\u2019s President Paul Sagan to talk. Sagan hangs up on Jobs, thinking it", "positive": "Greenland sharks maintain vision for centuries through DNA repair mechanism. Sharks are so cool, man. They\u2019ve just been chilling on the planet for 400 million years, swimming the oceans while epochs passed them by in their periphery. Their entire biology is pretty much unchanged. They\u2019ve been sharks the whole time. This is so messed up harvesting the eye from a creature that lives hundreds of years. I guess they put the shark down. RIP one eye. It\u2019s sinful to fish and kill these ancient creatures up from the deep for minor scientific progress. So wait did they just catch a 200 year old shark and cut its eye ball out to have a look? Highly recommend the book \"Shark Drunk: The Art of Catching a Large Shark from a Tiny Rubber Dinghy in a Big Ocean\" by Morten Str\u00f8ksnes if you're interested in old sharks, small boats or deep oceans  https://bookshop.org/p/books/shark-drunk-the-art-of-catching...  I\u2019m starting to realise we don\u2019t really want a cure to aging. Imagine a world where people like Stalin never die. People like bill gates never have to pretend to be a nice person\u2026 If there\u2019s no chance of death, there will never be any progress in society. People in power would just establish a tighter and tighter grip. All the boomers would be immune to death and disease, but the treatment would be banned for the young because they haven\u2019t done enough to earn it. Unfortunately it also seems like these sharks are plagued by parasites in their eyes:  The shark is often infested by the copepod Ommatokoita elongata, a crustacean that attaches itself to the shark's eyes.[17] The copepod may display bioluminescence, thus attracting prey for the shark in a mutualistic relationship, but this hypothesis has not been verified.[18] These parasites can cause multiple forms of damage to the sharks' eyes, such as ulceration, mineralization, and edema of the cornea, leading to almost complete blindness.[11] This does not seem to reduce the life expectancy or predatory ability of Greenland shar", "negative": "US administration to require app, social media, possibly DNA for travelers. Apart from the alarming privacy implications of these proposed rules, I wonder how FIFA might feel about this, ahead of the World Cup. Maybe they could award Trump a privacy prize if his administration backs down from this. I would love to visit the US one day and i do understand that it has no obligation to just let me in, but this seems a bit excessive for a short visit especially seeing as my country has a deal with the US not to require visas. I wonder if i would have to disclose my hn account(s). My cover would be blown! I guess i'm lucky i've made pro Trump comments... I wonder what position the U.S. Chamber of Commerce (which Wikipedia describes as the largest lobbying group in the US) will take on this. I'd like to think they are rational and recognize that 99.99%+ of visitors are bringing tourism money to the United States. I was sure this was going to say that they were going to force travelers to get Truth Social accounts. Somehow I was surprised beyond my wildest guess what they would be asking people to do. I want people to visit the U.S., but if they require that they submit all of this data, I expect that they all protest by not visiting or even coming here for work. There are just so many terrible ideas that come from this administration that I think that they should try to harness the power all of those bad ideas in a infinite idiocy power plant to power the world for all generations to come. Not that I know the details, but wasn't it easier to join the Mafia? Just the names and addresses of your folks in the old country (as \"collateral\") The analogy is kind of striking, when you think about it. Ah, more laws and regulations that cannot be followed by most people. I couldn\u2019t tell you every single \u201csocial media\u201d account I\u2019ve made over the years as various startups failed after I tried them. I definitely couldn\u2019t get all my family\u2019s information, even if constrained to just imm"}
{"anchor": "Ask HN: Where do seasoned devs look for short-term work?. Now is not a great time to be looking for this kind of work unfortunately. I think your network is the best place to look for this sort of work. Sometimes people will reach out to me with short term projects which is the best way to get gigs like this. Maybe start looking at your colleagues on linkedin, see what they are up to, and think of ways to contribute to what they are working on. The best people to contact in this scenario are leadership and decision makers. A SWE II isn't gonna help you much but a CTO at an early stage startup might be a good person to send a DM if they are friends with you (or even if they aren't!) :) Short term work is more plentiful when money is easy and there\u2019s a lot of entrepreneurial activity going on due to some recent catalyst such as mobile app platforms or the dotcom boom etc. Right now we\u2019re in the AI boom and some people may be making money peddling agentic solutions but money is tight and businesses are hurting. It\u2019s also hard to trust a short term dev who doesn\u2019t really need the money. You have no leverage over them. They sort of just do as they please. Most ad-hoc work I've picked up has been people I've previously worked with/for. Maybe worth reaching out to people you have a prestablished relationship with I did this a few years ago and the winning recipe was a shameless (i.e. deeply shameful) linkedin post where I pretty much just summarized my skillset and explained that I was looking for a senior engineer equivalent of a summer internship, with no chance of extension. Got me 3-4 offers. None of the offering companies had ads out for roles like this, so this was pretty much the only way. I'd believe you're better off working on yourself. Maybe do toy projects for your potential portfolio, learn an additional skill (AI?), and build many weekend projects until something sticks. Publishing articles, etc to demo your skill helps you stay top of mind. Even if only the ", "positive": "Review of Anti-Aging Drugs. From the conclusion paragraph: > Your primary life extension program is diet and exercise. Choose a diet that works for you. Stay slim. Considering heart disease is the #1 killer, doing whatever you can to not die from heart disease is the best place for most people to start. Even in 2025, diet and exercise are still king. Winner, \"Ascorbic\". Do they mean Vitamin C? Would any of the OTC stuff even be effective? Melatonin, NAC, and Berberine. Be careful when reading such blogs: > Note that the dosage in the mouse experiments is quite high \u2014 0.1% of the body weight every day, meaning about 2 ounces a day for me (70 kg). Mouse and human metabolism are very different. A better starting estimate would be 5g/day, not 57g/day. I hope people dont accidentally overdose themselves because of lack of a pharmacology background. A lot of people in the comments are talking about the \"problem\" of death and approaches to take, but really, the only thing you can do is philosophically make your peace. Anything else at this point is yelling into infinity. > Fast for short intervals regularly, and longer fasts as they feel good to you. You can effectively do this every day if you just eat once per day. When I was properly obese, this technique resulted in rapid weight loss. Zero exercise was required to see results, which was good at the time because the not eating part was about all I could handle. Being in a fasted state is as close as you can get to actually  reversing  aging. Your body engages in a process called autophagy when nutrient-sensing pathways are down-regulated. When you are stuffing your face constantly (i.e., every ~8 hours), there is less opportunity for this mechanism to do its job.  https://en.wikipedia.org/wiki/Autophagy  Richard Miller's Intervention Testing Program should really be your go-to for this:  https://www.nia.nih.gov/research/dab/interventions-testing-p...  He has no conflicts of interests, works for the NIA, and he's quite o", "negative": "Show HN: Rails UI. Is this another Tailwind wrapper? Yes, it is. ugh this looks dated even by 2016 standards when will developers learn UI actually matters bootstrap was a mistake, and lowered the bar for everyone i don't get these types of products anymore. i think they're useful in their own way, but i can literally create styles with claude/gemini in a heartbeat and not have to pay some insane fee. I think you missed a trick not naming it Railwind UI. I used this about a year ago when I went through a short Rails phase. I was a bit surprised not to see more Rails-specific UI libraries considering how batteries-included the rest of the framework is, and at the time I didn't really 'get' tailwind. I'm not in a Rails phase anymore, but nice work on the library! maybe I'm just dumb but a lot of these elements don't seem to work? the \"...\" buttons don't open any flyout, the dropdown doesn't open up... otherwise looks cool though I wish I could use this \u2013 unfortunately UI frameworks are a political problem at every company I've worked at. The designers feel undermined or threatened by it, and product owners want to dictate design. Despite the massive productivity benefits of a UI framework, I've never been able to convince stakeholders to actually adopt one. If you\u2019re showing off a UI framework, I shouldn\u2019t be accidentally scrolling left and right on the page on mobile / my iPhone. Couldn\u2019t be bothered to scroll down the page to look at components while accidentally activating horizontal scrolling. Broken in Safari on iphone. For example: - table background moves left when table is scrolled horizontally - actions in table and dropdown do nothing on tap - text on buttons is selectable (really?) im always surprised that Rails is still relevant i havent used it since 2006 opting for php and django i might give it another shot, any reason you like this more than django or other frameworks I have hardware acceleration disabled in Firefox and my 5800X spins up trying to rend"}
{"anchor": "Space Elevator. I always enjoy Neal's pages. I found planes at high altitude very interesting, didn't know we could fly that high! It was enjoyable and informative. Learned that sprites can be 50km long!! This was incredible! Couldn't stop scrolling and reading. For a kid of a certain age and curiosity it'll blow their mind! I'm so grateful the creator made this, shame that his \"buy me a coffee\" isn't a simple PayPal or Apple Pay but you have to put in credit card or bank details!! Excellent! My wrist started to hurt 0.01% of the way to the moon. The atmosphere of this reminded me of the game Outer Wilds TIL it's estimated that over 48 tons of meteors hit the atmosphere every day. Regarding actual space elevators though, while they're not sci-fi to the extent of something like FTL travel - ie. they're technically not physically impossible - they're still pretty firmly in the realm of sci-fi. We don't have anything close to a cable that could sustain its own weight, let alone that of whatever is being elevated. Plus, how do you stabilize the cable and lifter in the atmosphere? A space elevator on the moon is much more feasible: less gravity, slow rotation, no atmosphere, less dangerous debris. But it's also much less useful. Was hoping would go to geostationary orbit as an actual space elevator would :) I love this guy. Re playing this gem  https://neal.fun/stimulation-clicker/  I just clicked the temperature thingy in annoyance because I don't use Fahrenheit and to my delight, it just switched to Celcius the website feels heavy, can we optimize this further?? (not a web dev) A beautifully executed project here, I bought Neal a coffee. What evolutionary advantage, I wonder, is there to Ruppell's griffon vulture flying at 11400 meters? edit: units The rockets at higher altitudes were all in wrong orientation. In reality, they don't fly straight up. Giant Space Bola is much more attractive. It is a 10000 km string with capsules at both ends. It rotates in sync with ear", "positive": "Photos capture the breathtaking scale of China's wind and solar buildout. Also worth checking out some of the mega projects on Open Infrastructure Maps like this one in central China.  https://openinframap.org/#9.12/36.0832/100.4215/A,B,L,P,S  Meanwhile, in London, UK, local council doesn't allow you to put anything on your rooftop that doesn't gel with the Victorian look.. Technological, manufacturing and energy advancements aside (congrats China on those), the pictures look beautiful. Amazing work from the photographer. Why aren't we doing it in the rest of the world as well? Wow, pictures look great, well done Mr Weimin Chu Wouldn't it be better to just go with nuclear? Isn't this a gigantic waste of space and overhead to maintain it? And how \"renewable\" are the materials used to produce these? China has also just launched a megawatt scale wind generator a the helium-lifted balloon, the S2000 , they have active thorium rector the TMSR-LF1 and GW/h Vandium flow battery. The scale , speed and breadth of what they are doing is incredible and I think missed my people It genuinely makes me so sad to see the US not doing the same. Having grown up to the constant beat of \u201cenergy independence\u201d as the core goal of a party it seemed obvious that the nearly limitless energy that rains down from the sky would be the answer. But instead we\u2019ve kept choosing the option which requires devastating our, and other\u2019s around the world, community. That\u2019s not to exclude the harsh reality of mining for the minerals required to build these, nor the land use concerns. But it\u2019s difficult to compare localized damage to war and globalized damage. I find the idea of blanketing mountainous wilderness in relatively short-lived e-waste just awful. Surely there are much better terrains for solar panels? I know nothing about the topic.\nAlthough it seems a better alternative than coal or petrol, is it free of side effects for the nature?\nI wonder if the heat that would be spread around the atmosphe", "negative": "Xfwl4 \u2013 The Roadmap for a Xfce Wayland Compositor. I've been using Xfce as a daily driver in one machine for about a decade now. Great to know there's work on the wayland support front. Also, writing it in Rust should help bring more contributors to the project. If you use Xfce I urge you to donate to their Open Collective:  https://opencollective.com/xfce   https://opencollective.com/xfce-eu  I hope that XFCE remains a solid lightweight desktop option. I've become a huge fan of KDE over the past couple of years, but it certainly isn't what you would consider lightweight or minimal. Personally, I'm a big proponent of Wayland and not big Rust detractor, so I don't see any problem with this. I do, however, wonder how many long-time XFCE fans and the folks who donated the money funding this will feel about it. To me the reasoning is solid: Wayland appears to be the future, and Rust is a good way to help avoid many compositor crashes, which are a more severe issue in Wayland (though it doesn't necessarily need to be fatal, FWIW.) Still I perceive a lot of XFCE's userbase to be more \"traditional\" and conservative about technologies, and likely to be skeptical of both Wayland  and  Rust, seeing them as complex, bloated, and unnecessary. Of course, if they made the right choice, it should be apparent in relatively short order, so I wish them luck. Very interesting that they opted for a rewrite in Rust instead of adjusting the existing codebase. I wonder how long it'll take them writing a compositor from scratch. FYI, you can currently use most wlroots-based compositors with XFCE. I myself am running Hyprland + XFCE on Gentoo.  https://github.com/bergutman/dots  If wayland support was there already I would be using xfce. I truly admire it, it's great to see this happening and I hope the project continues in great speed. With DE's requiring hard system-d support, I would rather have something like xfce i'm trying to build a Linux desktop and the first thing I got stuck at is"}
{"anchor": "Antirender: remove the glossy shine on architectural renderings. Wow, someone finally made Poland-filter. It all looks exactly like I'm used to. Looks beautiful tbh. I prefer the greyness That's funny, the second example is the Peace Bridge in Calgary. On a nice day the render actually looks close to the real thing! I ran it on the \"society if...\" meme lol  https://imgur.com/a/nFQN5tx  This is ingenious and actually useful. I'm looking for a new apartment and I always wanted to know how do these places look in a bad weather, because that's when I need beautiful surroundings the most. They still look great on a rainy November day. A nice cozy, quiet vibe. This filter seems to also change some architectural details and features, as well as degrade the quality of some materials in an unrealistic way. I am very curious if this app is making money or are users just using the two generators and then leaving? If so I am very impressed with your wrapper around the image gen models. Nano Banana is indeed a powerful model :) Used it on some Fortnite screenshots, I'd play that depressing version!  https://files.catbox.moe/i8tfkl.jpg   https://files.catbox.moe/mw8vbc.jpg  Then I thought what would it make from an already dark and grim scene, like HL2 Ravenholm  https://files.catbox.moe/d7z77h.jpg  but nothing really? Just made the whole thing a different color scheme + changed some architecture And the real killer app of contact lens AR will be ... this in reverse. Ha this is great - I always thought this would be a brilliant application for AI. Wow. Umm, the \"free generations\" limit is running on a client-based honour system... It would be great if I can run this as a browser extension that works on Zillow and Redfin. I did exactly the opposite with  https://prontopic.com  Maybe a real picture of the actual bridge was in the training set?  Similar to how prompting for a story about a boy wizard can result in verbatim Harry Potter passages. Looks like Machinarium. I like it. Un", "positive": "\u201cThe closer to the train station, the worse the kebab\u201d \u2013 a \u201cstudy\u201d. Anecdotally, it's the same for coffee. Office lobby coffee shops are invariably terrible. The decent ones are always at least a 5-10 minute walk away. This makes intuitive sense. High mass-transit corridor real-estate (rail, air, road) leases come at a premium so those higher fixed-costs and must be balanced against a higher-volume of less-breadth of service with the same fixed (or even slightly higher) labor costs. In food service, high-volume is (mostly) inversely correlated with quality. Looking at their actual results ( https://preview.redd.it/znmnejgab5je1.png?width=1000&format=... ), I don't see any positive or negative correlation. Although I can subjectively confirm the hypothesis. I've observed the following: 1) An alarming number of regions in the world have a pizza joint called \"New York Pizza\", \"Manhattan Pizza\", or similar. 2) The similarity of the pizza therein to the actual thin, greasy slices served up in pizza joints from actual New York is inversely proportional to the location's distance from New York. So, the New York Pizza in Boston -- pretty close. The New York Pizza in Brisbane, QLD is alien by comparison and I think they consider \"pepperoni\" and \"salami\" interchangeable down there. He didn't find a correlation, or rather found that there is no correlation, between proximity to a railway station and how the kebab is reviewed. It's a nice study for a statistics class! The only place this isn't true is Japan. Always like reading the Best Kebab reviews on trip advisor. It\u2019s right next to Queen Street railway station so fits with the study.  https://www.tripadvisor.co.uk/Restaurant_Review-g186534-d125...  > Not only was my food uncooked but I also discovered a pubic hair in my chips and cheese, then when I proceeded to report the problem, I was chased with a knife. Down Dundas Street.Absolutely scandalous LOL we may need to update the title of this post, half the top level comment", "negative": "Spanish track was fractured before high-speed train disaster, report finds. Wow, that's a really big gap. No wonder it derailed What are the some of the ways that tracks are monitored for fractures like this?  It must have been pretty substantial in order to be described as \"complete lack of continuity\".  Makes me think of literally electronic continuity tests -- are those ever used in this context?  Or how about cameras mounted on trains using image processing?  Or drones? It seems a shame that a few other trains passed beforehand with this anomaly in place and yet it went undetected. My gut feeling says a lot of fatalities could have been prevented with a physical barrier between both tracks. Shouldn't this be mandatory with high speed trains? While these events are statistically very rare, it is worth remembering that there have been two separate events in the past twenty years in Spain where high-speed trains have derailed leading to multiple fatalities [1][2]. In contrast, the Japanese Shinkansen has a spotless record since its introduction in the 1960s [3]. Not a single fatality due to a crash or derailment. And that's in a country with a much larger population and much higher passenger count per year. What do they do differently? [1]  https://en.wikipedia.org/wiki/Santiago_de_Compostela_derailm...  [2]  https://en.wikipedia.org/wiki/2026_Adamuz_train_derailments  [3]  https://en.wikipedia.org/wiki/Shinkansen#Safety_record  I wonder how common it is for train tracks to fracture? And what systems are in place to actually detect this. There was recently a post on a German subreddit where the OP found a fracture in the German rail[0], albeit much smaller. 0.  https://old.reddit.com/r/drehscheibe/comments/1qe9ko2/ich_gl...  AFAIK continuously welded tracks (like those used in high speed rail) are also slightly tensioned, so a break in a single point could make it look like a whole section of track is missing, as tension is released. Some more info from Spanish med"}
{"anchor": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT. Last time they tried to do this they got huge push back from the AI boyfriend people lol I can't see o3 in my model selector as well? RIP That\u2019s really going to upset the crazies. Despite 4o being one of the worst models on the market, they loved it. Probably because it was the most insane and delusional. You could get it to talk about really fucked up shit. It would happily tell you that you are the messiah. > [...] the vast majority of usage has shifted to GPT\u20115.2, with only 0.1% of users still choosing GPT\u20114o each day. I still don\u2019t know how openAI thought it was a good idea to have a model named \"4o\" AND a model named \"o4\", unless the goal was intentional confusion There will be a lot of mentally unwell people unhappy with this, but this is a huge net positive decision, thank goodness. Which one is the AI  boyfriend model? Tumblr, Twitter, and reddit will go crazy >We brought GPT\u20114o back after hearing clear feedback from a subset of Plus and Pro users, who told us they needed more time to transition key use cases, like creative ideation, and that they preferred GPT\u20114o\u2019s conversational style and warmth. This does verify the idea that OpenAI does not make models sycophantic due to attempted subversion by buttering up users so that that they use the product more, its because people actually  want  AI to talk to them like that. To me, that's insane, but they have to play the market I guess If people want an AI as a boyfriend at least they should use one that is open source. If you disagree on something you can also train a lora. I wish they would keep 4.1 around for a bit longer.  One of the downsides of the current reasoning based training regimens is a significant decrease in creativity.  And chat trained AIs were already quite \"meh\" at creative writing to begin with.  4.1 was the last of its breed. So we'll have to wait until \"creativity\" is solved. Side note: I've been wondering lately about ", "positive": "Uv: Running a script with dependencies. This is my absolute favourite uv features and the reason I switched to uv. I have a bunch of scripts in my git-hooks which have dependencies which I don't want in my main venv. #!/usr/bin/env -S uv run --script --python 3.13 This single feature meant that I could use the dependencies without making its own venv, but just include \"brew install uv\" as instructions to the devs. The \"declaring script dependencies\" thing is incredibly useful:  https://docs.astral.sh/uv/guides/scripts/#declaring-script-d...      #  script\n  # dependencies = [\n  #   \"requests<3\",\n  #   \"rich\",\n  # ]\n  # \n  import requests, rich\n  # ... script goes here\n  \nSave that as script.py and you can use \"uv run script.py\" to run it with the specified dependencies, magically installed into a temporary virtual environment without you having to think about them at all. It's an implementation of Python PEP 723:  https://peps.python.org/pep-0723/  Claude 4 actually knows about this trick, which means you can ask it to write you a Python script \"with inline script dependencies\" and it will do the right thing, e.g.  https://claude.ai/share/1217b467-d273-40d0-9699-f6a38113f045  - the prompt there was:     Write a Python script with inline script\n  dependencies that uses httpx and click to\n  download a large file and show a progress bar\n  \nPrior to Claude 4 I had a custom Claude project that included special instructions on how to do this, but that's not necessary any more:  https://simonwillison.net/2024/Dec/19/one-shot-python-tools/  Why doesn't pip support PEP 723?  I'm all for spreading the love of our lord and savior uv, but it should be necessary to have an official implementation. Oh this looks amazing!  I had pretty much stopped using Python for my one-off scripts because of the hassle of dependencies.  I can't wait to try this out. Oh nice, I was already a happy user of the uv-specific shebang with in-script dependencies, but the `uv lock --script example.py` ", "negative": "We X-Rayed a Suspicious FTDI USB Cable. I have a slow burn project where I simulate a supply chain attack on my own motherboard. You can source (now relatively old) Intel PCH chips off Aliexpress that are \u201cunfused\u201d and lack certain security features like Boot Guard (simplified explanation). I bought one of these chips and I intend to desolder the factory one on my motherboard and replace it with the Aliexpress one. This requires somewhat difficult BGA reflow but I have all the tools to do this. I want to make a persistent implant/malware that survives OS reinstalls. You can also disable Intel (CS)ME and potentially use Coreboot as well, but I don\u2019t want to deal with porting Coreboot to a new platform. I\u2019m more interested in demonstrating how important hardware root of trust is. Yeah - these [0] kinds of cables are so extremely scary. \"The O.MG Cable is a hand made USB cable with an advanced implant hidden inside. It is designed to allow your Red Team to emulate attack scenarios of sophisticated adversaries\" \"Easy WiFi Control\" (!!!!!) \"SOC2 certification\"? Dawg, the call is coming from inside the house... [0]  https://shop.hak5.org/products/omg-cable  it's a serious problem they could be regulated to expose their chip with transparent covering rather than plain dark wiring Jeese. I was not sure which image was the suspect one. Just to be clear suspicious in this sense is a cable that is likely counterfeit and wasn't able to do high speed transfer unlike the genuine known good one. this is an advertisement for the company Related  USB-C head-to-head comparison  (389 points, 2023, 219 comments)  https://news.ycombinator.com/item?id=37929338  To be fair, this story is basically an ad, but a pretty good one, and many featured HN stories are really marketing. Personally, I don\u2019t mind marketing stuff, if it\u2019s interesting and relevant (like this). But the fact that most comms cables, these days, have integrated chips, makes for a dangerous trust landscape. That\u2019s something"}
{"anchor": "Roam 50GB is now Roam 100GB. Nice that instead of completely cutting you off at the cap they put it in super slow 500 kbits. That is actually usable and used to be the fastest speed you could get at home. That's not bad for the cheap plan. Even the slow mode is fast enough for video conferencing and doing basic remote work. They still have a separate unlimited plan for anyone who needs more. I\u2019ve kept it on the backup service for 10 GB at $10 or whatever and it\u2019s pretty cool. Used it off my balcony in SF when Google Fiber had a 1 hr outage, take it on road trips, and stuff like that. Totally worth it. I'm actually a huge fan of \"unlimited slow speeds\" as a falloff, instead of a cliff. Aside from the fact it allows you to work with Starlink to buy more fast speed, it also allows core stuff to continue to function (e.g. basic notifications, non-streaming web traffic, etc). They could make it 1000GB for US$10/month and I still wouldn't give any money to a company associated with that man. Finally I can use Codex/OpenCode even out in the woods. No work-life balance; just vibing everywhere I go. I had a \u201chit\u201d post on bsky [0] (90 likes, big numbers for me) asking whether people would want an unlimited mobile plan throttled at 256kbps for $2/month. Seems like yes? There\u2019s lots to say about how useable it is (I often get throttled when traveling and it\u2019s really not that bad + it helps curb any desire to scroll videos!) But mainly I want to ask - I looked into it for a minute and it seems like you couldn\u2019t start an mvno because carriers wouldn\u2019t let you cannibalize them? You can get very cheap IoT plans but if you tried reselling IoT as esims for consumers, the carriers would kill it? So yeah - Starlink to mobile is actually the only viable way that routes around this problem? (((email in profile if you\u2019re cuckoo enough like me and want to start a self service\u2019d throttled mvno))) [0]  https://bsky.app/profile/greg.technology/post/3mbmwsytnyc23  I want the old plan back. If ", "positive": "The Prophet of Parking: A eulogy for the great Donald Shoup. Oregon eliminated burdensome parking regulations in most larger cities and: it's fine. Many home builders still add parking to new projects because there is market demand for it - and they are also competing for tenants or buyers against existing housing which has parking. But there is now the flexibility to do some projects without parking, which really helps at the affordable end of the spectrum, and is a good fit for more walkable locations. BTW, Nolan Gray, cited as the author, has a book out himself that's really approachable and good reading if you're interested in cities:  https://islandpress.org/books/arbitrary-lines  Shoup passed away on February 6: *  https://parkingreform.org/donald-shoup/  *  https://cal.streetsblog.org/2025/02/08/streetsblog-mourns-th...  *  https://news.ycombinator.com/item?id=43004881  His book: *  https://en.wikipedia.org/wiki/The_High_Cost_of_Free_Parking  * EconTalk podcast episode:  https://www.youtube.com/watch?v=8Sgmw3jQcyc  > Nor are minimum parking requirements even needed: developers have the knowledge and incentives to provide the appropriate amount of off-street parking. If a developer builds too little parking, she will struggle to attract tenants and command lower rents. This isn't entirely true. In cities where parking requirements are eliminated, many new businesses move into locations that would have previously been illegal, showing that many commercial tenants view parking requirements as excessive. In my city, judging by public comment, support for parking requirements comes not from business owners or developers but from voters who fear a lack of parking at the businesses they frequent and who fear that parking for nearby businesses or apartment buildings will overflow into their neighborhood (the horror.) > One survey of the literature suggests that drivers in the typical American city spend an average of eight minutes looking for parking at the end of ea", "negative": "HTTP Cats. I love how there is a Catalan version too! I guess it\u2019s probably a requirement for getting the .cat domain. HTTP 000: HTTP not found. HTTPS CA TLS only. That said, at least they have a broad cipher set support and their HTTPS-only implemetation does work in older browsers and systems. That's nice. But HTTP+HTTPS would be better. Previous discussions:  https://news.ycombinator.com/item?id=37735614  (2023)  https://news.ycombinator.com/item?id=31438989  (2022)  https://news.ycombinator.com/item?id=20283794  (2019)  https://news.ycombinator.com/item?id=10161323  (2015) There's an alternative[0] for the canine lovers. [0]:  https://httpstatusdogs.com  Do any browsers recognize a 420 response code? Not to be confused with Cat as a Service -  https://cataas.com/  this is exactly what I was looking for! Is the picture for 303 meant to be the device from Heisenberg\u2019s thought experiment? Nginx makin' up status codes... This is fun because it\u2019s pre-AI and most of the pics are real. Doing this nowadays would be a meh. 404 should have been the cat footprints in the concrete but without the cat. I unironically use this website everytime I forget a status code at work. The name is instantly memorable, it loads immediately, and I can ctrl-f it. It's basically muscle memory at this point. I used similar idea in an app a while back:  https://github.com/tantalor/emend/blob/master/app/static/ima...  Still gives me a chuckle Why is the quality of the pictures so low? Came for 418. Left happy for Caturday. (Every web site I've built in the last ten years has a series of conditions that combined will trigger a 418.) I\u2019ve used this site every time I\u2019m doing http networking stuff for the past few years. It\u2019s so easy to just go to http.cat/303 to check a status code you don\u2019t know, or to scroll down the homepage to find the number you need for a specific response. The cats make it much more fun than a regular docs page, whilst still being a useful quick reference. I wonder if oth"}
{"anchor": "\u201cThe closer to the train station, the worse the kebab\u201d \u2013 a \u201cstudy\u201d. Anecdotally, it's the same for coffee. Office lobby coffee shops are invariably terrible. The decent ones are always at least a 5-10 minute walk away. This makes intuitive sense. High mass-transit corridor real-estate (rail, air, road) leases come at a premium so those higher fixed-costs and must be balanced against a higher-volume of less-breadth of service with the same fixed (or even slightly higher) labor costs. In food service, high-volume is (mostly) inversely correlated with quality. Looking at their actual results ( https://preview.redd.it/znmnejgab5je1.png?width=1000&format=... ), I don't see any positive or negative correlation. Although I can subjectively confirm the hypothesis. I've observed the following: 1) An alarming number of regions in the world have a pizza joint called \"New York Pizza\", \"Manhattan Pizza\", or similar. 2) The similarity of the pizza therein to the actual thin, greasy slices served up in pizza joints from actual New York is inversely proportional to the location's distance from New York. So, the New York Pizza in Boston -- pretty close. The New York Pizza in Brisbane, QLD is alien by comparison and I think they consider \"pepperoni\" and \"salami\" interchangeable down there. He didn't find a correlation, or rather found that there is no correlation, between proximity to a railway station and how the kebab is reviewed. It's a nice study for a statistics class! The only place this isn't true is Japan. Always like reading the Best Kebab reviews on trip advisor. It\u2019s right next to Queen Street railway station so fits with the study.  https://www.tripadvisor.co.uk/Restaurant_Review-g186534-d125...  > Not only was my food uncooked but I also discovered a pubic hair in my chips and cheese, then when I proceeded to report the problem, I was chased with a knife. Down Dundas Street.Absolutely scandalous LOL we may need to update the title of this post, half the top level comment", "positive": "X For You Feed Algorithm. anything interesting? anything that is a surprise? what is the difference between this and  https://github.com/twitter/the-algorithm  I did not expect to see Rust. They seem to have forgotten to commit Cargo.toml though. Oh I see it is not meant to be built really. Some code is omitted. ooh, LLM Recsys alert! (we had an LLM Recsys track at ai.engineer last year). official announcement here:  https://x.com/XEng/status/2013471689087086804  looks like this is the \"for you\" feed, once again shared without weights so we only have so much visibility into the actual influence of each trait. \"We have eliminated every single hand-engineered feature and most heuristics from the system. The Grok-based transformer does all the heavy lifting by understanding your engagement history (what you liked, replied to, shared, etc.) and using that to determine what content is relevant to you.\" aka it's a black box now. the README is actually pretty nice, would recommend reading this. it doesnt look too different form Elon's original code review tweet/picture  https://x.com/elonmusk/status/1593899029531803649?lang=en  sharing additonal notes while diving through the source:  https://deepwiki.com/xai-org/x-algorithm  and a codemap of the signal generation pipeline:  https://deepwiki.com/search/make-a-map-of-all-the-signals_3d...  - Phoenix (out of network) ranker seems to have all the interesting predictive ML work. it estimates P(favorite), P(reply), P(repost), P(quote), P(click), P(video_view), P(share), P(follow_author), P(not_interested), P(block_author), P(mute_author), P(report) independently and then the `WeightedScorer` combines them using configurable weights. there's an extra DiversityScore and OONScore to add some adjustments but again dont know the weights  https://deepwiki.com/xai-org/x-algorithm/4.1-phoenix-candida... \n- other scores of interest: photo_expand_score, and dwell_score and dwell_time. share via copy, share, and share  via dm are all obvi", "negative": "The browser is the sandbox. I like the perspective used to approach this. Additionally, the fact that major browsers can accept a folder as input is new to me and opens up some exciting possibilities. The folder input thing caught me off guard too when I first saw it. I've been building web apps for years and somehow missed that `webkitdirectory` attribute. What I find most compelling about this framing is the maturity argument. Browser sandboxing has been battle-tested by billions of users clicking on sketchy links for decades. Compare that to spinning up a fresh container approach every time you want to run untrusted code. The tradeoff is obvious though: you're limited to what browsers can do. No system calls, no arbitrary binaries, no direct hardware access. For a lot of AI coding tasks that's actually fine. For others it's a dealbreaker. I'd love to see someone benchmark the actual security surface area. \"Browsers are secure\" is true in practice, but the attack surface is enormous compared to a minimal container. We never say that it isn't. There is a reason Google developed NaCl in the first place that inspired WebAssembly to become the ultimate sandbox standard. Not only that, DOM, JS and CSS also serves as a sandbox of rendering standard, and the capability based design is also seen throughout many browsers even starting with the Netscape Navigator. Locking down features to have a unified experience is what a browser should do, after all, no matter the performance. Of course there are various vendors who tried to break this by introducing platform specific stuff, but that's also why IE, and later Edge (non-chrome) died a horrible death There are external sandbox escapes such as Adobe Flash, ActiveX, Java Applet and Silverlight though, but those external escapes are often another sandbox of its own, despite all of them being a horrible one... But with the stabilization of asm.js and later WebAssembly, all of them is gone with the wind. Sidenote: Flash's script"}
{"anchor": "Level S4 solar radiation event. Possible aurora visible through central US tonight This page looks like an accessibility nightmare. The entire warning text is an image. There is no transcription present for screen reader users. I did not expect this from a government website. Nice, you can already see some solar flares in Austria again.  https://www.foto-webcam.eu/webcam/kleinfleisskees/   https://www.foto-webcam.eu/  It seems that the peak was several hours ago, and I haven't observed any effects from it... We had intense aurora in Berlin, Germany. Green clouds dancing in the sky levels. Started around 22:10 local time or a bit earlier, and at this point there's only a faint red/green glow remaining. Do you need long exposure to make it visible with a camera? How does that work in the presence of light pollution? If anyone is interested in what \"G4\" means in context, here's the scale:  https://www.swpc.noaa.gov/noaa-scales-explanation  PJM had some geomagnetic disturbance warnings, but did not progress to the alert stage or grid re-configuation actions. So, no US power grid problems.       104955 Warning Geomagnetic Disturbance Warning 01.19.2026 14:30 \n    PJM-RTO\n    A Geomagnetic Disturbance Warning has been issued for\n    14:30 on 01.19.2026 through 16:00 on 01.19.2026 .\n    A GMD warning of K8 or greater is in effect for this period. \n    End time: 01.19.2026 16:00 \n  \n(All times are prevailing Eastern US time) I've posted on this before, for other warnings. Not going to repeat that. Years ago I was concerned about this and made a plan with my wife for what to do if she was at work. But now we have a bunch of kids in different schools and haven't updated our plan. Does anyone have a plan for what happens if we have a really bad event? Weirdly, while the site in question is \"blaring klaxons!\" there are more \"cool night lights!\" posts than concern. Australian Bureau of Meteorology advisory for visible aurora:  https://www.sws.bom.gov.au/Aurora  I'll be going out", "positive": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT. Last time they tried to do this they got huge push back from the AI boyfriend people lol I can't see o3 in my model selector as well? RIP That\u2019s really going to upset the crazies. Despite 4o being one of the worst models on the market, they loved it. Probably because it was the most insane and delusional. You could get it to talk about really fucked up shit. It would happily tell you that you are the messiah. > [...] the vast majority of usage has shifted to GPT\u20115.2, with only 0.1% of users still choosing GPT\u20114o each day. I still don\u2019t know how openAI thought it was a good idea to have a model named \"4o\" AND a model named \"o4\", unless the goal was intentional confusion There will be a lot of mentally unwell people unhappy with this, but this is a huge net positive decision, thank goodness. Which one is the AI  boyfriend model? Tumblr, Twitter, and reddit will go crazy >We brought GPT\u20114o back after hearing clear feedback from a subset of Plus and Pro users, who told us they needed more time to transition key use cases, like creative ideation, and that they preferred GPT\u20114o\u2019s conversational style and warmth. This does verify the idea that OpenAI does not make models sycophantic due to attempted subversion by buttering up users so that that they use the product more, its because people actually  want  AI to talk to them like that. To me, that's insane, but they have to play the market I guess If people want an AI as a boyfriend at least they should use one that is open source. If you disagree on something you can also train a lora. I wish they would keep 4.1 around for a bit longer.  One of the downsides of the current reasoning based training regimens is a significant decrease in creativity.  And chat trained AIs were already quite \"meh\" at creative writing to begin with.  4.1 was the last of its breed. So we'll have to wait until \"creativity\" is solved. Side note: I've been wondering lately about ", "negative": "Tesla unsupervised Robotaxis are nowhere to be found. I don't think actual unsupervised robotaxis exist, given the reports that they're just having the supervisor follow in a chase car[1]. [1]  https://futurism.com/advanced-transport/car-following-tesla-...  This should not be surprising to anyone who pays any attention to Elon Musk's  \u0336l\u0336i\u0336e\u0336s\u0336 , er... \"predictions\" And yet TSLA sits comfortably at ~$450. If someone knowledgeable can explain this to me, I'd be very grateful. Maintaining a meme stock is hard, really hard.  You do have to hand it to the bloke that he is working hard on this. Back in the day, the term \"snake oil salesman\" was used and it is as fresh today as it always was. There\u2019s no consequences to Musk not delivering and simply making up bullshit. I just saw a LinkedIn post from someone totally unrelated to Musk, or Tesla fawning about how amazing the Tesla Optimus robots are, how they are going to operate in space and how he would prefer one to give him surgery over a doctor. 100s of positive interactions followed Humans seem to need some fiction to believe to get them through their day. So as long as people don\u2019t demand that reality is the driver of their future they will continue to live in whatever fantasy world that makes them the main character There are only around 50[0] unique vehicles operating in Austin (not all operating at the same time) and initially only about 3 are operating \"with no safety monitor in the car.\" Based on social media posts it seems they all have chaser vehicles. [0]  https://robotaxitracker.com  JerryRigEverything randomly started dissing Tesla's FSD system two days before he posts a sponsored video for Ford's self-driving feature. It's probably a good thing they are doing this ultra-conservative rollout of robotaxi. No amount of failed promises, missed deadlines or just plain lies is going to dampen the stock, it\u2019s just the way it is with this. Staying away is the best one can do. HN is so fucked at this point. For th"}
{"anchor": "Ask HN: How are Markov chains so different from tiny LLMs?. LLMs include mechanisms (notably, attention) that allow longer-distance correlations than you could get with a similarly-sized Markov chain.  If you squint hard enough though, they are Markov chains with this \"one weird trick\" that makes them much more effective for their size. A Markov Chain trained by only a single article of text will very likely just regurgitate entire sentences straight from the source material.  There just isn't enough variation in sentences. But then, Markov Chains fall apart when the source material is very large.  Try training a chain based on Wikipedia.  You'll find that the resulting output becomes incoherent garbage.  Increasing the context length may increase coherence, but at the cost of turning into just simple regurgitation. In addition to the \"attention\" mechanism that another commenter mentioned, it's important to note that Markov Chains are  discrete  in their next token prediction while an LLM is more fuzzy.  LLMs have latent space where the meaning of a word basically exists as a vector.  LLMs will generate token sequences that didn't exist in the source material, whereas Markov Chains will ONLY generate sequences that existed in the source. This is why it's impossible to create a digital assistant, or really anything useful, via Markov Chain.  The fact that they only generate sequences that existed in the source mean that it will never come up with anything creative. Your example is too sparse to make a conclusion from I\u2019d offer an alternative interpretation: LLMs follow the Markov Decison modeling properties to encode the problem but use a very efficient policy for solver for the specific token based action space. That is to say they are both within the concept of a \u201cmarkovian problem\u201d but have wildly different path solvers. MCMC is a solver for an MDP, as is an attention network So same same, but different Would you be willing to write an article comparing the result", "positive": "Sergey Brin's Unretirement. > Having given so much of themselves to their careers, they often felt unmoored and purposeless when they left their jobs. That's in contrast with all of us who see the companies led by these guys as the cancer of society and we'd quit and never look back if we had FU money. My feelings aside, if all their purpose is to grow their company, I kinda get why they wouldn't give a damn about bettering the mankind, improving their communities or raising a healthy family. Financial freedom is about not having to worry about losing your job, or tolerating shitty work conditions. Why would you retire if you do what you love? I think the real problem might be if there's nothing you actually love doing (long term), that's when money won't help. Once you're hooked, you're hooked When I started my company, we suddenly found that we were in a good small fortune, not enough to be millionaires or billionaires, but enough to get people to run the business semi automatically with very minimum input from the founders. I took a semi retirement approach to the business, there really wasn't a lot of things to do, my role was sort of just \"managing\" programmers. I got so much free time that I could even start a second business on the side. Despite my best ability to stretch my work, I couldn't even fill up half of my working hours. One would have thought that this is heaven. But the time I was most free was also the time I was most miserable. I wasn't happy, I was gaining weight, I was perennially asking myself why the business couldn't be bigger and I couldn't sell it, so that I can be real millionaires and billionaires with financial freedom! Then fate intervened, the sudden fortune disappeared and I no longer had the luxury of just \"managing people\"; I have to do hands-on. And it was this activity, the feeling that I was contributing to something, that I was writing code again and actually building stuffs, that made me happy again. Today we are bigger than w", "negative": "Many Small Queries Are Efficient in SQLite. This feels like a very elaborate way of saying that doing O(N) work is not a problem, but doing O(N) network calls is. One index scan beats 200 index lookups though surely? I.e. sometimes one query is cheaper. It is not network anymore. Also you can run your \"big\" DB like postgres on the same machine too. No law against that. I\u2019ve been experimenting with LiveStoreJS which uses a custom SQLite WASM binary for event sync, so for simplicity I\u2019ve also used it for regular application data in browser and found no issues (yet). It surprised me that using a full database engine in memory could perform well vs native JS objects at scale but perhaps at scale is when it starts to shine. Just be wary of size limits beyond 16-20mb. Make sure you click this link  https://sqlite.org/src/timeline  So the sqlite developers use their on versioning system which uses sqlite for storage. Funny. The article doesnt make it at all clear what it is comparing to - mysql running remotely or on the same server? I'm sure sqlite still has less \"latency\" than mysql on localhost or unix socket, but surely not meaningfully so. So, is SQLite really just that much faster at any SELECT query, or are they just comparing apples and oranges? Or am i mistaken in thinking that communicating to mysql on localhost is comparable latency to sqlite? quite interesting. So SQL patterns can be optimised differently in SQLite There is some risk that, if you design your website to use a local database (sqlite, or a traditional database over a unix socket on the same machine), then switching later to a networked database is harder.  In other words, once you design a system to do 200 queries per page, you\u2019d essentially have to redesign the whole thing to switch later. It seems like it mostly comes down to how likely it is that the site will grow large enough to need a networked database.  And people probably wildly overestimate this.  HackerNews, for example, runs on a singl"}
{"anchor": "Eulogy for Dark Sky, a data visualization masterpiece (2023). Meteoswiss app is the best weather app ever created Yes Dark Sky had the best UI of any weather app I have used. I now use Weathergraph which does it differently but I would go back to Dark Sky (and pay for it) in a flash. It shows the correct things and on a phone understands that showing the temperatures across the screen is useless as if I go out I want to know what the weather is like when I might make the journey back in 8+ hours time. I might not care what the weather is in 4 hours time as I will be inside. Dark Sky was a marvel, and when it first came out, its ability to say rain will start where you are in 2-3 minutes was a marvel. The information design argument is 100% valid, but I also marvel that, having bought the company, Apple's weather app still isn't as precise or accurate. I don't know whether Apple's privacy focus prevents them making the same precise predictions, or if there is some other reason they don't, but it's sad that in 2025 we don't have the same level of performance as we did twelve years ago. Apple should have just used that app itself, rather than trying to build whatever that they have right now. The Apple Weather app has gotten better over time, though it\u2019s still not a perfect replacement. Scrolling through the Dark Sky screenshots, I can recognize many of the same things now incorporated with Apple\u2019s. And Apple does offer location specific notifications of rain which I find to be pretty accurate, about as accurate as Dark Sky. There\u2019s largely a perception problem with Apple. People loved Dark Sky as an independent small app that worked well, before Apple took it and destroyed it. Now, even if Apple incorporated all of the same data and features, it still wouldn\u2019t give that same spark of joy people had. fwiw, on iOS, I like using WeatherGraph:  https://weathergraph.app/  The developer is very responsive, lots of UI customization (both app and widgets) is possible, and pri", "positive": "Over 36,500 killed in Iran's deadliest massacre, documents reveal. I can't comprehend how a population can kill that many of their own people. They aren't even an \"other\" people, which has been the most common scapegoat lately. Same skin color, same religion, same language, same homeland. For comparison, estimates of the 1989 Tiananmen Square massacre death count are usually put in the 300-1,000 range by journalists and human rights groups.  https://en.wikipedia.org/wiki/1989_Tiananmen_Square_protests...  hm, I think we should re-evaluate sanctioning or civilian pressure campaigns, since the guise is for them to coax or turn on the government for regime change, but the government can just hire mercenaries from outside the country. don't know a solution but this one isn't it The source (Iran International) is backed by Saudi money and has a bias to dunk on Iran. That said, I'm sure the death count numbers from the Rasht Massacre are staggeringly higher than the initial tallies of 2-5k. This is certainly the end of peaceful Iranian protests. Whether it leads to a violent revolution or a static police state like North Korea remains to be seen. How is this possible without explosives? Even with vehicle mounted machine guns it seems like a crazy high number. Did the protestors get boxed in somehow? And across so many locations, that seems to require a crazy amount of coordination to kill so many in so little time. That's crazy. That's like ~40% of the deaths in the current gaza war, except over just 2 days instead of 2 years. This is depressing because we will go to war over this and it\u2019s going to be five years before people realizing they were tricked by \u201cbabies in incubators\u201d propaganda. The internet is fragile. Access can be so easily cut off for the masses in dire times. Take a good look US, because once you're down far enough the fascist drain, that's the cost of trying to claw your way back out. And there's no hope of external intervention given nuclear arms Earlie", "negative": "Tell HN: I cut Claude API costs from $70/month to pennies. Can you discuss a bit more of the architecture? Are you also adding the proper prompt cache control attributes? I think Anthropic API still doesn't do it automatically Consider using z.ai as model provider to further lower your costs. It sounds like you don\u2019t need immediate llm responses and can batch process your data nightly? Have you considered running a local llm? May not need to pay for api calls. Today\u2019s local models are quite good. I started off with cpu and even that was fine for my pipelines. This is the way. I actually mapped out the decision tree for this exact process and more here:  https://github.com/NehmeAILabs/llm-sanity-checks  Have you looked into  https://maartengr.github.io/BERTopic/index.html  ? You also can try to use cheaper models like GLM, Deepseek, Qwen,at least partially. As much as I like the Claude models, they are expensive. I wouldn't use them to process large volumes of data. \nGemini 2.5 Flash-Lite is $0.10 per million tokens. Grok 4.1 Fast is really good and only $0.20. They will work just as well for most simple tasks. consider this for addtl cost savings if local doesnt interest you -  https://docs.cloud.google.com/vertex-ai/generative-ai/docs/m...  Pretty straightforward. Sources dump into a queue throughout the day, regex filters the obvious junk (\"lol\", \"thanks\", bot messages never hit the LLM), then everything gets batched overnight through Anthropic's Batch API for classification. Feedback gets clustered against existing pain points or creates new ones. Most of the cost savings came from not sending stuff to the LLM that didn't need to go there, plus the batch API is half the price of real-time calls. This is what i was going to suggest too. Do they or any other providers offer any improvements on the often-chronicled variability of quality/effort from the major two services e.g. during peak hours? Or minimax - m2.1 release didn't make a big splash in the news, but it'"}
{"anchor": "Talking to LLMs has improved my thinking. This article matches my experience as well. Chatting with LLM has helped me to crystalize ideas I had before and explore relevant topics to widen the understanding. Previously, I wouldn't even know where to begin with when getting curious about something, but ChatGPT can tell you if your ideas have names, if they were explored previously, what primary sources there are. It's like a rabbit hole of exploring the world, a more interconnected one where barriers of entry to knowledge are much lower. It even made me view things I previously thought of as ultra boring in different, more approachable manner - for example, I never liked writing, school essays were a torture, and now I may even consider doing that out of my own will. Finally I can relate to someone\u2019s experience. For me even playing with image generators has improved my imagination. I share the sentiment here about LLMs helping to surface personal tacit knowledge and the same time there was a popular post[1] yesterday about cognitive debt when using AI. It's hard not to be in agreement with both ideas. [1]  https://news.ycombinator.com/item?id=46712678  Of course it has, I doubt this is uncommon. All my childhood I dreamed of a magic computer that could just tell me straightforward answers to non-straightforward questions like the cartoon one in Courage the Cowardly Dog. Today it's a reality; I can ask my computer any wild question and get a coherent, if not completely correct, answer. I agree that LLMs can be useful companions for thought when used correctly. I don\u2019t agree that LLMs are good at \u201csupplying clean verbal form\u201d of vaguely expressed, half-formed ideas and that this results in clearer thinking. Most of the time, the LLM\u2019s framing of my idea is more generic and superficial than what I was actually getting at. It looks good, but when you look closer it often misses the point, on some level. There is a real danger, to the extent you allow yourself to accept th", "positive": "Greenland sharks maintain vision for centuries through DNA repair mechanism. Sharks are so cool, man. They\u2019ve just been chilling on the planet for 400 million years, swimming the oceans while epochs passed them by in their periphery. Their entire biology is pretty much unchanged. They\u2019ve been sharks the whole time. This is so messed up harvesting the eye from a creature that lives hundreds of years. I guess they put the shark down. RIP one eye. It\u2019s sinful to fish and kill these ancient creatures up from the deep for minor scientific progress. So wait did they just catch a 200 year old shark and cut its eye ball out to have a look? Highly recommend the book \"Shark Drunk: The Art of Catching a Large Shark from a Tiny Rubber Dinghy in a Big Ocean\" by Morten Str\u00f8ksnes if you're interested in old sharks, small boats or deep oceans  https://bookshop.org/p/books/shark-drunk-the-art-of-catching...  I\u2019m starting to realise we don\u2019t really want a cure to aging. Imagine a world where people like Stalin never die. People like bill gates never have to pretend to be a nice person\u2026 If there\u2019s no chance of death, there will never be any progress in society. People in power would just establish a tighter and tighter grip. All the boomers would be immune to death and disease, but the treatment would be banned for the young because they haven\u2019t done enough to earn it. Unfortunately it also seems like these sharks are plagued by parasites in their eyes:  The shark is often infested by the copepod Ommatokoita elongata, a crustacean that attaches itself to the shark's eyes.[17] The copepod may display bioluminescence, thus attracting prey for the shark in a mutualistic relationship, but this hypothesis has not been verified.[18] These parasites can cause multiple forms of damage to the sharks' eyes, such as ulceration, mineralization, and edema of the cornea, leading to almost complete blindness.[11] This does not seem to reduce the life expectancy or predatory ability of Greenland shar", "negative": "A 26,000-year astronomical monument hidden in plain sight (2019). I first heard about this in a Graham Hancock book. Found it a fascinating example of an attempt to encode a date that far distant future generations might understand (provided it survives). That was an excellent rabbit hole to go down while eating lunch :) For a hypothesis concerning the precession of the equinoxes and religious pantheons, see  https://news.ycombinator.com/item?id=38761574  > There is an angle for doubt, for sorrow, for hate, for joy, for contemplation, and for devotion. I\u2019m so intrigued - what was going on inside Hansen's brain? This is the kind of stuff I love about ancient architecture. It seems they were full of such clever things (or maybe only the few constructions which survived until today). Its nice to see that some people still care about creating such thoughtful art for modern constructions. It seems that most building of our time are just optimized for fast and efficient construction. I hope there are many more out there, so that Earth's Graham Hancock of the year 16000 has something to explore on his/her ayahuasca trip. Sounds like it's about the precession of the equinoxes and the new \"Age of Aquarius\". More: >  Due to the precession of the equinoxes (as well as the stars' proper motions), the role of North Star has passed from one star to another in the remote past, and will pass in the remote future. In 3000 BC, the faint star Thuban in the constellation Draco was the North Star, aligning within 0.1\u00b0 distance from the celestial pole, the closest of any of the visible pole stars.[8][9] However, at magnitude 3.67 (fourth magnitude) it is only one-fifth as bright as Polaris, and today it is invisible in light-polluted urban skies.  >  During the 1st millennium BC, Beta Ursae Minoris (Kochab) was the bright star closest to the celestial pole, but it was never close enough to be taken as marking the pole, and the Greek navigator Pytheas in ca. 320 BC described the celestial"}
{"anchor": "Ask HN: What's the Point Anymore?. No point, buy tinned food and head for the darkest part of the forest. Do you read a book just so you know what happens at the end, or because you like the journey there too? Do you read blog posts \"just to know\" or because you like reading? Sure, if you don't like reading, then it's great you don't have to. But personally I like to read, and be taken on an adventure by writers, that's why I read, I don't read just so I \"know what happened\". So everything remains the same, nothing has changed. Nothing been destroyed by AI, it only seems to have destroyed your own perspective. The point is to cultivate the ability to distinguish between real and fake.  Soon enough, that ability will be extremely rare, and for the people who really need it, nothing else will do. Sounds like you're getting burned out by too much hype-chasing. Follow your interests, and you'll always discover something that AI hasn't solved by itself. And keep in mind that people have always had these concerns whenever something new came along - photography, computers, etc. > Why read a blog post, when Google AI Summary can just give you the summary? Because the summary is often wrong, and the summary might not even be the point? > Why read a book, when you can just get AI summary of it? You've been able to read a good summary by a human for most books on Wikipedia for decades now. Going to the example used thousands of times, maybe the horse drivers thought the same way, but guess what? now we have cars, race cars, super cars, flying cars. The engine kept changing, car markets kept evolving. \nPeople kept adapting. \nAdapting is the only way or the Penguin way :P Imho there are still tasks that can't be done by AI good enough. Wouldn't let clawbot handle my personal relationships. Not even scheduling a football [or dota2] game. Yet alone navigate job. So, maybe level up the goal post? Try do something not-easily-done by AI? Select from your fringe interests [if core is ", "positive": "\u201cThe closer to the train station, the worse the kebab\u201d \u2013 a \u201cstudy\u201d. Anecdotally, it's the same for coffee. Office lobby coffee shops are invariably terrible. The decent ones are always at least a 5-10 minute walk away. This makes intuitive sense. High mass-transit corridor real-estate (rail, air, road) leases come at a premium so those higher fixed-costs and must be balanced against a higher-volume of less-breadth of service with the same fixed (or even slightly higher) labor costs. In food service, high-volume is (mostly) inversely correlated with quality. Looking at their actual results ( https://preview.redd.it/znmnejgab5je1.png?width=1000&format=... ), I don't see any positive or negative correlation. Although I can subjectively confirm the hypothesis. I've observed the following: 1) An alarming number of regions in the world have a pizza joint called \"New York Pizza\", \"Manhattan Pizza\", or similar. 2) The similarity of the pizza therein to the actual thin, greasy slices served up in pizza joints from actual New York is inversely proportional to the location's distance from New York. So, the New York Pizza in Boston -- pretty close. The New York Pizza in Brisbane, QLD is alien by comparison and I think they consider \"pepperoni\" and \"salami\" interchangeable down there. He didn't find a correlation, or rather found that there is no correlation, between proximity to a railway station and how the kebab is reviewed. It's a nice study for a statistics class! The only place this isn't true is Japan. Always like reading the Best Kebab reviews on trip advisor. It\u2019s right next to Queen Street railway station so fits with the study.  https://www.tripadvisor.co.uk/Restaurant_Review-g186534-d125...  > Not only was my food uncooked but I also discovered a pubic hair in my chips and cheese, then when I proceeded to report the problem, I was chased with a knife. Down Dundas Street.Absolutely scandalous LOL we may need to update the title of this post, half the top level comment", "negative": "Judge order bars feds from altering or destroying evidence in Pretti shooting. The original title is: > Judge grants order barring feds from altering or destroying evidence in Pretti shooting Well that is reassuring. This administration has demonstrated an excellent track record of respecting and following judicial orders. Why is that even needed, honestly? Like is \"destroying or altering evidence\" usually legitimate or what? Not sure why anyone thinks a judges order is worth anything in the USA anymore. I am not reassured at all. Why was such an order needed? Seems like this should be the default and if you are caught tampering, straight to jail. I don\u2019t think this belongs on HN. There isn\u2019t a compelling technical angle and there are plenty of other venues to discuss politics. These judges can spend the rest of eternity issuing these orders but there is no mechanism to enforce it since the current administration has shown a complete disregard for precedence and mores. Currently doing the rounds of non US but allied special forces and commando chat groups is the blunt response of US General Tony Thomas, former head of the Special Operations Command (2016 to 2019) to each and every one of the senior Trump administration pushing the domestic terrorist line. Stephen Miller (for one of many) tweeted:     A would-be assassin tried to murder federal law enforcement and the official Democrat account sides with the terrorists.\n  \nGeneral Tony Thomas responded with a high resolution image of the first shot taken, from the rear, execution style:  https://x.com/TonyT2Thomas/status/2015629593265250810  I hope the US population can reign in Hegseth, Miller, Bondi, et al clown car. It's obvious to all across the globe what's going here. For reference:  https://en.wikipedia.org/wiki/Raymond_A._Thomas  can be compared to the bio of the day drinking weekend warrior currently heading the US DoD. Given Trump folks' previous behavior surrounding court orders of documents, I'm sure this"}
{"anchor": "Putting Gemini to Work in Chrome. This is a big deal, the highlight is Chrome autobrowse. Goes head-to-head with OpenAI Atlas. Serious question, not snark: Does anyone actually want this? I honestly can't imagine a use for these features even among people tech savvy enough to understand them. I must be using web browsers completely wrong. Like browsing a page isn't a problem for me. I can do it at the speed of my needs. I'm having a hard time understanding why I will tell gemini to create an account on some website for me or send an email. Those are usually just a tab away. That's why I feel like I'm missing something here. Allowing anyone to edit someone else's images from the browser with an AI model is deeply evil stuff. All I ever wanted my entire life. I feel whole again. Thank you Googool I wish this executive (author of that post)  https://xcancel.com/laparisa?lang=en   will show their browser in REAL LIFE everyday use. Really do they use it? Only PMs at Google need this. They still don't get how AI is used... I do hope we get vertical tabs before that, though. Gee thanks, now I have a big Ask Google buttong in the url bar but only on Youtube for some reason, how can I disable it? Could not figure how to disable like the others. > We\u2019re also bringing the creative power of Nano Banana directly into Chrome, allowing you to transform images on the fly without needing to download and re-upload images or open another tab. Are there really people who are like \"Man, if only I could this straight in Chrome\" ? Is this something worth bloating a browser (further) with?  https://blog.google/products/ads-commerce/agentic-commerce-a...  I think the vision here is for your browser agent to spend money with google's partners on pointless consumer slop while you sleep. Am I being too cynical, or does anyone else envision a future where you ask Chrome to buy you something, anything, online and instead of it actually buying you the \u201cbest\u201d item, you end up with items it \u201cprefer", "positive": "Show HN: Ten years of running every day, visualized. Love it! How did you stay motivated? Do you have the source/pipeline available? I love the design and would want to do something similar for my own runs. Congrats on the decade! Did you ever focus on specific metrics or was it always just about the run? just wanted to say the site looks awesome! I love the minimal black+white/grayscale and the fonts are just lovely. vis looks great too, I enjoyed poking around nearly all of the unique runs to look at the map and paces. This is so cool! At what point did you start thinking about this project? Like, were you quietly working on it a year ago after every run, just waiting for this moment? And hey, great run in Japan! (Tokyo here!) I love the map visualization too. Love it! I will hit one year mark in a couple of weeks. Currently maintaining stats in a Google spreadsheet :)  https://vijaykillu.com/  SVGs? So, some of the staistics graphs do not update, or have you made them dynamic by hand? beautifullllllll\u2014both the streak and the stack. Love how lightweight the architecture is for something so personal and long-term. Curious if you noticed any patterns in the data that surprised you once you visualized it? Impressive. I did streak running for 6 months nice and it was some of the most productive running in my life. Interestingly I have much higher yearly averages than you do but still consider daily streak running quite hard. Not being a morning runner myself might contribute since I get into a lot of close calls that way. My streak literally ended when my daughter went into the hospital and I couldn\u2019t well just fuck off for a run any longer. That's awesome! any tips for people who are just starting out? do you have code it on github ? I don't have the tenacity to run strictly _everyday_, so as a middle ground I don't run when it rains at anytime during daylight. Of course the effectiveness of this rule depends on where you live :P I\u2019ve always wanted to do this, but I ", "negative": "Show HN: A small programming language where everything is pass-by-value. At the risk of telling you what you already know and/or did not mean to say: not everything can be a value. If everything is a value then no computation (reduction) is possible. Why? Because computation stops at values. This is traditional programming language/lambda calculus nomenclature and dogma. See Plotkin's classic work on PCF (~ 1975) for instance; Winskel's semantics text (~ 1990) is more approachable. Things of course become a lot more fun with concurrency. Now if you want a language where all the data thingies are immutable values and effects are somewhat tamed but types aren't too fancy etc. try looking at Milner's classic Standard ML (late 1970s, effectively frozen in 1997). It has all you dream of and more. In any case keep having fun and don't get too bogged in syntax. (Edit: in the old post title:) \"everything is a value\" is not very informative. That's true of most languages nowadays. Maybe \"exclusively call-by-value\" or \"without reference types.\" I've only read the first couple paragraphs so far but the idea reminds me of a shareware language I tinkered with years ago in my youth, though I never wrote anything of substance: Euphoria (though nowadays it looks like there's an OpenEuphoria). It had only two fundamental types. (1) The atom: a possibly floating point number, and (2) the sequence: a list of zero or more atoms and sequences. Strings in particular are just sequences of codepoint atoms. It had a notion of \"type\"s which were functions that returned a boolean 1 only if given a valid value for the type being defined. I presume it used byte packing and copy-on-write or whatever for its speed boasts.  https://openeuphoria.org/  -  https://rapideuphoria.com/  > In herd, everything is immutable unless declared with var So basucally everything is var? I have implemented similar behavior in some of my projects. For one, I also have also implemented 'cursors' that point to some p"}
{"anchor": "RSS.Social \u2013 the latest and best from small sites across the web. It's nice to have a lot of small web content laid out like this! Some suggestions to make discovery more palatable: - 1-2 sentence summaries for the content. most titles are not sufficiently descriptive and clicking on something un-interesting a few times is a sure fire way to get folks to churn - checks for included feeds that they are correctly configured and the resources load in-browser (not download a random file to my computer) We don't have a RSS Feed yet, but the alivenet will welcome your visit -  https://vvesh.de  I really like it! Sometimes I just want to read something random and sampling from small personal websites is a great way to discover new people to follow.      Learn how it works. -> This page is incomplete\n  \nA pity. It's obvious what the \"latest\" would be, but what is the best? How is that decided? Shameless plug: for randomly discovering IndieBlogs check out  https://indieblog.page/  Since \"best\" is subjective maybe give a description of the kind of content you hope to feature and why you think it's important. Using HN as a filter for Kagi's Small Web list[0] works really well:  https://hcker.news/?smallweb=true  I find the small web feed too noisy without it. [0]:  https://kagi.com/api/v1/smallweb/feed  Love this! RSS is alive and well! Going to try and integrate this into my personal RSS Tinder:  https://news.ycombinator.com/item?id=46680013  .social domain has a negative connotation nowadays. Not a wide in the number of sources (yet), but I'm curating a directory/reader/search engine of personal blogs, and the \"Global\" view shows the latest posts across 1300+ feeds:  https://minifeed.net/global  Great! And funny too, just posted about my personal take on the same topic:  https://squeaki.sh/p/i-turned-my-website-into-my-feed-reader...  I was surprised to find my blog there. I did a git blame on  https://github.com/kagisearch/smallweb  and it was in the initial commit, so I gu", "positive": "Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete. I read the release but didn't quite understand the difference between a next-edit model and a FIM model - does anyone have a clear explanation of when to use one over the other? I'd love if there was a sublime plugin to utilize this model and try it out, might see if I can figure that out. I use Sweep\u2019s Jetbrains autocomplete plugin daily, it really stands out. Based on qwen2.5-coder? seems like a \"why not/resume embellish/show VC\" type release I guess can it be integrated in monaco editor ? So SFT cost less only low hundreds of dollars? (1-10$ per hour per H100 if I'm seeing this correctly). What about SFT? Presumably basing this of Qwen is the reason it can be done for so cheap? Wow super fun read, I love how it went into the technical details. Any way to make it work with vscode? This is cool! I am more interested in how you guys generated next edit training data from repos, seems like there are lots of caveats here. Would love your insights Again amazing work! waiting for what you guys cook next I'm very green to this so forgive if this question sounds silly: Would instead of the RL step a constrained decoding say via something like xgrammar fix   syntax generation issue ? Do you plan to release Sweep 3B/7B on HF? It's good. The blog post about it is very interesting.\nI hope, a plugin for neovim will be made soon.  https://blog.sweep.dev/posts/oss-next-edit  Followed your work since the beginning and used it for inspiration for some cool demos on self-healing web scrapers. fascinating to see the transition from original concept to producing models. cool stuff. Very interesting - and cool to read about the development process. I'd love to hear more about how genetic algorithm worked here. I wonder whether we are perhaps the point of usefulness of 'next edit' code development in 2026 though. Any easy way to try on vscode? Surprising how badly Jetbrains implemented AI. Apparently to such an extent ", "negative": "San Francisco coyote swims to Alcatraz. I would be surprised if the Coyote would be quick to get back into the water after such a difficult swim. It would, I suspect, want to recover and find food. So I support the theory the Coyote is just hiding somewhere. The island is small but not that small that it couldn\u2019t hide somewhere. I wonder if a turtle drowned halfway across. If a Coyote could do it, all those famous escapees must have had too. That roadrunner thought he'd be safe hiding out on the notorious prison island... It's a 1.5mi swim. I remember visiting Angel Island (a 0.5mi swim) and seeing the abundance of raccoons they have, and asked a ranger how they got there. They also swam. Growing up on a lake I would regularly watch deer swim the quarter mile back and forth between the shore and a nearby island, with no problem. Video:  https://www.youtube.com/watch?v=br4-VsvRcII  Poor thing, talk about going in the wrong direction :) Impressive though. If you time things right, and don't get swept out to sea, it's the 54 degree water that is the real danger. I'm no medical person, but it sure seems like that the animal is suffering from hypothermia and fatigue. I'm sure it'll have happy hunting once it recovers. This sort of thing is a huge problem here in New Zealand. The only native mammal here is a bat, we have mostly birds which evolved for a really long time with only avian predators. So they\u2019re hilariously poorly adapted for surviving standard predators (cats, rats, dogs etc) which first the Maori and subsequently Europeans brought. For example, many of them are flightless and tend to freeze when threatened - works well against eagles but is a terrible idea when threatened by a cat. As a result, we have many animals, mostly birds, which are totally unique and also critically endangered. Many of them can only survive on offshore islands which have been comprehensively cleared of predators at vast effort and expense. The islands need to be relatively accessible"}
{"anchor": "I pulled data on 1378 restaurants from Google Maps to rank them in order. Hi Matt! I did something similar for  https://sweetspots.fr/  it would get updated every week and was very helpful for discovering cities. I stopped maintaining the project 1 year ago so the list are getting stale now but it was fun while it lasted. Glad to see someone else look into this! I want to do this where I live! I think it might be easier for me because my county has an open data portal with a list of the restaurants (because of inspections) and their locations. Looks like Boulder County has one too  https://opendata-bouldercounty.hub.arcgis.com/documents/c9d2...  >  I saw some shady third-party tool that scrapes the data, but it's against the Terms of Service and I don't want to worry about being banned from my entire Google ecosystem. I'm pretty sure Google Terms prohibits using data from its Maps API otherwise than in connection with displaying/using a Google Maps service, but maybe they won't go after this, because it's not much data. Copyright Matt Sayar\u00a9, yet: 3.2.3 Restrictions Against Misusing the Services. (a) No Scraping. Customer will not export, extract, or otherwise scrape Google Maps Content for use outside the Services. For example, Customer will not: (i) pre-fetch, index, store, reshare, or rehost Google Maps Content outside the services; (ii) bulk download Google Maps tiles, Street View images, geocodes, directions, distance matrix results, roads information, places information, elevation values, and time zone details; (iii) copy and save business names, addresses, or user reviews; or (iv) use Google Maps Content with text-to-speech services. Foursquare released its database of places, maybe that would be more interesting to OP (as well as the data from OSM).  Foursquare's 104M Points of Interest   https://news.ycombinator.com/item?id=42219578   Foursquare Open Source Places: A new foundational dataset   https://news.ycombinator.com/item?id=42191781  Demo of the datas", "positive": "Total monthly number of StackOverflow questions over time. The decline is not surprising. I am sure AI is replacing Stackoverflow for a lot of people. And my experience with asking questions was pretty bad. I asked a few very specific questions about some deep detail in  Windows and every time I got only some smug comments about my stupid question or the question got rejected outright. That while a ton of beginner questions were approved. Definitely not a very inviting club. I found i got better responses on Reddit. Do I read that correctly \u2014 it is close to zero today?! I used to think SO culture was killing it but it really may have been AI after all. Probably similar for google. My first line of search is always chatgpt Everything we have done and said on the internet since its birth has just been to train the future AI. Now that StackOverflow has been killed (in part) by LLMs, how will we train future models? Will public GitHub repos be enough? Precise troubleshooting data is getting rare, GitHub issues are the last place where it lives nowadays. The result is not surprising! Many people are now turning to LLMs with their questions instead. This explains the decline in the number of questions asked. Wow. I was expecting a decline but not to  that  extent. They will no doubt blame this on AI, somehow (ChatGPT release: late 2022, decline start: mid 2020), instead of the toxicity of the community and the site's goals of being a knowledgebase instead of a QA site despite the design. PS - This comment is closed as a [duplicate] of this comment:  https://news.ycombinator.com/item?id=46482620  Not a big surprise once LLMs came along: stack overflow developed some pretty unpleasant traits over time.  Everything from legitimate questions being closed for no good reason (or being labeled a duplicate even though they often weren\u2019t), out of date answers that never get updated as tech changes, to a generally toxic and condescending culture amongst the top answerers.  For all ", "negative": "Iran's internet blackout may become permanent, with access for elites only. \u2026 while every other country waits to see how it goes while drafting plans to emulate this Do they have something like intranet with some local services, like in DPRK&Cuba? is this the case of completely losing connection and devices practically bricked for anything other than displaying the time? If I were a betting man I'd wager that technological determinism wins in the end. No shot. The economy is already in the gutter. The productivity hit of a total internet cutoff would be a death sentence Can ROTW sanction Iran by giving it zero internet access even to \"elites\" by refusing to peer. Spacex satellites blockage was the surprise. How did they do it? I thought it would be the best dooms day kind of insurance. Turns out not. But they unblocked it on Wed/Thur, I've been talking to friends normally since then. They already have uncensored unfiltered sim cards they issue to their own people, we found that out when X (Twitter) started showing which country you made the accout from and thousands of people had Iran which normal people can't access X without VPN. Its just that they shut off the internet for normal people now, which they hadn't done before. I\u2019m curious if it\u2019s possible to somehow retrieve the whitelist to see who\u2019s on it? It actually surprised me that they didn't do it before. China already achieved this in 2010s. There is active discussion on net4people about using DNSTT, but as more of these tunnels go up, I'm sure it will be blocked. Given the denied environment the Iranian people see themselves in. I believe its worth mentioning asynchronous networks[1]. For example, they could use NNCP[2] in sneakernet style op[3]. Couriers could even layer steganography techniques on top on the NNCP data going in and out on USB drives. This can all be done now, and doesn't require new circumvention research or tools. NNCPNET[4] is now active which provides email over NNCP and therefore can be"}
{"anchor": "AI 2027. Feels reasonable in the first few paragraphs, then quickly starts reading like science fiction. Would love to read a perspective examining \"what is the slowest reasonable pace of development we could expect.\" This feels to me like the fastest (unreasonable) trajectory we could expect. Ok, I'll bite. I predict that everything in this article is horse manure. AGI will not happen. LLMs will be tools, that can automate away stuff, like today and they will get slightly, or quite a bit better at it. That will be all.\nSee you in two years, I'm excited what will be the truth. This is absurd, like taking any trend and drawing a straight line to interpolate the future.\n If I would do this with my tech stock portfolio, we would probably cross the zero line somewhere late 2025... If this article were a AI model, it would be catastrophically overfit. AI now even got it's own fan fiction porn. It is so stupid not sure whether it is worse if it is written by AI or by a human. \"we demand to be taken seriously!\" Older and related article from one of the authors titled \"What 2026 looks like\", that is holding up very well against time. Written in mid 2021 (pre ChatGPT)  https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-...  //edit: remove the referral tags from URL I just spent some time trying to make claude and gemini make a violin plot of some polar dataframe. I've never used it and it's just for prototyping so i just went \"apply a log to the values and make a violin plot of this polars dataframe\". ANd had to iterate with them for 4/5 times each. Gemini got it right but then used deprecated methods I might be doing llm wrong, but i just can't get how people might actually do something not trivial just by vibe coding. And it's not like i'm an old fart either, i'm a university student > \"OpenBrain (the leading US AI project) builds AI agents that are good enough to dramatically accelerate their research. The humans, who up until very recently had been the best AI r", "positive": "Ask HN: How are you doing RAG locally?. Local LibreChat which bundles a vector db for docs. If your data aren't too large, you can use faiss-cpu and pickle  https://pypi.org/project/faiss-cpu/  LightRAG, Archestra as a UI with LightRAG mcp A little BM25 can get you quite a way with an LLM. try out chroma or better yet as opus to! Don't use a vector database for code, embeddings are slow and bad for code. Code likes bm25+trigram, that gets better results while keeping search responses snappy. lee101/gobed  https://github.com/lee101/gobed  static embedding models so they are embedded in milliseconds and on gpu search with a cagra style on gpu index with a few things for speed like int8 quantization on the embeddings and fused embedding and search in the same kernel as the embedding really is just a trained map of embeddings per token/averaging I built a lib for myself  https://pypi.org/project/piragi/  Embedded usearch vector database.  https://github.com/unum-cloud/USearch  Any suggestion what to use as embeddings model runtime and semantic search in C++? The Nextcloud MCP Server [0] supports Qdrant as a vectordb to store embeddings and provide semantic search across your personal documents. This enables any LLM & MCP client (e.g. claude code) into a RAG system that you can use to chat with your files. For local deployments, Qdrant supports storing embeddings in memory as well as in a local directory (similar to sqlite) - for larger deployments Qdrant supports running as a standalone service/sidecar and can be made available over the network. [0]  https://github.com/cbcoutinho/nextcloud-mcp-server  I have done some experiments with nomic embedding through Ollama and ChromaDB. Works well, but I didn't tested on larger scale  https://duckdb.org/2024/05/03/vector-similarity-search-vss   https://github.com/ggozad/haiku.rag/  - the embedded lancedb is convenient and has benchmarks; uses docling. qwen3-embedding:4b, 2560 w/ gpt-oss:20b. I thought that context building via ", "negative": "Netflix Animation Studios Joins the Blender Development Fund as Corporate Patron. Brilliant. Some of the animations that are put as showcases on the Blender site are absolutely phenomenal. \nThis one  https://studio.blender.org/projects/spring/  particularly is my all time favorite. I think especially since the UI overhaul in Blender 2.8 the project has been on a steep upwards trajectory. The software was always amazing, especially since it was free and open source, but the new UI and all subsequent improvements really put Blender on the map as a serious tool and not just an alternative for when you don't have money for the big players. Very cool news. Personally, I'd love to see some more focus on game-dev workflows. The game asset pipeline still feels janky: texture painting exists, but not great, and baking textures/previewing results or baking from high poly to low poly involves a lot of manual node fiddling and rewiring. Export/iterate/build/test cycles are also pretty painful still. How does it compare to Maya these days? <3 Blender is a treasure and must be protected. I really like Blender and it's an amazing product, but I can't get over the standard Blender keymap. The \"industry compatible\" workflow is more sane, but then I have to translate tutorials from the Blender keymap to the industry compatible controls, and they're not always 1:1 How does that translate into real cash? If someone is wondering who the Aras guy is\n https://mastodon.gamedev.place/@aras/115971315481385360  Anyone know why Netflix doesn\u2019t respond to their job site?  I applied to several positions where I\u2019m an  exact  match, with a decade of VFX and another decade of internet company experience in LA.  Never heard a single word in response from them, for years.  Reqs stay open a long time as well.  What are they doing?  Are they ghost jobs?  They don\u2019t even respond with a \u201cno\u201d form letter.  (Lately their site is broken at the verify email stage, pin post returns 403.) We might see a transi"}
{"anchor": "Ask HN: How are you doing RAG locally?. Local LibreChat which bundles a vector db for docs. If your data aren't too large, you can use faiss-cpu and pickle  https://pypi.org/project/faiss-cpu/  LightRAG, Archestra as a UI with LightRAG mcp A little BM25 can get you quite a way with an LLM. try out chroma or better yet as opus to! Don't use a vector database for code, embeddings are slow and bad for code. Code likes bm25+trigram, that gets better results while keeping search responses snappy. lee101/gobed  https://github.com/lee101/gobed  static embedding models so they are embedded in milliseconds and on gpu search with a cagra style on gpu index with a few things for speed like int8 quantization on the embeddings and fused embedding and search in the same kernel as the embedding really is just a trained map of embeddings per token/averaging I built a lib for myself  https://pypi.org/project/piragi/  Embedded usearch vector database.  https://github.com/unum-cloud/USearch  Any suggestion what to use as embeddings model runtime and semantic search in C++? The Nextcloud MCP Server [0] supports Qdrant as a vectordb to store embeddings and provide semantic search across your personal documents. This enables any LLM & MCP client (e.g. claude code) into a RAG system that you can use to chat with your files. For local deployments, Qdrant supports storing embeddings in memory as well as in a local directory (similar to sqlite) - for larger deployments Qdrant supports running as a standalone service/sidecar and can be made available over the network. [0]  https://github.com/cbcoutinho/nextcloud-mcp-server  I have done some experiments with nomic embedding through Ollama and ChromaDB. Works well, but I didn't tested on larger scale  https://duckdb.org/2024/05/03/vector-similarity-search-vss   https://github.com/ggozad/haiku.rag/  - the embedded lancedb is convenient and has benchmarks; uses docling. qwen3-embedding:4b, 2560 w/ gpt-oss:20b. I thought that context building via ", "positive": "Presence in Death. The research mentioned in the article (which indicates no EEG activity):  https://pmc.ncbi.nlm.nih.gov/articles/PMC7876463/  If TRUE and consciousness is eternal (using whatever framework you like), the idea of death becomes double sided. On one hand, physical death is sad as it's the end of this physical 'epic story' before our consciousness moves onto a new body/story. On the flip side, approaching body death is a sort of a temporary great relief as we are immortal and cannot actually die. I.e. After ten thousand years of being alive, a vampire looks forward to sleeping in their coffin at night. There are stories about bodies of Christian monks that did not decompose for a log time after the dearth. Modern take on it attributes it to the climate in caves where the body was put after the dearth. But another important part was diet. Often the well-preserved bodies were of those who had eaten only rough bread and water for months and years before the dearth. So I suspect both of the factors are at the play here as well. Here's the documentary referred to:  https://www.imdb.com/title/tt21945758/  It's rentable at the usual places. I might check it out. I have been studying and practicing tibetan buddhism for a little over a year now, particularly dream yoga, but branched into some of the other practices. I'm always a skeptic but it is fascinating some of the stuff they can do. There is scientific evidence they can raise and lower their body temperatures through meditation, withstanding great heat/cold and deprivation conditions. I've played around with deprivation and what it has done for my mental health and body has surprised me. I'm but a novice, but I absolutely believe they are tapping into something scientific about the body/mind that is still unknown. There are reasons to be extremely skeptical about some of their claims, but, some of it is very interesting and credible. Some of these esoteric states are really weird, and it seems crazy to me", "negative": "Adoption of EVs tied to real-world reductions in air pollution: study. I was out skating today. Everyone was having a fun time until a diesel truck simply drove down the nearby road. It stunk up and polluted the frozen lake air for a solid few minutes. I hate diesel trucks with a passion and if I live long enough to see it happen, I will celebrate the day they become defunct. Tesla's EV trucks need to deal the same hard kick to diesel trucks that they did to cars. No surprises. No matter how we look at it, EVs are much friendlier and safer to the environment. Some people argue the source of electricty can be contested against because that involves fossil fuel burning again, but in today's world we are rapidly moving away from it and towards nuclear/hydel/wind methods for generating power. I hope ICE cars completely become a thing of the past in the next couple of decades to come. Having spent a significant amount of time in Bangkok - the city center (and many urban hubs) is an amazing walkable place with pedestrian walkways suspended above major roads, lots of frequent public transit (metro, skytrain) that honestly makes my home city of Sydney feel like a developing country. The only downside is that traffic creates a lot of pollution, and the engine noise (not honking, there's very little of that) is so bad that you need to yell to a person standing next to you to have a conversation. As a visitor, I can't claim to know how to fix the problems facing locals, however I can't help but feel that urban centers would be 1000x better with mass adoption of EVs (bikes, cars). I have seen a spike in the number of Chinese EVs across the city - however I'm aware that economic pressures prevent mass adoption by the majority of the road-users This study is about air quality  in neighborhoods . So it would show the same thing even if EVs just moved pollution from where people use their cars to where power plants get placed, because that's not the question it's addressing. Has th"}
{"anchor": "Vanguard's average fee is now 0.07% after biggest-ever cut. Not mentioned in any of the coverage I've seen (or the interview with Vanguard's new CEO in the WSJ) is Fidelity. Fidelity used to be known for actively managed funds, but has been eating Vanguard's indexing lunch for the past 10 years or so. Part of this relates to its dominance in workplace accounts, but Vanguard hasn't helped itself with some bad customer-facing software updates and a perception that its service levels are poor compared to Fidelity. Cutting fees helps, but Fidelity has shown its willing to do this, too, including no fee \"Zero\" index funds:  https://www.fidelity.com/mutual-funds/investing-ideas/index-...  (note Fidelity is very clear about who it's competing with) Article mentions their bond funds getting the most dramatic cuts \u2014 they didn't list specific symbols though. Anyone know off the top of their heads which funds specifically? Thinking I need to move away from being so stock-heavy. I always upvote the archive link unless it is already the top comment, ha ha. That only applies to US funds, but not in the UK ones which continue to be significantly more expensive... Didn't they recently increase uk fees a tonne? Straight from the source:  https://corporate.vanguard.com/content/corporatesite/us/en/c...  Let's not forget that Vanguard has taken a strong stance against crypto [0]. Claiming to significantly invest in technology while deliberately ignoring the latest advancements in financial technology, seems contradictory. If their business was doing so well, they wouldn't have to lower fees. [0]  https://news.ycombinator.com/item?id=42832026  Someone correct my math here, but if they have 10 trillion in assets under management and the management fee is 0.07% then that's still 7,000,000,000 or 7 billion in fees every year? Not bad Unless you have some super special edge, Vanguard is really good IMO. Having a 0.01% or 0.05% fund is really as good as you can do and never pay attention. Va", "positive": "Attention lapses due to sleep deprivation due to flushing fluid from brain. Long live healthy sleep for brain health, and thank goodness light exercise helps this same glymphatic system. I slept around 5 hours last night split up into two periods because my baby daughter woke up crying from fever and wanted to play / was hallucinating / etc. She's totally fine now but I am wondering if there is a correlation between dementia and having kids. I wonder if a 30-min nap improves the situation. But I need to tell the brain to hold the flushing until the nap. Good to know that the brain finds a way to flush itself while awake. I think I've become pretty good at putting unused parts of my brain to sleep while awake. My brain is like that of a dolphin now. But on rare occasions (like a couple of times a year), I get migraine auras and stuff disappears from my field of view. Can last about an hour. I feel like that's my visual cortex falling asleep. Rest in peace to all the college dudes covering the whole syllabus within 24 hours of the exam anecdotally, i never feel better than when i haven't slept. spent 8pm tuesday -- 8pm thursday this week awake nursing cheap energy drinks, and not only could i manage a higher-than-usual level of focus, i was genuinely content. bombed a midterm halfway though, but at least i felt good about it. I wonder if this could help explain why creatine helps mitigate the effects of sleep deprivation. Since creatine aids in water retention.  https://pubmed.ncbi.nlm.nih.gov/16416332/  So biological garbage collection pauses then? skip sleep, and the brain tries to run gc cycles during runtime. Causing attention and performance latency spikes. Evolution wrote the original JVM. [This is one of those article titles that would really benefit from adding one more word.] > For example, what you don't want to do is NOT take amphetamines at testing if you had used them to study; Hard disagree there. If you get any anxiety during the test it's better to tak", "negative": "LM Studio 0.4. This release introduces parallel requests with continuous batching for high throughput serving, all-new non-GUI deployment option, new stateful REST API, and a refreshed user interface. LMStudio introducing a command line interface makes things come full circle. I\u2019m really excited for lmster and to try it out. It\u2019s essentially what I want from ollama. Ollama has deviated so much from their original core principles. Ollama has been broken and slow to update model support. There\u2019s this \u201cvendor sync\u201d I\u2019ve been waiting (essentially update ggml) for weeks. What\u2019s the main use-case for this? I get that I can run local models, but all the paid for (remote) models are superior. So is the use-case just for people who don\u2019t want to use big tech\u2019s models? Is this just for privacy conscious people? Or is this just for \u201cadult\u201d chats, ie porn bots? Not being cynical here, just wanting to understand the genuine reasons people are using it. edit: disregard, new version did not respect old version's developer mode setting man they really butchered the user interface, the \"dark\" mode now isn't even dark, it's just grey, and it's looking more like a whitespacemaxxed children's toy than a tool for professionals I was hoping for the /v1/messages endpoint to use with Claude Code without any extra proxies :( lmster is what was lacking in lmstudio (yes, they have lms but it lacks so many functionalities that the GUI version has). but it's a bit too little too late. people running this probably can already setup llama.cpp pretty easily. lmstudio also has some overhead like ollama; llama.cpp or mlx alone are always faster. Personally, I would not run LM Studio anywhere outside of my local network as it still doesn't support adding an SSL cert. I guess you can just layer a proxy server on top of it, but if it's meant to be easy to set up, it seems like a quick win that I don't see any reason not to build support for.  https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/1"}
{"anchor": "De-dollarization: Is the US dollar losing its dominance? (2025). It's not losing it so much as that it is being destroyed on purpose. The international value of the dollar as a reserve and trade currency is inherently tied to the behavior of the US Government and the Federal Reserve. The behavior of the US Government has been very unusual lately, and the independence of the Federal Reserve is actively being challenged. So draw from that whatever conclusions you wish. Trump is destroying it intentionally. Compromised people and useful idiots within the Trump administration are being persuaded by Russia to destroy the dollar and break up NATO. No idea how those things work but surprised the $/\u20ac exchange rate stabilized. If the goal is to make US goods attractive to other countries and to decrease our trade deficit (not saying I agree with this goal), either the dollar has to become fundamentally weaker or the goods have to become more valuable.  The latter feels more difficult than the former at this point.  However, the side effects of a weaker dollar may not be worth weakening it. If Trump announces some toady lunatic to run the Fed, watch out below, because the dollar is going to crash.  I know I have moved a bunch of money into international stocks and currency and I suspect when the right leaning crowd finally catches on it will be a stampede. The export driven economies like China or the EU rely on the dollar to weaken their own currencies for competitive trade. Without it, natural FX mechanisms would naturally begin to appreciate their currencies and make their exports uncompetitive. The biggest problem of all social sciences is that they measure only what can be measured or is easier to measure. Sorry for the redundancy, but they don't see what is hard to see and, therefore, think it doesn't exist. I suspect there might be a lot of \"de-dollarization\" going on in realms that might not be easy to measure. To be specific: it is interesting that crypto-currencies ", "positive": "Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model. > For complex tasks, Kimi K2.5 can self-direct an agent swarm with up to 100 sub-agents, executing parallel workflows across up to 1,500 tool calls. > K2.5 Agent Swarm improves performance on complex tasks through parallel, specialized execution [..] leads to an 80% reduction in end-to-end runtime Not just RL on tool calling, but RL on agent orchestration, neat! Those are some impressive benchmark results. I wonder how well it does in real life. Maybe we can get away with something cheaper than Claude for coding. Kimi was already one of the best writing models. Excited to try this one out Huggingface Link:  https://huggingface.co/moonshotai/Kimi-K2.5  1T parameters, 32b active parameters. License: MIT with the following modification:  Our only modification part is that, if the Software (or any derivative works\nthereof) is used for any of your commercial products or services that have\nmore than 100 million monthly active users, or more than 20 million US dollars\n(or equivalent in other currencies) in monthly revenue, you shall prominently\ndisplay \"Kimi K2.5\" on the user interface of such product or service.  Actually open source, or yet another public model, which is the equivalent of a binary? URL is down so cannot tell. I've read several people say that Kimi K2 has a better \"emotional intelligence\" than other models. I'll be interested to see whether K2.5 continues or even improves on that. There are so many models, is there any website with list of all of them and comparison of performance on different tasks? Curious what would be the most minimal reasonable hardware one would need to deploy this locally? The chefs at Moonshot have cooked once again. As your local vision nut, their claims about \"SOTA\" vision are absolutely BS in my tests. Sure it's SOTA at standard vision benchmarks. But on tasks that require proper image understanding, see for example BabyVision[0] it appears very much lacking compar", "negative": "Sumerian Star Map Recorded the Impact of an Asteroid (2024). That is one crazy story. I need to see this done in Hollywood graphics. They're claiming the asteroid came in so low that it did a flyby of the Levant, igniting any flammable object or person on its way, and slammed into the side of a mountain in the Alps It's definitely not what I normally picture when I think about asteroids. We found an ancient tablet, dated it, reconstruded a long-dead language well enough to read it, reconstructed the night sky on that day, five and a half thousand years ago, found the orbit of this thing, and connected it to a geological formation thousands of kilometers away. Humans can do some amazing stuff.  https://www.bristol.ac.uk/news/2008/212017945233.html   http://historyofgeology.fieldofscience.com/2011/04/landslide...  There is something here that I do not understand. The article claims that \u201c[The tablet] is a copy of the night notebook of a Sumerian astronomer as he records the events in the sky before dawn on the 29 June 3123 BC\u201d But radiocarbon dating of trees buried in the landslide seems to have reliably dated the landslide to 7500 BC. For example  https://www.sciencedirect.com/science/article/abs/pii/S01695...  Update: The Wikipedia article about the coauthor Mark Hempsell says: \u201cHempsell got public audience as author of the book \"A Sumerian Observation of the K\u00f6fels' Impact Event\", with Alan Bond proposes a theory not accepted by the scientific community\u2026\u201d The link posted in this thread by user arto calls the theory \u201cpseudoscience\u201d: \u201cDespite this new evidence, curiously in 2008 the impact hypothesis was revived by some pseudoscientists in connection to supposed observations of a meteorite by the Sumerians\u2026\u201d Now it seems very suspicious that the article claims that the tablet is from 3123 BC, when it was excavated from the palace of Ashurbanipal (650 BC). I bought the book \"A SUMERIAN OBSERVATION OF THE K\u00d6FELS\u2019 IMPACT EVENT\" by Mark Hempsell, and Alan Bond. Tried to "}
{"anchor": "Trump says Venezuela\u2019s Maduro captured after strikes. Prediction: this headline will be renamed \"US invades Venezuela\" very soon. Footage is quickly spreading, looks like strikes on military bases as well as a bunch of low-flying helicopters, so a strike + a ground invasion? They didn't even  try very hard to manufacture consent for a war against Venezuela. Wonderful. It will be pretty amusing to watch all those westerners who, not so long ago, were talking about \"rules based order\" pretend nothing is happening or to justify it. Always remember the role of the Nobel Peace Prize committee in preparing this unprovoked and illegal (under international law) attack on Venezuela by awarding the prize to Mar\u00eda Corina Machado. Julian Assange actually filed a Swedish criminal complaint against Nobel Foundation officials, alleging misappropriation of Nobel endowment funds and facilitating war crimes and crimes against humanity in connection with the 2025 Nobel Peace Prize awarded to Mar\u00eda Corina Machado, and it seeks immediate freezing of funds and a full investigation:  https://just-international.org/articles/assanges-criminal-co...  FIFA looking awful silly right now.  https://vxtwitter.com/FaytuksNetwork/status/2007338956241985...  Not Venezuelan helicopters... They're American aircraft. It sure seems like after repeatedly threatening to invade Venezuela, Trump is now invading Venezuela. For what though? Don't know why, this link gives me: Access Denied Our apologies, the content you requested cannot be accessed. I think something like The Hague is the moderate position with this administration. It is definitely not Russia unprovokenly and illegibly attacking its neighbor, so why even care? How does this differ from Russia invading Ukraine? We have to wake up to the world where USA no longer cares about ideals like liberal democracy or allies, but is a warmongering corporatist autocracy. after Iran and now venezuela Iran, I totally understand that if they want to acquire n", "positive": "How Google Maps allocates survival across London's restaurants. Interesting work, but ultimately silly: of course google maps ranks results.  No one (yes, yes, I'm sure like 3 people) want a list of all results, unordered or ordered by something useless like name, when they type in restaurant.  And I cannot put into words how uneager I am to have the city or state government manage what comes up when I put indian or burrito into a map. The other commenter thought the work was silly, but I think it's brilliant. Keep at this!! You're making me hungry :) super interesting project. I would love to generate a similar list for my own neighbourhood I love the idea! And I want to have it for my city :) Is there a project on GitHub or somewhere that I could clone??  (smiling face with halo)  Very interesting. But I wonder how much Google (and other) Maps can actually shape the scene. For tourist hotspots with a lot of visitors, it IS clearly the driving force. But for locals, I don\u2019t think it has an overwhelming effect. Locals know their restaurants and they visit them based on their own rating. They could explore total strange and new ones, but then they will form their own rating and memory immediately and will not get fooled/guided by algorithm (the next time) Google maps is doing the same thing to local business success that social media algorithms are doing to political success. The algorithm controls what you perceive as the consensus of others. It is a dangerous world to have such power so highly concentrated. At least in central London, the \"underrated gems\" feature does not seem to be very good at finding underrated gems. That might just be a feature of the area though. I have gotten so sick of Google Maps that I've done the unthinkable, and have started walking around the city trying establishments at random. It has yielded quite good results basically immediately. People (myself included) have gotten too used to living In The Box. Putting aside the time to just go", "negative": "Russia using Interpol's wanted list to target critics abroad, leak reveals. > Pestrikov found he was named in a red diffusion after he fled Russia in June 2022 It doesn't say how he found out, I would imagine he's regularly checking online, he was stopped at a control check somewhere? Seems to me that most people wouldn't have a clue until they're being arrested. But again another scummy behaviour from the Russian government. It might as well just be prudent to ignore their requests altogether. Boy who cried wolf. Edit : it did indeed say how. I missed it. > After he fled to France, he was worried that the Kremlin might try to target him there, so he contacted Interpol I\u2019ll just assume this is correct because I believe the Russian government has mastered the art of just lying when there are no consequences, but if I was being critical, this phrase is giving me pause for evaluating the conclusions. > The data is not complete\u2026 Currently in my country (Austria) there is a court process against an official who made register look-ups of critical journalists who live here and handed the address to FSB-Agents who later broke into this journalists apartment. The ruzzians are completely unscrupulous.  https://www.reuters.com/world/austrian-ex-intelligence-accus...  Not denying that Russia abuses Interpol, but I have doubts about this particular narrative that he was some kind of \"government critic.\" From what I can find, he privatized a state corporation in the 90s for pennies (lots of very shady deals back then, usually facilitated by organized crime). From 2010-2020, I can find media reports about his legal problems with tax evasion. In 2021, there was a case where he threatened people with murder while holding a rifle. He was perfectly fine living in Putin's Russia until 2022, when he took 250 mln from the company's budget without consulting the board of directors and left Russia (and prosecutors also found that the privatization in the 90s was illegal). I suspect he's pa"}
{"anchor": "Making RSS More Fun. >  I want to sit somewhere and passively consume random small creators content, then upvote some of that content and the service should show that more often to other users. That's it. No advertising, no collecting tons of user data about me, just a very simple \"I have 15 minutes to kill before the next meeting, show me some random stuff.\" In other words consume things for free and don\u2019t support the small content creators work. Sounds very similar to what the AI companies are doing, consuming RSS feeds and not paying it back to the small creators, but when we are doing it, it is okay because  we  are not AI companies. hmmm. My YOShInOn reader basically looks like this.  It takes a few 1000 up/down judgements to make good content-based recs [1],  a reader that does collaborative filtering probably learns faster. [1] train a BERT+SVM classifer to predict my judgements,  create 20 k-Means clusters to get some diversity,  take the top N from each cluster,  blend in a certain fraction of randoms to keep it honest. The clusters are unsupervised and identify big interest areas such as programming, sports, climate change, advanced manufacturing, anime, without putting labels on the clusters -- the clusters do change from run to run but so what.  If I really wanted a stable classification I would probably start with clusters,  give them names,  merge/split a little,  and make a training set to supervised classifier to those classes. The way I have made RSS more fun is by adding local LLM functionalities[0] and push notifications. (that can notify me when something I expect to happen, happens) [0]  https://github.com/piqoni/matcha  > I rarely want to read all of a websites content from beginning to end I get the impression this person is using RSS reader wrong. Or is there really a culture of people you are using RSS like a youtube-channel, consuming everything from beginning to end? For me the purpose of RSS is to get the newest headlines, choose the inte", "positive": "High air pollution could diminish exercise benefits by half \u2013 study. It sounds terrible . What will happend in the future?! The research doesn't differentiate between seasons , and every one knows how polluted the air is in the winter when everyone is heating their home and apartament. I look at the PM2.5 data for my city every day, and at this point (Nov) in the winter season, the only acceptable time to exercise is between 2PM-4PM after vertical mixing kicked in. Outside that duration, particulates are elevated after morning rush our, after evening rush hour, or during overnight inversion trapping evening rush hour + wood burning smoke until the next morning rush hour. This is one the main reasons why I would prefer working remote, it is hard to utilize this time well (for exercise) if you are in the office. At least with PM you can wear a mask, although I am still searching for the best one that works during intense exercise. Also wanted to point out\n\"Trump EPA moves to abandon rule that sets tough standards for deadly soot pollution\"  https://apnews.com/article/epa-soot-air-pollution-trump-zeld...  If only you could see it. In the big cities the air quality has improved, however, I am not sure if it really has, or if we are now just burning hydrocarbons more efficiently so that the particle sizes have become invisible. Put it this way, although cars are allegedly better than they were, fuel consumption hasn't dropped considerably. The cars are more numerous than ever, and, although there are EVs, there are still more ICE cars than there were in the good old days when petrol came with lead in it. I am not sure that most people in urban areas even know what good air tastes and smells like. I take a canal path through lush countryside, far from any cars for most of the way. This canal has an aqueduct (or is it a viaduct?) over a motorway and the contrast is incredible. You go from basically smelling flowers to air pollution and back to clean air again quite quickly", "negative": "Hacker News: Savage Edition.  prompt: in the vein of our classic Gemiini Pro 3 hallucinates the HN front page 10 years from now, or HN front page right now, but the titles are honest. Please scrape the HN front page RIGHT NOW and make honest titles. Here's a preview: <... snip ... >  This is actually a 2-shot. I asked Gemini Pro 3 to turn it up to 11. If you want the less savage, more anodyne 1st-version...I posted that too. Great work, this is funny! I saw a way too soon joke in this vein of savage humor earlier today... \"What do you think of the work ICE is doing in Minnesota?\" Pretti Good. This made me click on several HN posts I totally wouldn't have otherwise. I gotta say, it nails the titles! Great work! As a regular poster for 14 years this is really great and nails the vibe. The \u201cfights\u201d for threads is chefkiss Also \u201cPostgres cult celebrates death of another vector database\u201d was so spot on I looked for the meta post name but looks like it hasn\u2019t updated yet. I\u2019ll be interested to see if there\u2019s a recursion that turns into the singularity An internet posts their own, LLM-generated and therefore far less funny, take on n-gate.com, long after n-gate is dead. Hackernews waxes nostalgic about a site deemed not to promote the kind of discussion they want to see when it was still alive. These HN spoofs are quickly becoming the lowest of form of content slop that people know will get a ton of karma. This has to be  at least  the fifth or sixth in a month or two? C'mon keepamovin. I know you know how easy it is for this type of stuff to get karma. Truly savage. Well done. Lots of laughs.\nWay funnier than my contribution, which is probably why mine hasn't picked up any traction yet.\nWould be interesting to read a brief once your Story hits 50+ comments.  https://news.ycombinator.com/item?id=46765448  ROFL: > Sk\u00e5pa, a parametric 3D printing app like an IKEA manual > Show HN: Hybrid Markdown Editing > Palantir Defends Work with ICE to Staff Following Killing of Alex Pre"}
{"anchor": "Presence in Death. The research mentioned in the article (which indicates no EEG activity):  https://pmc.ncbi.nlm.nih.gov/articles/PMC7876463/  If TRUE and consciousness is eternal (using whatever framework you like), the idea of death becomes double sided. On one hand, physical death is sad as it's the end of this physical 'epic story' before our consciousness moves onto a new body/story. On the flip side, approaching body death is a sort of a temporary great relief as we are immortal and cannot actually die. I.e. After ten thousand years of being alive, a vampire looks forward to sleeping in their coffin at night. There are stories about bodies of Christian monks that did not decompose for a log time after the dearth. Modern take on it attributes it to the climate in caves where the body was put after the dearth. But another important part was diet. Often the well-preserved bodies were of those who had eaten only rough bread and water for months and years before the dearth. So I suspect both of the factors are at the play here as well. Here's the documentary referred to:  https://www.imdb.com/title/tt21945758/  It's rentable at the usual places. I might check it out. I have been studying and practicing tibetan buddhism for a little over a year now, particularly dream yoga, but branched into some of the other practices. I'm always a skeptic but it is fascinating some of the stuff they can do. There is scientific evidence they can raise and lower their body temperatures through meditation, withstanding great heat/cold and deprivation conditions. I've played around with deprivation and what it has done for my mental health and body has surprised me. I'm but a novice, but I absolutely believe they are tapping into something scientific about the body/mind that is still unknown. There are reasons to be extremely skeptical about some of their claims, but, some of it is very interesting and credible. Some of these esoteric states are really weird, and it seems crazy to me", "positive": "Scaling up test-time compute with latent reasoning: A recurrent depth approach. Twitter thread about this by the author:  https://x.com/jonasgeiping/status/1888985929727037514  Interesting stuff. As the authors note, using latent reasoning seems to be a way to sink more compute into the  model and get better performance without increasing the model size, good news for those on a steady diet of 'scale pills' Latent / embedding-space reasoning seems a step in the right direction, but building recurrence into the model while still relying on gradient descent (i.e. BPTT) to train it seems to create more of a problem (training inefficiency) than it solves, especially since they still end up externally specifying the number of recurrent iterations (r=4, 8, etc) for a given inference. Ideally having recurrence internal to the model would allow the model itself to decide how long to iterate for before outputting anything. One of the benefits of using thinking tokens compared to \u201cthinking in a latent\u201d space is that you can directly observe the quality of the CoT. In R1 they saw it was mixing languages and fixed it with cold start data. It would be hard to SFT this because you can only SFT the final result not the latent space. I also notice the authors only had compute for a single full training run. It\u2019s impressive they saw such good results from that, but I wonder if they could get better results by incorporating recent efficiency improvements. I would personally not use this architecture because 1) it adds a lot of hyperparameters which don\u2019t have a strong theoretical grounding and 2) it\u2019s not clearly better than simpler methods. My opinion is that opaque reasoning is a prerequisite for many of the worst possible AI outcomes. We should make reasoning fully visible in the output space. Slightly off topic, I rarely see paper talks about their failed training runs, and why those runs failed. This paper is definitely a breath of fresh air. And their analyses of their failures", "negative": "Computer History Museum Launches Digital Portal to Its Collection. Link:  https://www.computerhistory.org/collections/catalog  This is realllly cool. I have a rabbit hole to go down into tonight I have come across (and enjoyed) many of the videos [1] they have posted to YouTube. [1]  https://www.youtube.com/@ComputerHistory  Very cool stuff. Vintage marketing of the future:  https://www.computerhistory.org/collections/curator-picks/vi...  Lectures:\n https://www.computerhistory.org/collections/catalog/search-c...  Oral Histories:\n https://www.computerhistory.org/collections/catalog/search-c...  Related, of the more in-person variety:  Favorite Tech Museums   https://news.ycombinator.com/item?id=46504220  I'm a fan of CHM. That said there collections have (understandably) a rather Silicon-Valley-legacy-centric view of, erm, computer history.  You'll find little mention, for example, of these tantalizing early mentions of alternative computer architectures (with pictures!) in NSA's predecessor OP-20-G, as posed alongside the then-nascent von Neumann architecture (also covered).  https://www.governmentattic.org/8docs/NSA-WasntAllMagic_2002...  CHM employee here. Always great to see CHM on HN. Glad folks are excited about this -- as are we! There's so much cool stuff in the Collection. Ooh check out the Discovery wall!  I see a Furby, a Power Glove (call AVGN) and a Ninja Turtles NES Game:   https://www.computerhistory.org/collections/discovery  This is really awesome. The CHM is one of my favorite places in the world. I had applied for a web developer position there not too long ago, great to see them expand things online like this This one has always been a favorite:  https://computerhistory.org/blog/the-two-napkin-protocol/  This place is great, but my work had a function here and I walked around with one of our juniors and never have I felt so old. The pure astonishment and confusion when looking at a \u201cfloppy disk\u201d aged me instantly. I've been to this museum ~10 time"}
{"anchor": "Toronto\u2019s network of pedestrian tunnels. Once on a lunch break I walked from St Andrew to King (parallel stations on the horns of line 1) in the tunnels and took the TTC back. Going overground is usually faster and easier to navigate, buts impressive how far you can go underground. One of these days I\u2019ll need to try an extreme point hike. One thing that I was surprised wasn't mentioned is the impact that I believe weather must have had on the development of the Path. Winters in Toronto get rather cold and snowy. Even with a dense downtown core, walking a few blocks outside can be rather unpleasant. I use to love exploring Path as a teenager. More northern cities like Montreal and Winnipeg also have very interesting indoor pedestrian systems. The one in Winnipeg is particularly useful, since there are approximately 72 hours per year that it's comfortable to be outside between the bone-chilling cold and the biblical swarms of mosquitos and flies in the summer.  https://en.wikipedia.org/wiki/Underground_City,_Montreal   https://en.wikipedia.org/wiki/Winnipeg_Walkway  We also have a 5K race in the PATH! [1] In the winter the tunnels are amazing for commute. [1]  https://www.bougebouge.com/en/shop/events/5km-bougebouge-tor...  >  Montreal has a similar system, while Tokyo, Osaka, Seoul, Hong Kong, Singapore and Houston have systems that resemble the Path in some respects. A few European cities also make considerable use of pedestrian tunnels, including Helsinki, Stockholm and Munich.  Japan's northernmost major city, Sapporo, has a very extensive one -- of those I've seen, it's the one that's most comparable to Toronto's. The other Japanese tunnel/undercity complexes are mostly subterranean malls around subway stations.  (This also applies to all of the ones in Hong Kong.)  But Sapporo's is seriously huge. I think the common denominator is that people would rather walk in a heated underground space when it gets cold. It's my understanding that underground walkways were c", "positive": "GPT-5.2-Codex. would love to see some comparison numbers to Gemini and Claude, especially with this claim: \"The most advanced agentic coding model for professional software engineers\" I actually have 0 enthusiasm for this model. When GPT 5 came out it was clearly the best model, but since Opus 4.5, GPT5.x just feels so slow. So, I am going to skip all `thinking` releases from OpenAI and check them again only if they come up with something that does not rely so much on thinking. I hope this makes a big jump forward for them. I used to be a heavy Codex user, but it has just been so much worse than Claude Code both in UX and in actual results that I've completely given up on it. Anthropic needs a real competitor to keep them motivated and they just don't have one right now, so I'd really like to see OpenAI get back in the game. > In parallel, we\u2019re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety. Yeah, this makes sense. There's a fine line between good enough to do security research and good enough to be a prompt kiddie on steroids. At the same time, aligning the models for \"safety\" would probably make them worse overall, especially when dealing with security questions (i.e. analyse this code snippet and provide security feedback / improvements). At the end of the day, after some KYC I see no reason why they shouldn't be \"in the clear\". They get all the positive news (i.e. our gpt666-pro-ultra-krypto-sec found a CVE in openBSD stable release), while not being exposed to tabloid style titles like \"a 3 year old asked chatgpt to turn on the lights and chatgpt hacked into nasa, news at 5\"... Can anyone elaborate on what they're referring to here? >  GPT\u20115.2-Codex has stronger cybersecurity capabilities than any model we\u2019ve released so far. These advances can help streng", "negative": "Rust\u2019s Standard Library on the GPU. I feel like the title is a bit misleading. I think it should be something like \"Using Rust's Standard Library from the GPU\". The stdlib code doesn't execute on the GPU, it is just a remote function call, executed on the CPU, and then the response is returned. Very neat, but not the same as executing on the GPU itself as the title implies. How different is it from rust-gpu effort? UPDATE: Oh, that's a post from maintainers or rust-gpu. Can I execute FizzBuzz and DOOM on GPU? Are there any details around how the round-trip and exchange of data (CPU<->GPU) is implemented in order to not be a big (partially-hidden) performance hit? e.g. this code seems like it would entirely run on the CPU?       print!(\"Enter your name: \");\n    let _ = std::io::stdout().flush();\n    let mut name = String::new();\n    std::io::stdin().read_line(&mut name).unwrap();\n  \nBut what if we concatenated a number to the string that was calculated on the GPU or if we take a number:       print!(\"Enter a number: \");\n    [...] // string number has to be converted to a float and sent to the GPU\n    // Some calculations with that number performed on the GPU\n    print!(\"The result is: \" + &the_result.to_string()); // Number needs to be sent back to the CPU\n\n  \nOr maybe I am misunderstanding how this is supposed to work? I'm confused about this: As the article outlines well, Std Rust (over core) buys you GPOS-provided things. For example:     - file system\n  - network interfaces\n  - dates/times\n  - Threads, e.g. for splitting across CPU cores\n  \nThe main relevant one I can think which applies is an allocator. I do a lot of GPU work with rust: Graphics in WGPU, and Cuda kernels + cuFFT mediated by Cudarc (A thin FFI lib). I guess, running Std lib on GPU isn't something I understand. What would be cool is the dream that's been building for decades about parallel computing abstractions where you write what looks like normal single-threaded CPU code, but it automagically "}
{"anchor": "Siddhartha. Huh, funny this should pop up here. I recently started commuting by subway into work, so I had to pick up a subway book. I had been meaning to read this, so I went to my local book store and grabbed a copy. It\u2019s a really great book. Such a fascinating story. And short, too. I highly recommend giving it a read. It might synthesize some of your loose connections about Hinduism, Buddhism, and your own place in a chaotic world and what it means to live a happy life. great book.  I recommend people read it every 10 years or so as your perspective on life changes. I'm very grateful that this was assigned reading in high school, since it was a sort of gateway book for reading more about Buddhism. It's short. If you haven't read it, I highly recommend it. One of the greatest authors of all time. Hesse taps into the mind of the modern human and beautifully presents its inner workings. Each of his books takes a different angle, a different perspective or philosophy with which to observe the evolving personhood. I read this in high school, but not because it was assigned.  At the time I was really into \"rare\" Queen MP3s, and there's a studio recording of the fast version of \"We Will Rock You\" where Brian May reads a passage from this book before the music starts.  An odd way to be inspired to read a book, but I still think I got a fair bit out of it. Off topic: Im curious what\u2019s the most prominent religion among HNers? Is it different from the normal population? Buddhism seems to be number 1 after atheism which isn\u2019t a religion. I liked this quite a bit the first time I'd read it. A decade later, not as much. Narcissus and Goldmund is my favorite book by Hesse - it's beautifully crafted. I read this annually, typically in a day, usually when I'm feeling lost. For me it distills the human experience into a simply story that helps me find meaning for where I am in my own journey. Love this book. I have three sons and read this when them when they're about 12 or 13. I", "positive": "27M Fewer Car Trips: Life After a Year of Congestion Pricing. non-paywall link:  https://www.nytimes.com/interactive/2026/01/05/upshot/conges...  Sounds like a good reason to not invest in parking garages. I never understood why big, congested cities in the USA (NYC, Boston, L.A., D.C., Chicago) aren't dumping money into funding FSD research hand over fist. It's not a moonshot anymore, and it would be the game-changer of the century in terms of public transportation and GDP. This is a long-term no-brainer for prosperity. The first graph makes no sense? Why are the initial actual and expected values so far apart? Shouldn't they start at the same point? That's valuable real estate for housing. House people, not cars. What's the theory of change here? FSD fundamentally is going to make keeping a car moving on the road cheaper, and making something cheaper makes it happen more. In what world do you get that conclusion? Dense cities in other parts of the world rely on mass transit to move people. FSD so you can have self driving cars in the street -> ? -> increasing congestion. The point is you should have more effective volume transit not optimizing random ones.  1000 cars on FSD are an optimization better than 1000 taxi drivers, compared to a train or a few buses.  https://www.youtube.com/watch?v=040ejWnFkj0  They don't describe the graphic very well in the article, but they do link to the source data [1]. The \"Expected\" line seems to refer to a historical average. Since the starting point of the graph coincides with the beginning of congestion pricing, we would expect a difference between the two values at that point. [1]  https://metrics.mta.info/?cbdtp/vehiclereductions  FSD could largely eliminate privately owned vehicles, it could also allow cities to get rid of most parking infrastructure. It eliminates traffic, parking, and alll the other pain points of owning a vehicle just to get from A to B when a car is the only true option. Certainly, FSD buses would be a w", "negative": "Exactitude in Science \u2013 Borges (1946) [pdf]. I can't get enough of Borges. His way with words and way to highlight to absurdity of situations is first class. My favorite is the Celestial Emporium of Benevolent Knowledge. It's a critique of the classification used by the Institute of Bibliography which he considered nonsensical. He claims to have found the list in an ancient Chinese encyclopaedia: - those belonging to the Emperor - embalmed ones - trained ones - suckling pigs - mermaids - fabled ones - stray dogs - those included in this classification - those that tremble as if they were mad - innumerable ones - those drawn with a very fine camel hair brush - et cetera - those that have just broken the vase - those that from afar look like flies The most avid members of the Cartographers Guilds had even proposed a Map of the Empire several times larger than the Empire itself to depict microscopic details that would otherwise be invisible. Such proposals were considered the peak of academic excess after the Study of Cartography fell out of favor. \"I have a map of the United States... Actual size. It says, 'Scale: 1 mile = 1 mile.' I spent last summer folding it. People ask me where I live, and I say, 'E6.\" Steven Wright Though it's not as funny without his delivery Ficciones is full of mockings of intellectualism. I Particularly like the critique on the critical philosophical work of Menard's Quixote. Where Menard, the subject of the story, carefully writes parts of a novel that is word-for-word a copy of Cervante's Quixote, but shaped by Menard's intellectual efforts, one is to draw the opposite appreciations than from the one written by Cervantes. His stories are such a strange read. The plot, the characters, the mentions, all feel almost secondary to the feeling they evoke. \"The House of Asterion\" is the most beautifully written thing I have ever read.  https://klasrum.weebly.com/uploads/9/0/9/1/9091667/the_house...  I think it's a message about how science is rea"}
{"anchor": "Review of Anti-Aging Drugs. From the conclusion paragraph: > Your primary life extension program is diet and exercise. Choose a diet that works for you. Stay slim. Considering heart disease is the #1 killer, doing whatever you can to not die from heart disease is the best place for most people to start. Even in 2025, diet and exercise are still king. Winner, \"Ascorbic\". Do they mean Vitamin C? Would any of the OTC stuff even be effective? Melatonin, NAC, and Berberine. Be careful when reading such blogs: > Note that the dosage in the mouse experiments is quite high \u2014 0.1% of the body weight every day, meaning about 2 ounces a day for me (70 kg). Mouse and human metabolism are very different. A better starting estimate would be 5g/day, not 57g/day. I hope people dont accidentally overdose themselves because of lack of a pharmacology background. A lot of people in the comments are talking about the \"problem\" of death and approaches to take, but really, the only thing you can do is philosophically make your peace. Anything else at this point is yelling into infinity. > Fast for short intervals regularly, and longer fasts as they feel good to you. You can effectively do this every day if you just eat once per day. When I was properly obese, this technique resulted in rapid weight loss. Zero exercise was required to see results, which was good at the time because the not eating part was about all I could handle. Being in a fasted state is as close as you can get to actually  reversing  aging. Your body engages in a process called autophagy when nutrient-sensing pathways are down-regulated. When you are stuffing your face constantly (i.e., every ~8 hours), there is less opportunity for this mechanism to do its job.  https://en.wikipedia.org/wiki/Autophagy  Richard Miller's Intervention Testing Program should really be your go-to for this:  https://www.nia.nih.gov/research/dab/interventions-testing-p...  He has no conflicts of interests, works for the NIA, and he's quite o", "positive": "Karpathy on Programming: \u201cI've never felt this much behind\u201d. For the longest time, the joy of creation in programming came from solving hard problems. The pursuit of a challenge meant something. Now, that pursuit seems to be short-circuited by an animated being racing ahead under a different set of incentives. I see a tsunami at the beach, and I\u2019m not sure whether I can run fast enough. Being a nondeterministic tool, the output for a given input can vary. Rather than having a solid plan of, \"if I provide this input, then that will happen\", it's more like, \"if I do something like this, I can expect something like that, probably, and if not, then try again until it works, I suppose\". What are the productivity gains? Obviously, it must vary. The quality of the tool output varies based on numerous criteria, including what programming language is being used and what problem is trying to be solved. The fact that person A gets a 10x productivity increase on their project  does not  mean that person B will also get a 10x productivity increase on their project, no matter how well they use the tool. But again, tool usage itself is variable. Person A themselves might get a 10x boost one time, and 8x another time, and 4x another time, and 2x another time. Man, this is giving me a cognitive dissonance compared to my experiences. Actually, even the post itself reads like a cognitive dissonance with a dash of the usual \"if it's not working for you then you are using it wrong\" defence. > There's a new programmable layer of abstraction to master (in addition to the usual layers below) involving agents, subagents, their prompts, contexts, memory, modes, permissions, tools, plugins, skills, hooks, MCP, LSP, slash commands, workflows, IDE integrations, and a need to build an all-encompassing mental model for strengths and pitfalls of fundamentally stochastic, fallible, unintelligible and changing entities suddenly intermingled with what used to be good old fashioned engineering. Slop-o", "negative": "AI on Australian travel company website sent tourists to nonexistent hot springs. Weldborough seems to have done well out of it either way. New variant on \"I followed my satnav blindly and now I'm stuck in the river\", except less reliable. It is however fraud on the part of the travel company to advertise something that doesn't exist. Another form of externalized cost of AI. Australia has drop bears anyhow. Do they exist? Seems par for course. >Scott Hennessey, the owner of the New South Wales-based Australian Tours and Cruises, which operates Tasmania Tours, told the Australian Broadcasting Network (ABC) earlier this month that \u201cour AI has messed up completely.\u201d To me this is the real takeaway for a lot of these uses of AI. You can put in practically zero effort and get a product. Then, when that product flops or even actively screws over your customers, just blame the AI! No one is admitting it but AI is one of the easiest ways to shift blame. Companies have been doing this ever since they went digital. Ever heard of \"a glitch in the system\"? Well, now with AI you can have as many of those as you want, STILL never accept responsibility, and if you look to your left and right, everyone is doing it, and no one is paying the price. How often do you have to update your page on \"what's in a town\" to \"compete with the big boys\"? Seems like you could just google what's in the town, or visit if you really want to make sure, rather than just asking your favourite LLM \"What's there to do in Weldborough\"? \u201cour AI has messed up completely.\u201d No, it worked as designed. Generative AI simply creates content of the type that you specify, but has no concept of truth or facts. has anyone checked to see if the AI included time co ordinates as well?\nit  might be that AI is missunderstanding our tempotral limitations, and if prompted correctly will provide a handy portal to when, there will in fact be hot springs at the location suggested. In case anyone else is curious, I just entered"}
{"anchor": "AI will make formal verification go mainstream. I'm convinced now that the key to getting useful results out of coding agents (Claude Code, Codex CLI etc) is having good mechanisms in place to help those agents exercise and validate the code they are writing. At the most basic level this means making sure they can run commands to execute the code - easiest with languages like Python, with HTML+JavaScript you need to remind them that Playwright exists and they should use it. The next step up from that is a good automated test suite. Then we get into quality of code/life improvement tools - automatic code formatters, linters, fuzzing tools etc. Debuggers are good too. These tend to be less coding-agent friendly due to them often having directly interactive interfaces, but agents can increasingly use them - and there are other options that are a better fit as well. I'd put formal verification tools like the ones mentioned by Martin on this spectrum too. They're potentially a fantastic unlock for agents - they're effectively just niche programming languages, and models are really good at even niche languages these days. If you're not finding any value in coding agents but you've also not invested in execution and automated testing environment features, that's probably why. Unless you feed a spec to the LLM, and it nitpicks compiled TLA+ output generated by your PlusCal input, gaslights you into saying the code you just ran and pasted the output of is invalid, then generates invalid TLA+ output in response. Which is exactly what happened when I tried coding with Gemini via formal verification. I love HN because HN comments have talked about this a fair bit already. I think on the recent Erdos problem submission. I like the idea that languages even like Rust and Haskell may be more accessible. Learn them of course but LLM can steer you out of getting stuck. If AI is good enough to write formal verification, why wouldn't it be good enough to do QA? Why not just have AI do ", "positive": "Peerweb: Decentralized website hosting via WebTorrent. I don't get it, I upload my files to your site, then I send my friends links to your site? How is this not a single point of failure? Github:  https://github.com/omodaka9375/peerweb  This is pretty interesting! I think serving video is a particularly interesting use of Webtorrent. I think it would be good if you could add this as a front end to basically make sites DDOS proof. So you host like a regular site, but with a JS front end that hosts the site P2P the more traffic there is. Fun! I wish WebTorrent had caught on more. I've always thought it had a worthy place in the modern P2P conversation. In 2020, I messed around with a PoC for what hosting and distributing Linux distros could look like using WebTorrent[1]. The protocol project as a whole has a lovely and brilliant design but has stayed mostly stagnant in recent years. There are only a couple of WebRTC-enabled torrent trackers that have remained active and stable. 1.  https://github.com/leoherzog/LinuxExchange  In its own reimagined way from what\u2019s possible in 2026, this could kick off a new kind of geocities. This is cool - I actually worked on something similar way back in the day:  https://github.com/tom-james-watson/wtp-ext . It avoided the need to have any kind of intermediary website entirely. The cool thing was it worked at the browser level using experimental libdweb support, though that has unfortunately since been abandoned. You could literally load URLs like wtp://tomjwatson.com/blog directly in your browser. I think one of the values of (what appears to be) AI generated projects like this is that they can make me aware of the underlying technology that I might not have heard about - for example WebTorrent:  https://webtorrent.io/faq  Pretty cool! Not sure what this offers over WebTorrent itself, but I was happy to learn about its existence. Nice, I clicked on the first demo, and I got stuck at connecting with peers. I like the idea though. N", "negative": "When two years of academic work vanished with a single click. A plant science academic who can't be bothered to back up their work... If that was the intellectual calibre of the person, I wonder how truly worthwhile the lost work was. The 2nd comma he uses is incorrect. Did he also use ChatGPT for this article? I frown when people currently trust AI, let alone have been doing so for 2 years already. This is exactly why I don't rely on the web UI for anything critical. It seems like a mistake to treat a chat log as a durable filesystem. I just hit the API and store the request/response pairs in a local Postgres DB. It's a bit of extra boilerplate to manage the context, but at least I own the data and can back it up properly. once upon a time i had a boss who asked for a \"super admin\" account to \"trump\" the domain administrators..and a \"master key\" to decrypt any file , in case the user lost their key. Is it deleted though? Last I heard there was a court case or some such that required them retaining all data for a lawsuit, did that go away? Never rely on any subscription based service for any data that is important. Never use data formats that lock you in. Especially not online services without (automatic) export options. Keep a copy (cloud) and a backup (offline) for all you own data. relevant:  https://www.youtube.com/watch?v=7pqF90rstZQ  This issue looks like a situation where one person stored all their files and folders in the Windows Recycle Bin and somebody emptied it. It might be my professional deformation, but I never store anything in ChatGPT and Claude for longer than a day or two. A typical example of Hyrum's Law:  ...all observable behaviors of your system\nwill be depended on by somebody . It's like how your draft folder feature will be used as a secret messaging app by a general and his mistress, or as Don Norman points out, your flat topped parapet will be used as a table for used cups, or your reliable data store of chats will be used as academic res"}
{"anchor": "Peerweb: Decentralized website hosting via WebTorrent. I don't get it, I upload my files to your site, then I send my friends links to your site? How is this not a single point of failure? Github:  https://github.com/omodaka9375/peerweb  This is pretty interesting! I think serving video is a particularly interesting use of Webtorrent. I think it would be good if you could add this as a front end to basically make sites DDOS proof. So you host like a regular site, but with a JS front end that hosts the site P2P the more traffic there is. Fun! I wish WebTorrent had caught on more. I've always thought it had a worthy place in the modern P2P conversation. In 2020, I messed around with a PoC for what hosting and distributing Linux distros could look like using WebTorrent[1]. The protocol project as a whole has a lovely and brilliant design but has stayed mostly stagnant in recent years. There are only a couple of WebRTC-enabled torrent trackers that have remained active and stable. 1.  https://github.com/leoherzog/LinuxExchange  In its own reimagined way from what\u2019s possible in 2026, this could kick off a new kind of geocities. This is cool - I actually worked on something similar way back in the day:  https://github.com/tom-james-watson/wtp-ext . It avoided the need to have any kind of intermediary website entirely. The cool thing was it worked at the browser level using experimental libdweb support, though that has unfortunately since been abandoned. You could literally load URLs like wtp://tomjwatson.com/blog directly in your browser. I think one of the values of (what appears to be) AI generated projects like this is that they can make me aware of the underlying technology that I might not have heard about - for example WebTorrent:  https://webtorrent.io/faq  Pretty cool! Not sure what this offers over WebTorrent itself, but I was happy to learn about its existence. Nice, I clicked on the first demo, and I got stuck at connecting with peers. I like the idea though. N", "positive": "Gemini Diffusion. Interesting to see if GROQ hardware can run this diffusion architecture..it will be  two time magnitude of currently known speed :O That's...ridiculously fast. I still feel like the best uses of models we've seen to date is for brand new code and quick prototyping. I'm less convinced of the strength of their capabilities for improving on large preexisting content over which someone has repeatedly iterated. Part of that is because, by definition, models cannot know what is  not  in a codebase and there is meaningful signal in that negative space. Encoding what  isn't  there seems like a hard problem, so even as models get smarter, they will continue to be handicapped by that lack of institutional knowledge, so to speak. Imagine giving a large codebase to an incredibly talented developer and asking them to zero-shot a particular problem in one go, with only moments to read it and no opportunity to ask questions. More often than not, a less talented developer who is very familiar with that codebase will be able to add more value with the same amount of effort when tackling that same problem. I think the lede is being buried. This is a great and fast InstructGPT. This is absolutely going to be used in spell checks, codemods, and code editors. Instant edits feature can surgically perform text edits fast without all the extra fluff or unsolicited enhancements. I copied shadertoys, asked it to rename all variables to be more descriptive and pasted the result to see it still working. I'm impressed. Diffusion is more than just speed. Early benchmarks show it better at reasoning and planning pound for pound compared to AR. This is because it can edit and doesn\u2019t suffer from early token bias. Nit: Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like Mercury [1] still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I ", "negative": "A list of fun destinations for telnet. uff I hope i can list my MUD game (still in dev, though) Oh man RIP towel.blinkenlights.nl 23 The Star Wars ASCII animation was how I learned telnet existed. Felt like discovering a secret passage in the internet. There's something pure about text-based interfaces. No loading spinners, no JavaScript frameworks, no cookie banners. Just text. nethack.alt.org is conspicuously absent... This is insane > doom.w-graj.net 666 > Play Doom in the terminal (code and details) Very cool, some nice nostalgia looking through that list! Missed a trick not being able to \u201ctelnet telnet.org\u201d though. :-) Related to the last Telnet CVE?\nWhy talking about telnet now otherwise? for years I had this in my .muttrc. it's been commented out since it stopped working... #set signature=\"cat ~/.signature && telnet towel.blinkenlights.nl 666 | tail -n3|\" I was wondering why the Starwars one is not at the top of the list. Then I saw it no longer exists :-( Wasted opportunity for a telnet.net or tel.net domain. I can forsee a future when all the AI slop, popups, fake news, propaganda and ads have fully consumed the web. Maybe then we just go back to an oldschool text based way of communicating. No google. No socials. Just text.        ~/work/...> telnet towel.blinkenlights.nl\n    zsh: command not found: telnet   Note that this is much more dangerous than visiting a website. ANSI escape sequences can seriously mess with your system, RCE included. For those of you curious about what the Star Wars one looked like, the tradition lives on here: ssh -p 1977 sw.taigrr.com My first introduction to the internet was through the telnet-based EW-too talkers like Foothills (Boston U) and Forest (UTS). I have very fond memories of staying up late talking to people from all over the globe. It was truly amazing to me. The best part was how the users moderated behaviour - bad actors were ejected swiftly but rarely permanently. The first BBS I used  in the 80's  eventually ende"}
{"anchor": "Anthropic blocks third-party use of Claude Code subscriptions. This appears to be a part of a crackdown on third-party clients using Claude Code's credentials/subscriptions but not through Claude Code. Not surprising as this type of credential reuse is always a gray area, but weird Anthropic deployed it on a Thursday night without any warning as the inevitable shitstorm would be very predictable. No. Do you realize how much of a joke Claude code is? Under the hood. How they implemented client auth? Well let me tell you  https://github.com/anomalyco/opencode/blob/dev/packages/open...  You literally send your first message \u201cyou are Claude code\u201d The fact that this ever worked was insane. Headline is more like anthropic vibes a bug and finally catches it. I\u2019m not surprised they closed the loophole, it always felt a little hacky using an Anthropic monthly sub as an API with a spoofed prompt (\u201cYou are Claude Code, Anthropic's official CLI for Claude\u201d) with OpenCode. Google will probably close off their Antigravity models to 3P tools as well. Meanwhile, OpenAI co-signs  https://github.com/steipete/oracle  which lets you use your ChatGPT subscription to gain programmatic/agentic access to 5.2 Pro via automating browser access to the web frontend. Karpathy and other leaders have praised this feature on X. If that is indeed so welcome, imagine what else you could script via their website to get around Codex rate limits or other such things. After all what coud be so different about this than what browsers like Atlas do already Anthro is having its Apple moment: too many customers means the company is always on the news, for better or worse. When iPhones receive negative reviews it's not like only Apple screwed up; others did too, but they sell so much less than Apple that no one hears about them:       \"Apple violated my privacy a tiny bit\" makes the news;\n    \"Xiaomi sold my fingerprint info to 3rd party vendors\" doesn't.\n\n  \nSimilarly, Anthropic is under heavy fire recently", "positive": "Ask HN: How are Markov chains so different from tiny LLMs?. LLMs include mechanisms (notably, attention) that allow longer-distance correlations than you could get with a similarly-sized Markov chain.  If you squint hard enough though, they are Markov chains with this \"one weird trick\" that makes them much more effective for their size. A Markov Chain trained by only a single article of text will very likely just regurgitate entire sentences straight from the source material.  There just isn't enough variation in sentences. But then, Markov Chains fall apart when the source material is very large.  Try training a chain based on Wikipedia.  You'll find that the resulting output becomes incoherent garbage.  Increasing the context length may increase coherence, but at the cost of turning into just simple regurgitation. In addition to the \"attention\" mechanism that another commenter mentioned, it's important to note that Markov Chains are  discrete  in their next token prediction while an LLM is more fuzzy.  LLMs have latent space where the meaning of a word basically exists as a vector.  LLMs will generate token sequences that didn't exist in the source material, whereas Markov Chains will ONLY generate sequences that existed in the source. This is why it's impossible to create a digital assistant, or really anything useful, via Markov Chain.  The fact that they only generate sequences that existed in the source mean that it will never come up with anything creative. Your example is too sparse to make a conclusion from I\u2019d offer an alternative interpretation: LLMs follow the Markov Decison modeling properties to encode the problem but use a very efficient policy for solver for the specific token based action space. That is to say they are both within the concept of a \u201cmarkovian problem\u201d but have wildly different path solvers. MCMC is a solver for an MDP, as is an attention network So same same, but different Would you be willing to write an article comparing the result", "negative": "Xmake: A cross-platform build utility based on Lua. I think a syntax example on the homepage would be a good idea. Also comparison charts for things like cmake, ninja, meson, and bazel. If you have a dependency finding strategy, highlight the pros and cons of that. Basically the only reason states for why I should use this is lua, and that\u2019s not inherently compelling to me for build tooling. At my work we use MSBuild and vcpkg. What would a transition from that to XMake be like? My work uses this and it's slooooooow. Would not recommend. Just yesterday someone was telling me xmake does a lot of what bazel can do (hermetic, deterministic, optionally remote builds) while being easier to use. I took a look at the docs later and couldn\u2019t find a direct comparison. But there does seem to be a remote build system. And there were a few mentions of sandboxing. Can anyone provide a head to head comparison? Does xmake strictly enforce declared dependencies? Do actions run in their own sandboxes? Can you define a target whose dependency tree is multi language, multi toolchain, multi target platform and which is built across multiple remote execution servers? I apologise for this: What's wrong with premake which is also Lua based? when I meant: What advantage does this have over premake which is also Lua based? Can anyone explain xmake in terms of Build Systems a la Carte? A teammate evaluated this and the experience was night and day compared to cmake + vcpkg. However, there wasn\u2019t a lot of motivation to cutover our existing large project over because of the unknown unknowns. I think projects like these looking to dethrone the status quo definitely need some case studies or examples of larger projects using it to increase confidence because I\u2019d much rather use xmake over cmake if it can get the job done I am deeply distressed that this doesn't require Xlib. So, digging through the website git repo a bit, some basic facts: * The project started almost 11 years ago, in 2015. * It"}
{"anchor": "Qwen3-TTS family is now open sourced: Voice design, clone, and generation. great news, this looks great!  is it just me, or do most of the english audio samples sound like anime voices? I still don't know anyone who managed Qwen3-Omni to work properly on a local machine. Qwen team, please please please, release something to outperform and surpass the coding abilities of Opus 4.5. Although I like the model, I don't like the leadership of that company and how close it is, how divisive they're in terms of politics. How does the cloning compare to pocket TTS? it isn't often that tehcnology gives me chills, but this did it. I've used \"AI\" TTS tools since 2018 or so, and i thought the stuff from two years ago was about the best we were going to get. I don't know the size of these, i scrolled to the samples. I am going to get the models set up somewhere and test them out. Now, maybe the results were cherrypicked. i know everyone else who has released one of these cherrypicks which to publish. However, this is the first time i've considered it plausible to use AI TTS to remaster old radioplays and the like, where a section of audio is unintelligible but can be deduced from context, like a tape glitch where someone says \"HEY [...]LAR!\" and it's an episode of Yours Truly, Johnny Dollar... I have dozens of hours of audio of like Bob Bailey and people of that era. Kind of a noob, how would I implement this locally?\nHow do I pass it audio to process. I'm assuming its in the API spec? Huh. One of the English Voice Clone examples features Obama. If you want to try out the voice cloning yourself you can do that an this Hugging Face demo:  https://huggingface.co/spaces/Qwen/Qwen3-TTS  - switch to the \"Voice Clone\" tab, paste in some example text and use the microphone option to record yourself reading that text - then paste in other text and have it generate a version of that read using your voice. I shared a recording of audio I generated with that here:  https://simonwillison.net/", "positive": "Learning Languages with the Help of Algorithms. There are many apps that have utilized formal methods in an attempt to teach languages as optimally as possible. But Duolingo is still the leader in language learning. Why? Language learning is an emotional process. Every word you can bring to mind likely has some specific memories tied to them, from another time and place. So even though Duolingo is far from optimal in terms of how and when to present new items to learn, it is close to optimal in vibes, and apparently in the market of language learning this is what consumers prioritize over all else. I believe it is for good reason. Whoever displaces Duolingo will do so not because they teach more efficiently, but because they improve on embedding particular emotions and sentiments into the lessons. > People have many ways to learn a language, different for each person. Suppose you wanted to improve your vocabulary by reading books in that language. To get the most impact, you\u2019d like to pick books that cover as many common words in the language as possible. I think the article is just using this as a hook to introduce the submodularity of the maximum weighted cover problem. But I'll talk about a different way of using the same collection of books to learn a language that I think is better. First of all, you'll probably want to take into account which words you already know, instead of just removing stopwords. If a book uses lots of common words, but you already know them, you're not learning much. Secondly, no matter how much or how little you already know, you're unlikely to find a book that fits your level well. If you're just beginning to learn the language, no matter which book you pick, the very first sentence will be full of new words, but most of those will be rare ones that you won't encounter again until much later. If on the other hand you already have a very good command of the language, you might be able to breeze through entire chapters and only pick up a", "negative": "I built a 2x faster lexer, then discovered I/O was the real bottleneck. Zip with no compression is a nice contender for a container format that shouldn't be slept on.  It effectively reduces the I/O, while unlike TAR, allowing direct random to the files without \"extracting\" them or seeking through the entire file, this is possible even via mmap, over HTTP range queries, etc. You can still get the compression benefits by serving files with Content-Encoding: gzip or whatever.  Though it has builtin compression, you can just not use that and use external compression instead, especially over the wire. It's pretty widely used, though often dressed up as something else.  JAR files or APK files or whatever. I think the articles complaints about lacking unix access rights and metadata is a bit strange.  That seems like a feature more than a bug, as I wouldn't expect this to be something that transfers between machines.  I don't want to unpack an archive and have to scrutinize it for files with o+rxst permissions, or have their creation date be anything other than when I unpacked them. Amazing article, thanks for sharing. I really appreciate the deep investigations in response to the comments \"I/O is the bottleneck\" is only true in the loose sense that \"reading files\" is slow. Strictly speaking, the bottleneck was latency, not bandwidth. there are a loooot of languages/compilers for which the most wall-time expensive operation in compilation or loading is stat(2) searching for files Same thing applies to other system aspects: compressing the kernel loads it faster on RAM even if it still has to execute the un compressing operation. Why? Load from disk to RAM is a larger bottleneck than CPU uncompressing. Same is applied to algorithms, always find the largest bottleneck in your dependent executions and apply changes there as the rest of the pipeline waits for it.\nOften picking the right algorithm \u201csolves it\u201d but it may be something else, like waiting for IO or coordinating ac"}
{"anchor": "Ask HN: How can we solve the loneliness epidemic?. I'm also in this group, so I have a few theories as to what causes it and how to fix it. For one thing, I was severely traumatized as a kid, which delayed a lot of my social skills. I'm catching up but not all the way there yet. When my social battery is full, I can do pretty well, but if I'm even a little down, it's basically impossible to act normally. I also had it hammered into me as a kid that nobody wants me around, nobody could ever love me, I'm a failure, a burden, a creep, a weirdo, and nothing but a bothersome nuisance that nobody would ever want to spend 30 seconds alone with. I'm trying to reject these thoughts, but it's difficult when you have nobody to talk to. It's like pulling yourself up by your bootstraps. I wonder how many people have the same issue. I've made a few friends in person, but I rarely get to see them. Well I've started doing public surveys in my nearby big city, and documenting the results. I just hold out a posterboard that says \"how alone do you feel\"[1] or \"have you ever been in love\" etc, and hold out a marker, and people come up and take the survey. At first I did this out of sheer loneliness and boredom. But I have done it for enough months that some people have come up to me and told me that I've helped them, or that they look forward to my signs. I'm trying to reach those people who feel the way I feel have no way of connecting with anyone, or at least feel that they don't. Do you have any new ideas of how to achieve this? [1]  https://chicagosignguy.com/blog/how-alone-do-you-feel.html  Why do they feel they can't join any local groups?  Fix that. Intentionally choose community and the effort it takes to build and cultivate it [1] [2] [3] [4] [5]. People are work, but you cannot live without community [6]. [1]  https://web.archive.org/web/20250212233145/https://www.hhs.g...  [1]  https://thepeoplescommunity.substack.com/  [3]  https://www.tiktok.com/@amandalitman/video/7592750", "positive": "The Illustrated Transformer. Haven't watched it yet... ...but, if you have favorite resources on understanding Q & K, please drop them in comments below... (I've watched the Grant Sanderson/3blue1brown videos [including his excellent talk at TNG Big Tech Day '24], but Q & K still escape me). Thank you in advance. Here's the comment from the author himself (jayalammar) talking about other good resources on learning Transformers:  https://news.ycombinator.com/item?id=35990118  Kudos also to Transformer Explainer team for putting some amazing visualizations  https://poloclub.github.io/transformer-explainer/ \nIt really clicked to me after reading this two and watching 3blue1brown videos I have this book. Really a life savior to help me catching up a few months ago when my team decided to use LLMs in our systems. (Going on a tangent.) The number of transformer explanations/tutorials is becoming overwhelming. Reminds me of monads (or maybe calculus). Someone feels a spark of enlightenment at some point (while, often, in fact, remaining deeply confused), and an urge to share their newly acquired (mis)understanding with a wide audience. People need to get away from this idea of Key/Query/Value as being special. Whereas a standard deep layer in a network is matrix * input, where each row of the matrix is the weights of the particular neuron in the next layer, a transformer is basically input* MatrixA, input*MatrixB, input*MatrixC (where vector*matrix is a matrix), then the output is C*MatrixA*MatrixB*MatrixC. Just simply more dimensions in a layer. And consequently, you can represent the entire transformer architecture with a set of deep layers as you unroll the matricies, with a lot of zeros for the multiplication pieces that are not needed. This is a fairly complex blog but it shows that its just all matrix multiplication all the way down.  https://pytorch.org/blog/inside-the-matrix/ . I think the internal of transformers would become less relevant like internal of compile", "negative": "Show HN posts p/month more than doubled in the last year. I suspect that this will drive the folks who insist LLM productivity gains are the real hallucinations truly bonkers. I think I read some days ago another stat, that the average rating of \u201cShow HN\u201d posts is going down. So the pessimistic take is that people feel the bar to present their product in a \u201cShow HN\u201d is lowering. (edit: striked) <strike>Is it deliberate that this post appears as \u201cShow HN\u201d itself? I hope not to be too negative, but to qualify as such I would expect much more that a page with two graphs.</strike> > Show HN is for something you've made that other people can play with. HN users can try it out, give you feedback, and ask questions in the thread. This is an interesting post, but not a Show HN. Would be nice to see some qualitative analyis to know if it's just slop, or actually more interesting projects. Not sure how to do that though. I think just looking at votes wouldn't work. I would guess more posts causes lower average visibility per post which should cause upvotes to slump naturally regardless of quality. Edit: maybe you could: - remove outliers (anything that made the front page) - normalise vote count by expected time in the first 20 posts of shownew, based on the posting rate at the time One of the reasons is that there are a lot of adverts masquerading as Show HN. Do you have any numbers on the number that get some number of upvotes? What about a chart of upvotes on Show HN? I assume the vast, vast majority never get any upvotes. Probably the same happening for websites being built or apps being published. This is very similar to  https://news.ycombinator.com/item?id=46702099  posted 4 days ago. And there is also this  https://dewmal.medium.com/hacker-news-is-a-living-time-capsu...  Laid off people have more time on their hands, while on llm-powered steroids? A better metric would be how many Show HN posts are reaching the front page. Related: \"Data on AI-related Show HN posts\" O"}
{"anchor": "Toad is a unified experience for AI in the terminal. I'm really looking forward to trying this out over Christmas break. Textualize is awesome for building Python console apps. This looks really cool. I wonder if they support vi keybinds Hi. Will McGugan here. I built Toad. Ask me anything. This looks great! Looking forward to trying it out. I recently tried moving to OpenCode but it didn\u2019t quite scratch the itch UX wise. I see what you did what that intro and I approve :) This is absolutely awesome but the little jokey captions that Claude did (Discombobulating... Laminating...) all that stuff, they were a little annoying but cute enough, but whatever is running this one (I did not murder him... I thought I was special....) they are genuinely offputtingly bad. This great app doesn't need clunky humour front and centre, I'm not sure if it's Claude or toad but it seems markedly worse than Claude used to be. I already used Toad to run a conversion task I've been procastinating on. It worked perfectly and looked splendid doing so. Excited to dig in further. toad is next level in many ways Very excited to see this come out - though coding agents are impressive their UIs are a bit of a mixed bag. Textual offers incredibly impressive terminal experiences so I'm very much looking forward to this. I wonder how much agentic magic it'll be able to include though - Claude Code often seems like a lot of its intelligence comes from the scaffolding, not just the LLM.  I'm excited to see! It would be a matrushka to run Toad in a Zed terminal. The name Toad gave me a flashback to Tool for Oracle Application Development, an IDE and debugger for SQL and pl/sql back in the 1990s. I\u2019m not a big fan of the name Toad, but the Textual framework is fantastic. I\u2019ve been using it for years in a small project and it\u2019s just a wonderful tool - it makes it really easy to get a super fast little UI for scripts. I strongly resonate with the problem statement, but this implementation was very far o", "positive": "Sergey Brin's Unretirement. > Having given so much of themselves to their careers, they often felt unmoored and purposeless when they left their jobs. That's in contrast with all of us who see the companies led by these guys as the cancer of society and we'd quit and never look back if we had FU money. My feelings aside, if all their purpose is to grow their company, I kinda get why they wouldn't give a damn about bettering the mankind, improving their communities or raising a healthy family. Financial freedom is about not having to worry about losing your job, or tolerating shitty work conditions. Why would you retire if you do what you love? I think the real problem might be if there's nothing you actually love doing (long term), that's when money won't help. Once you're hooked, you're hooked When I started my company, we suddenly found that we were in a good small fortune, not enough to be millionaires or billionaires, but enough to get people to run the business semi automatically with very minimum input from the founders. I took a semi retirement approach to the business, there really wasn't a lot of things to do, my role was sort of just \"managing\" programmers. I got so much free time that I could even start a second business on the side. Despite my best ability to stretch my work, I couldn't even fill up half of my working hours. One would have thought that this is heaven. But the time I was most free was also the time I was most miserable. I wasn't happy, I was gaining weight, I was perennially asking myself why the business couldn't be bigger and I couldn't sell it, so that I can be real millionaires and billionaires with financial freedom! Then fate intervened, the sudden fortune disappeared and I no longer had the luxury of just \"managing people\"; I have to do hands-on. And it was this activity, the feeling that I was contributing to something, that I was writing code again and actually building stuffs, that made me happy again. Today we are bigger than w", "negative": "DHS keeps trying and failing to unmask anonymous ICE critics online. Also related: \"Don\u2019t say \u2018Watch out for ice\u2019: FEMA warned storm announcements could invite memes\"[0] This administration is  really  sensitive about ICE being shined in a bad light. 0:  https://www.cnn.com/2026/01/23/politics/fema-ice-storm-memes  Given ICE's unpopularity this is like trying a find a very specific piece of hay in a hay stack. This is about posting license plates (presumably not of personal vehicles), facial images, and names of federal officers. I mean I thought we already make federal employees and vehicles public knowledge. The national guard currently deployed in Minneapolis are unmasked as far as I know to compare. I'm not understanding why DHS federal employees are exempt from this standard. I see a future where your comment history builds your known profile -  at scale for everyone. Meanwhile at least two people who have openly murdered people are now effectively in witness protection without even investigation, forget trial Just firing a gun on a street will open an investigation on any other cop in the country Now killing someone gets a pass? We are a banana republic now with the government executing protestors Eventually it will be a dozen protestors shot at once, they already know they will get a pass based on policy, why stop at just one? It's pretty offensive that DHS is spending our tax dollars trying to supress critisim \u2014free speech. The most protected type free speech is poltical speech. Even pursuing identification could be construed as abusive and unlawful. Not sensitive enough to change behaviors, unfortunately. It turns out there wasn't a generation of crystal, these guys are the functionaries of crystal. Too frequently is functionaries loudly complainig about any kind of questioning. Or is it more like trying to find all the hay in a hay stack? The point is presumably to make an example of a few, and use that to deter future people from posting information about"}
{"anchor": "Tmux \u2013 The Essentials (2019). it's missing changing Ctrl+B to Ctrl+A:       # ~/.tmux.conf\n    set-option -g prefix C-a   I like tmux a lot, but like its predecessor \"screen\" I mostly use it for explicitly running long-lived jobs (i.e. for its detach feature), and for very special situations where I have elaborate tmux window configurations with dedicated stuff running in each window/pane. Note that I have been using text-only terminals since the 1980s, but I've adapted my tty usage over time. The problem that tmux (or screen) brings are first and foremost: * Smooth/fast scrolling goes away. I can no longer give my trackpad a slight push to find myself tens or hundreds of lines in the scrollback history, and visually scan by slightly pushing my fingers back and forth. Instead I have to use the horrendous in-tmux scrollback using \"Ctrl-b [\". * My terminal app's tabs and windows are not tmux's tabs and windows. I cannot freely arrange them in space, snap them off with the mouse, easily push them to another desktop, and so on. I have to start a multiple tmux clients and do awkward keyboard interactions with them for any of the same. * tmux's terminal emulation and my terminal emulator's terminal emulation (heh) are not congruent. As a result, programs cannot make full use of my actual terminal's capabilities. For example selecting, copying, and pasting text sometimes behave weirdly, and there are other annoyances. What I'd  really  like to have instead is terminal session management at a higher level, i.e. involving my actual graphical terminal app itself. Attaching to a running session would mean restoring the terminal app's windows and tabs, and the entire scrollback history within (potentially with some lazy loading). tmux could likely be a major part of that, by providing the option of replacing its tty-facing frontend with a binary protocol that the graphical terminal app talks to, while keeping the backend (i.e. the part that provides the tty to anything running ", "positive": "Siddhartha. Huh, funny this should pop up here. I recently started commuting by subway into work, so I had to pick up a subway book. I had been meaning to read this, so I went to my local book store and grabbed a copy. It\u2019s a really great book. Such a fascinating story. And short, too. I highly recommend giving it a read. It might synthesize some of your loose connections about Hinduism, Buddhism, and your own place in a chaotic world and what it means to live a happy life. great book.  I recommend people read it every 10 years or so as your perspective on life changes. I'm very grateful that this was assigned reading in high school, since it was a sort of gateway book for reading more about Buddhism. It's short. If you haven't read it, I highly recommend it. One of the greatest authors of all time. Hesse taps into the mind of the modern human and beautifully presents its inner workings. Each of his books takes a different angle, a different perspective or philosophy with which to observe the evolving personhood. I read this in high school, but not because it was assigned.  At the time I was really into \"rare\" Queen MP3s, and there's a studio recording of the fast version of \"We Will Rock You\" where Brian May reads a passage from this book before the music starts.  An odd way to be inspired to read a book, but I still think I got a fair bit out of it. Off topic: Im curious what\u2019s the most prominent religion among HNers? Is it different from the normal population? Buddhism seems to be number 1 after atheism which isn\u2019t a religion. I liked this quite a bit the first time I'd read it. A decade later, not as much. Narcissus and Goldmund is my favorite book by Hesse - it's beautifully crafted. I read this annually, typically in a day, usually when I'm feeling lost. For me it distills the human experience into a simply story that helps me find meaning for where I am in my own journey. Love this book. I have three sons and read this when them when they're about 12 or 13. I", "negative": "Velox: A Port of Tauri to Swift by Miguel de Icaza. A \"port\" or \"a nice Swift API for it\"? It seems like the latter in that it requires cargo (the rust build chain) to build. The runtime-wry-ffi ( https://github.com/velox-apps/velox/blob/f062211ced4c021d819... ) file which is 3.2K lines long and has close to a 100 unsafe calls, isn't that just interacting with wry which has it's own crate you could use instead? I'm not 100% sure, but seems to be basically the same as wry itself but without the cross-platform stuff, is that the purpose of that file? Together with the author's distaste for Rust, it seems awfully dangerous instead of pulling in a crate made by Rust developers, but I might misunderstand the purpose of the file here. Not to be confused with Velox a compute engine  https://github.com/facebookincubator/velox/  Eh. Dioxus to me is the more interesting project honestly. To anybody with experience, how's Swift? Especially outside MacOS/iOS programming. Let's say I want to use it standalone for doing some systems programming, how's the standard lib? I'd like to not rely on apple specific frameworks like uikit Have built a cross alternative tailscale gui client based on tauri, the rust and ffi to cgo tailscale feel a little tough, I was wondering it will save a lot time to me if the tauri had been written in go. Seems Miguel\u2019s velox point a new idea, leveraging the wry and use ffi to go, and rewrite some tooling. I hope I will have the spare time and energy to give a try\u2026 For the uninitiated: > Tauri is a framework for building tiny, fast binaries for all major desktop and mobile platforms. Developers can integrate any frontend framework that compiles to HTML, JavaScript, and CSS for building their user experience while leveraging languages such as Rust, Swift, and Kotlin for backend logic when needed.  https://v2.tauri.app/start/  I asked the author about whether this could be ported to support Android/Linux/Windows and he was optimistic it would not be much t"}
{"anchor": "Colab Pro. This seems to be a hosted Jupyter service, right mybinder, is that right? A preemptible P100 + VM on Google Compute Engine is about ~$0.45/hr, so to exceed that value with Colaboratory Pro (ignoring conveience factors) you'd need to train for more than 22 hours in a month. Which, for deep learning, is not too unreasonable. Reading between the lines of both the signup page and up-to-date FAQ, it seems like the free TPU in Colab notebooks will be depreciated, which isn't too surprising. Good idea, but it's the first premium product that I've seen where the pitch is 'you  may  get certain features if you subscribe'. In another words there is no guarantee and a premium subscriber may still end up with same GPU as a free user. You may end up with a high-end V100 (not available to free) might be a better pitch. Colab is the best notebook I've ever used. It is a real game-changer and I can totally understand why people who use daily would pay for it. > For now, Colab Pro is only available in the US. This is so much better than buying your own hardwares. There's so much data in this universe, people don't know what to do with it. When people don't know what to do, an industry grows to let them \"feel\" they are doing something useful. I wish they would connect Colab under  https://script.google.com  so you can run a notebook at interval times, something akin to what  https://github.com/TensorTom/colabctl  does. I've been using Colab for over a year now. I train deep learning models on NLP and medical imaging datasets. It's a great tool and it lets you focus on the code and the models, instead of the hardware and OS. But $9.99/month is a little expensive for my taste. You can't customize it and if they change something you have to install software by hand sometimes. It should be $1.99/month, that's the kind of price I'd pay for this basic cloud computing service. edit: I use Colab to play with ML models. I really don't think it's possible, for instance, to train a m", "positive": "TeraWave Satellite Communications Network. Interesting there is an optical networking option for end users (claims ~6TBps). Maybe a really dumb question, but how would the end user's ground station maintain connectivity during cloudy weather? Do they have cloud-penetrating lasers from the MEO satellites? Would that interfere with aircraft, astronomy tools, etc? Some short googling says they have lasers that clear a path for a data carrying beam, but that seems wasteful/infeasible for commercial uses Might be better to replace url with the full press release which has actual information  https://www.blueorigin.com/news/blue-origin-introduces-teraw...  >The TeraWave architecture consists of 5,408 optically interconnected satellites in low Earth orbit (LEO) and medium Earth orbit (MEO). this seems rather expensive but i get that its not competing with spacex here for consumer market Latency may play a factor here, I'm not sure at which height they plan to put them. All those AI datacenters in space will need a way to get data to them. Bezos can't even build his first constellation and already planning his second...  Possibly the real play here is snapping up more frequency licenses on earth (we need them because we're launching any day now promise). They are the real constraining resource and could be used to keep others out of the market for a while. From a technical standpoint: amazing achievement, and the tech nerd in me is in awe. But it feels like a lot of people don't understand (or care?) how much these companies are polluting the space. Before the \"new wave\", in 2010-2015 or so, Earth had around 1500 active satellites in orbit, and another 2,000-2,500 defunct ones. Starlink now has almost 9,500 satellites in orbit, has approvals for 12,000 and long-term plans for up to 42,000. Blue Origin has added 5,500 to that. Amazon plans for 3,000. China has two megaconstellations under construction, for a total of 26,000, and has filed for even larger systems, up to 200,0", "negative": "When employees feel slighted, they work less. Thanks professor, my boss didn't believe me when I tried to hint it On some level the headline is like \"yeah, no shit,\" but the surprising thing is the claimed strength of the effect.  50% absenteeism increase for missing a birthday congratulations?  Really? This seems obvious but I guess needs 'official research' to register. A quote I remember from a coleage - 'They wouldn't give me a pay rate rise, so I gave myself one, by working less hours in a day' how does most academia ever get funding? Breaking news: when it rains, people get wet Yes. We needed an essay to crack this one I understand the co-authors are research fellows at the Maximegalon Institute of Slowly and Painfully Working Out the Surprisingly Obvious I wonder if this is true for PhD students Lots of \u201cno shit\u201d in these comments makes me wonder how many VP level managers you guys have interacted with.\n Maybe it\u2019s just my location, but this is one of those things that legitimately NEVER makes it through to upper managers. When they tell their base managers to crack the whip and force them to give the whole \u201cyou are not working hard enough, tighten up. Shorter lunches, clock in 5 minutes early, etc\u201d speech to the base employees, they will absolutely feel resentment and do LESS work, not more. For more than one reason. A quite small few will be pushed over the edge and spend their energy trying to find a new position altogether. But the impact of losing them and having an open position for months will have a huge impact. The impact of losing even a below average worker is nearly always underestimated by uppers who see their 200+ indirects as just numbers on an HR chart. And the employees who hop jobs over bad management are usually in the top half of performance, not bottom. Another handful of over-achievers will realize that their \u201cextra mile\u201d approach is clearly being ignored or not having any effect, and simply become achievers. This alone can have an impac"}
{"anchor": "Insights into Claude Opus 4.5 from Pok\u00e9mon. The idea of Claude having \"anterograde amnesia\" and the top-rated comment there by Noosphere89 really resonated with me:     \"I would analogize this to a human with anterograde amnesia, who cannot form new memories, and who is constantly writing notes to keep track of their life. The limitations here are obvious, and these are limitations future Claudes will probably share unless LLM memory/continual learning is solved in a better way.\"\n\n  This is an extremely underrated comparison, TBH. Indeed, I'd argue that frozen weights + lack of a long-term memory are easily one of the biggest reasons why LLMs are much more impressive than useful at a lot of tasks (with reliability being another big, independent issue).\n\n  It emphasizes 2 things that are both true at once: LLMs do in fact reason like humans and can have (poor-quality) world-models, and there's no fundamental chasm between LLM capabilities and human capabilities that can't be cured by unlimited resources/time, and yet just as humans with anterograde amnesia are usually much less employable/useful to others than people who do have long-term memory, current AIs are much, much less employable/useful than future paradigm AIs.   I wonder if there's someone at Antrophic working to fine-tune the model's pokemon playing ability specifically. Maybe not but it sure would be funny. This actually matches my experience quite well. I use vision (often) to try and do 2 main things in Claude code: 1) give it text data from something that is annoying to copy and paste (eg labels off a chart or logs from a terrible web UI that doesn't make it easy to copy and paste). 2) give it screenshots of bugs, especially UI glitches. It's extremely good at 1), can't remember when it got it wrong. On 2) it _really_ struggled until opus 4.5, almost comically so, with me posting a screenshot and a description of the UI bug and it telling me \"great it looks perfect! What next?\" With opus 4.5 it's not ", "positive": "Tao Te Ching \u2013 Translated by Ursula K. Le Guin. For people who like The Big Lebowski, there's \"The Tao of the Dude\"  https://dudeism.com/taoofthedude/  I picked up Tao Te Ching as an American teenager and was moved by how it cuts against the American faith in visible dominance and self-assertion, proposing a form of strength that is low, quiet, and unseen. It's much more than that of course, but that aspect had immediate impact on my thinking. HN seems to like Tao Te Ching.  https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu...  From the bottom: > This is a rendition, not a translation. I do not know any Chinese. I could approach the text at all only because Paul Carus, in his 1898 translation of the Tao Te Ching, printed the Chinese text with each character followed by a transliteration and a translation. My gratitude to him is unending. This is wonderful. Ursula K. Le Guin is a great thinker and  I\u2019d highly recommend her novels. I\u2019ve read Ken Liu\u2019s, who many here probably know at least from translating The Three Body Problem and Death\u2019s End, Tao Te Ching and it was remarkably poetic. Excited to read another person\u2019s interpretation. I am just noticing how those ideas are present in Wizard of Earthsea. > I think of it as the Aleph, in Borges\u2019s story: if you can see it rightly, it contains everything. I'm a simple man. I see Borge, I upvote As another comment points out, Le Guin herself does not call this a translation, so we shouldn't misrepresent it (although it might be my favorite English version). However, it's not in the public domain. Her work deserves all the attention it can get, but I'd rather not see it pirated wholesale. This is one of my favorite versions, mostly for nostalgic reasons. My initial exposure to the Tao te Ching was this \"rendition\" and Stephen Mitchell's version. Comparing the two was always very thought provoking; the approach is very different between them. I often come to this site and compare chapters across multiple versions:", "negative": "Tesla\u2019s autonomous vehicles are crashing at a rate much higher tha human drivers. To be honest I think the true story here is: > the fleet has traveled approximately 500,000 miles Let's say they average 10mph, and say they operate 10 hours a day, that's 5,000 car-days of travel, or to put it another way about 30 cars over 6 months. That's tiny! That's a robotaxi company that is literally smaller than a lot of taxi companies. One crash in this context is going to just completely blow out their statistics. So it's kind of dumb to even talk about the statistics today. The real take away is that the Robotaxis don't really exist, they're in an experimental phase and we're not going to get real statistics until they're doing 1,000x that mileage, and that won't happen until they've built something that actually works and that may never happen. By the law of large numbers, it's not a significant distance. As long as there are still safety drivers, the data doesn't really tell you if the AI is any good. Unless you had reliable data about the number of interventions by the driver, which I assume Tesla doesn't provide. Still damning that the data is so bad even then. Good data wouldn't tell us anything, the bad data likely means the AI is bad unless they were spectacularly unlucky. But since Tesla redacts all information, I'm not inclined to give them any benefit of the doubt here. The comparison isn't really like-for-like. NHTSA SGO AV reports can include very minor, low-speed contact events that would often never show up as police-reported crashes for human drivers, meaning the Tesla crash count may be drawing from a broader category than the human baseline it's being compared to. There's also a denominator problem. The mileage figure appears to be cumulative miles \"as of November,\" while the crashes are drawn from a specific July-November window in Austin. It's not clear that those miles line up with the same geography and time period. The sample size is tiny (nine crashes)"}
{"anchor": "Scientists find a way to regrow cartilage in mice and human tissue samples. Of course, why are the good ones always in mice?     A study led by Stanford Medicine researchers has found that an injection blocking a protein linked to aging can reverse the natural loss of knee cartilage in older mice.    https://www.science.org/doi/10.1126/science.adx6649  A small molecule inhibitor of 15-hydroxy prostaglandin dehydrogenase causes cartilage regeneration. I hope they fast-track it to human trials. basically every growth process in the body can be induced by chemicals. and so now people are starting to take some of these chemicals. we will see how it turns out As long as regrowth can be controlled. Otherwise we call it cancer. Would be amazing to get a treatment for osteoarthritis. Fusion Power Cartilage Regrowth Room Temperature Semiconductors Quantum Computing       def generate(topic, year):\n       return f\"Scientists have made a major breakthrough in {topic}\"\n  \nThe only subjects that are more Year Of The Linux Desktop than Linux itself. The discovery of gerozymes is interesting. Maybe aging is pre-programmed after all, to make space for new generations. Would this work for rheumatoid arthritis? I don\u2019t know anything about it myself so it could be a completely different thing, but someone I know has it and it is awful. Would be great to see a treatment coming through. My dream is to be able to run again. Please. Let me run a 10k at least once more in my life. To feel that stillness and freedom and calm that sets in when the brain start going to hibernation after about 7km. That would be quiet something to feel that again. I\u2019ve had my shoulders \u201ccleaned up\u201d arthroscopically, and the pain is still a major preventer of movement. I would love to stay on the mats longer with something that doesn\u2019t harken to medieval times. So excited at this prospect. HN posts about mouse studies always trigger a bunch of skepticism.  I\u2019m a layperson so it\u2019s hard to separate the informed c", "positive": "A real-time 3D digital map of Tokyo's public transport system. Very cool. Even the building-by-building graphics seem to be correct: a boxy version of my house in Yokohama is in the correct location and has the correct height relative to its neighbors. The map also shows\u2014correctly\u2014that it is raining at this moment in Tokyo but not in Yokohama. This is great, however on first load I didn't get the trains moving. After a refresh they showed up again. Currently sitting on the Yokohama line to Hachioji, a little before Hashimoto station. Looking at the map the train had already reached Hashimoto. Seems like we're running 30 seconds or maybe 1 minute late. Do any of the 'live' camera feeds work? They're all static for me. This is super cool, though. Wow, I love that it shows live flights and airplanes! This is really awesome, I love the way you integrated the live camera feeds. Looking at this map makes me want to move to Tokyo. Sure, the trains stopping at night makes nightlife and catching a morning flight annoying, but train culture* of just making plans to meet at a train station with a friend is so much better than the car dependent place I live. *It's not unique to Tokyo, but I've spent extended periods of time in cities with trains and this is what we often did. Tokyo just has lots of train lines. I saw 3D in the title and assumed it was a cross-section view of the subway tunnels underground. An implementation like that would be a potential security risk to public infrastructure. Berlin edition:  https://www.vbb.de/fahrinfo , there was also a version in a similar 3D style but I wasn't able to dig it up through the search. Related thread with more of these kind of projects:  https://news.ycombinator.com/item?id=32647227  That was disappointing I thought I would see the 3D train track tubes and how deep they are and their position from each other in 3D I just came from working remotely from Japan for almost two months. One of the highlights was the infrastructure fo", "negative": "The $100B megadeal between OpenAI and Nvidia is on ice. ...and the merry go round stopped In the distance, Uncle Sam groans as his phone rings If the ice cream cone won't lick itself, who will? Last paragraph is informative: > Anthropic relies heavily on a combination of chips designed by Amazon Web Services known as Trainium, as well as Google\u2019s in-house designed TPU processors, to train its AI models. Google largely uses its TPUs to train Gemini. Both chips represent major competitive threats to Nvidia\u2019s best-selling products, known as graphics processing units, or GPUs. So which leading AI company is going to build on Nvidia, if not OpenAI? All these giant non-binding investment announcements are just a massive confidence scam. Does this mean OpenAI won't be needing all that RAM after all...? Not only has OpenAI's market share gone down significantly in the last 6mo, Nvidia has been using its newfound liquid funds to train its own family of models[1]. An alliance with OpenAI just makes less sense today than it did 6mo ago. [1]  https://blogs.nvidia.com/blog/open-models-data-tools-acceler...  Interesting to see this follow the news of their plan IPO in Q4 just yesterday.  https://www.wsj.com/tech/ai/openai-ipo-anthropic-race-69f06a...  > He[Jensen Huang] has also privately criticized what he has described as a lack of discipline in OpenAI\u2019s business approach and expressed concern about the competition it faces from the likes of Google and Anthropic, some of the people said. This video that breaks down the crazy financial positions of all the AI companies and how they are all involved with one called CoreWeave (who could easily bring the whole thing tumbling down)  is fascinating:  https://youtu.be/arU9Lvu5Kc0?si=GWTJsXtGkuh5xrY0  Would be interesting to see how Oracle's CDSs react to this news. How is this legal for them to do to pump stocks will there be more 5090 FE cards at a lower price? one can only hope OpenAI is too important to run out of cash. The gov wil"}
{"anchor": "The mushroom making people hallucinate tiny humans. This sounds like a variation of Machine Elves --  https://en.wikipedia.org/wiki/Dimethyltryptamine#Entity_enco...  Those are primarily associated with DMT (the one time I tried it, I too had such an encounter and I didn't know it was a thing until years later). I'm sure I'll be corrected on this but I  think  DMT and Psilocybin ultimately affect the same pathways so it's just more evidence that Machine Elves are real! (/s on the real part). Yes, but where can I get some? Previous discussion:  https://news.ycombinator.com/item?id=46393936  But can we grow them in the UK? > \"It sounded so bizarre that there could be a mushroom out there causing fairytale-like visions reported across cultures and time,\" Domnauer says. Now I'm kinda curious whether fairy tales are the result of these visions or the other way around. Probably both. Wonder if it will turn out to be related to muscimol that is in amanita mushrooms, as that is always described as more delirious and dream-like Almost all of the reports from people I know who have done ayahuasca have reported seeing \"elves\". It's not only common, they say it's not a \"valid\" trip unless you do, and even converse with them. Though I don't know any reports of profound conversations. In my youth I experimented with hallucinogenic drugs. Having shared hallucinations are very easy. It often just requires that someone give you an idea of a hallucination, or someone tells you what they see, and your brain will make you see it as well. Maybe people know these things make you see small people, and then they are primed to do so. >Current tests suggest it is not likely related to any other known psychedelic compound. For one, the trips it produces are unusually long, commonly lasting 12 to 24 hours, and in some cases even causing hospital stays of up to a week. Plenty of common psychedelics have durations in excess of 12 hours. Some even in excess of 24 e.g. high doses of 2C-P. This may", "positive": "I built my own CityMapper. Before Citymapper existed, there was OneBusAway, a Ph.D. student project at the University of Washington. It still exists and powers millions of transit rider trips every day all around the world in Seattle, Washington DC, New York City, Poznan Poland, Buenos Aires Argentina, Adelaide Australia, and who knows where else. If you\u2019re interested in hacking on something like Citymapper, or setting up an OBA server for your own city, you can find everything you need on our GitHub organization:  https://github.com/OneBusAway  That includes docker images, an iOS app and a trip planner framework, android app, Sveltekit web app, and even a next generation OBA server written in Go. As far as the data to power this, you can get GTFS for every US transit agency from  https://mobilitydatabase.org/  (nb I\u2019ve been involved in the OBA project since 2012) Why are the table and the description of the RAPTOR algorithm in the article images rather than text? During university, we've built OptiTravel ( https://github.com/denysvitali/optitravel ) to do something similar. We couldn't use Google Maps APIs (project requirement), so we wrote a custom routing algorithm based on A* and I've created a Rust server to host GTFS data ( https://github.com/denysvitali/gtfs-server ) \u00e0 la Transitland ( https://transit.land/ ). Performance wasn't great since everything had to run locally and do network roundtrips, but it found routes in my hometown that Google Maps didn't show. Pretty cool discovering hidden connections in the transit network and being able to customize your own params ( https://github.com/denysvitali/optitravel/blob/master/src/ma... ) I am involved with the OpenTripPlanner project, which is a Java trip planning application that also uses the RAPTOR algorithm! It\u2019s used in cities all over the world, with the biggest deployment being ENTUR\u2019s in Norway, which covers the entire country. I believe all trip planning apps in Norway use this deployment. It supports m", "negative": "New books aren't worth reading?. I almost gave up on this after the first couple of paragraphs but I\u2019m glad I stuck with it. It\u2019s an interesting perspective. I\u2019ll recommend on old book I randomly found a couple years ago: We Took to the Woods. It was surprisingly fascinating. > Because everyone alive today has the same perspective, and none of us have experienced a wide breadth of anything. Is the author living in completely different alternative reality then I do? > The average ancient historian led troops, tutored a prince, governed a province, advised a king, made a fortune, fell from favor, was exiled, and buried 7 of their 10 children. The average modern historian passed a few tests then wrote a book on their laptop next to their cat. Who are all those ancient historians author talks about? Isnt actually studying history better background for writing about history then \"making a fortune, being politician and having many dead kids\"? But obviously what you wont find in the books of these super high level people ... is experience majority of the people who lived earth never had. Nor even had option to have. Frequently because of their lives suffered greatly by actions of these great conquerors. So tldr, Mary Beard is bad at being historian, because she studied history. Also, because she credits feminism for her own understanding of what it is being a woman. Also, because she is estimated to have liberal opinions on climate change, democracy and religion. In the world where everyone is having the same opinions on those ... we will ignore the fact that fascism is currently not just on the rise, but literally winning the institutions. >  Unfortunately, reading books for entertainment is ridiculous. You do not live in a log cabin on the prairie. You have Netflix, you have video games, you have TikTok, you have Twitter (you really spend too much time on Twitter anon). No one reads books for entertainment anymore, because paper is an inferior entertainment platform. Tha"}
{"anchor": "Training my smartwatch to track intelligence. I've tracked sleep using a number of devices and algorithms and I haven't found a single one that regularly aligns with what and how I feel. I know it's tracking real data, but the conclusions feel completely made up. What are other people's experience -- especially from those who are more bullish about sleep tracking? I hope Garmin sees your passion project and greenlights it for inclusion. You have the right approach to ensuring folks are at their optimal health to grow intellectually as a person. > Often, it would also contradict how I was internally feeling. I\u2019d wake up feeling rested, see my stats are low, and play a game of chess out of algorithmic rebellion, only to feel my mind up against a barrier and handedly lose. It would be better to only look at the stats after playing if you want to verify it, this could easily be a self-fulfilling prophecy. The biggest thing for me is I don't understand how people can sleep with these watches on, it's so uncomfortable to me personally which is why the different ring technologies appeal to me more. I just wish either Garmin made one or that there was one I didn't have to buy a subscription to use. I didn't believe the stress numbers on my Garmin watch were very meaningful until I started taking Nebivolol (an atypical beta blocker) because there were so many gaps (even when I was sitting) that I didn't feel I could eyeball them or trust averages over time. Taking that drug,  however,  it sees far fewer gaps and I show up in the blue \"rest\" zone most of the time. I've been watching my heart rate a lot in the last month part because of health concerns and part because of a new stance I am practicing that has a physical component (e.g. adjusted gaits that are energy efficient) and a mental component, being an oceanic reservoir of calm with close mind-body-environment coupling 95% of the time but disconnecting that connection under peak stress -- like I am standing between two ", "positive": "Differentiable Logic Cellular Automata. This writing feels so strongly LLM flavored. It's too bad, since I've really liked Alexander Mordvintsev's other work. I wonder what Stephen Wolfram has to say about this. There\u2019s something compelling about these, especially w.r.t. their ability to generalize. But what is the vision here? What might these be able to do in the future? Or even philosophically speaking, what do these teach us about the world? We know a 1D cellular automata is Turing equivalent, so, at least from one perspective, NCA/these aren\u2019t terribly suprising. The result checkerboard pattern is the opposite (the NOT) of the target pattern.  But this is not remarked upon.  Is it too unimportant to mention or did I miss something? I wish we were all commenting about the ideas embedded in this paper. It intrigues me, but is out of my comfort zone. Love to read more content-related insights or criticisms rather than the long thread on the shamefully smooth, engaging, and occasionally rote style. This is wild. Long time lurker here, avid modeling and simulation user-I feel like there\u2019s some serious potential here to help provide more insight into \u201cemergent behavior\u201d in complex agent behavior models. I\u2019d love to see this applied to models like a predator/prey model, and other \u201csimple\u201d models that generate complex \u201cemergent\u201d outcomes but on massive scales\u2026 I\u2019m definitely keeping tabs on this work! This is very interesting. I've been chasing novel universal Turing machine substrates. Collecting them like Pok\u00e9mon for genetic programming experiments. I've played around with CAs before - rule 30/110/etc. - but this is a much more compelling take. I never thought to model the kernel like a digital logic circuit. The constraints of boolean logic, gates and circuits seem to create an interesting grain to build the fitness landscape with. The resulting parameters can be directly transformed to hardware implementations or passed through additional phases of optimization and", "negative": "An Illustrated Guide to Hippo Castration (2014). I mean, sensationalistic or \"Why didn't you post on / This isn't reddit\" or not, this is one of the more amazing opening sentences ever... > Few things in this world are as elusive as a hippopotamus testicle Short and sweet. An absolute masterpiece of scientific writing! A family friend used to run a travel business with tours to the Okavango Delta. When I asked him how it was going, he replied \"Great, we've only ever lost one honeymoon couple to hippos\"! People don't realise they are one of the most dangerous animals to humans. How long before this is included in an AI benchmark? Can't wait :-) > hippos' stunning wound-healing abilities\u2014perhaps related to the antibacterial properties of the creepy \"red sweat\" that coats their skin sounds interesting and definitely something worth looking into as well We have been taught in high school that the reason humans and \"all mammals\" had external testes was to cool them. But elephants have internal testicles, and, apparently, so do hippos. This seems a much better strategy than having such an important (and sensitive!) organ hanging out at the mercy of predators, foes, or even banal accidents. The evolution explanation for this appears to be lacking. reading this i wonder how long some tech bros need for a starup that offers AI assisted hippo castration for executives as the next hype Flashback to a Robin Williams bit, where Marlon Perkins of \"Wild Kingdom\" directs his assistant to circumcise a water buffalo.  https://www.youtube.com/watch?v=WFbhByVMhXM&t=45s  I don't think much has changed in the state of the art of hippo castration in the last twelve years. I was fond of \u201call the surviving animals were able to return to their feces-infested communal pools within hours of the surgery with no negative consequences\u201d \"Jaws\" hippo edition:  https://www.youtube.com/shorts/aKEgTUkpk64   https://www.youtube.com/watch?v=jJQpq8mLbm0  So is sanctimony, good job nobody around here is b"}
{"anchor": "Take the pedals off the bike. Apparently this guys is unaware that pedalless balance bikes for kids already exist, and are quite popular. The idea is to get kids used to doing the hard part--staying balanced--first, then when they get a 'real' bike, they don't need training wheels, or at least, not so much. This is the standard way that kids learn to ride bikes in Europe.  Apparently the English word for them is \"balance bikes\".  Both my kids could ride one of them when they were 2. Watching your kid learn to ride a bike is one of life's greatest joys If your kid is using a balance bike, be sure to take them somewhere flat like a playground. If the ground is even slightly inclined, the kid cannot coast, and they spend their energy inefficiently pushing themselves and their bike forward. Training wheels aim to maximise the utility of the bicycle (i.e. gears and pnumatic tyres) for a person of certain age, at the cost of learning how to actually ride a bike. I feel there are lots of parallels in e.g. Maths education in the more generalised form: In education, skills that allow you to utilise technology are prioritised and these are often directly opposed to skills needed for mastery. > Bicycles achieve balance through the gyroscopic effect No they don't. If that were true you could cycle along at a 45 degree angle. They balance because you steer into a fall. Now we need a tutorial how to take the pedals off the bike.\nSo you do not damage the crank because of the opposite threading left vs right. The gyroscopic effect contributes little to maintaining a balanced bike ride, contrary to the article claim. An idealized massless wheel/tire wouldn't diminish ridability. Steering dynamics (steering to counteract bike lean) and trail effect (bike are built to automatically counteract lean), along with rider input (steering, leaning body), are more important components. Sheldon Brown wrote about this years ago:  https://www.sheldonbrown.com/teachride.html  > The ideal bike for", "positive": "Ask HN: Where do seasoned devs look for short-term work?. Now is not a great time to be looking for this kind of work unfortunately. I think your network is the best place to look for this sort of work. Sometimes people will reach out to me with short term projects which is the best way to get gigs like this. Maybe start looking at your colleagues on linkedin, see what they are up to, and think of ways to contribute to what they are working on. The best people to contact in this scenario are leadership and decision makers. A SWE II isn't gonna help you much but a CTO at an early stage startup might be a good person to send a DM if they are friends with you (or even if they aren't!) :) Short term work is more plentiful when money is easy and there\u2019s a lot of entrepreneurial activity going on due to some recent catalyst such as mobile app platforms or the dotcom boom etc. Right now we\u2019re in the AI boom and some people may be making money peddling agentic solutions but money is tight and businesses are hurting. It\u2019s also hard to trust a short term dev who doesn\u2019t really need the money. You have no leverage over them. They sort of just do as they please. Most ad-hoc work I've picked up has been people I've previously worked with/for. Maybe worth reaching out to people you have a prestablished relationship with I did this a few years ago and the winning recipe was a shameless (i.e. deeply shameful) linkedin post where I pretty much just summarized my skillset and explained that I was looking for a senior engineer equivalent of a summer internship, with no chance of extension. Got me 3-4 offers. None of the offering companies had ads out for roles like this, so this was pretty much the only way. I'd believe you're better off working on yourself. Maybe do toy projects for your potential portfolio, learn an additional skill (AI?), and build many weekend projects until something sticks. Publishing articles, etc to demo your skill helps you stay top of mind. Even if only the ", "negative": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars. While the headline is interesting. I think the table at the end of the article is more so. - Worldwide sales -10% YoY - China sales -26% YoY And when you cross compare Porsche saying they sold more EV powertrains than their gas equivalents against China's new found foothold as the market leader in consumer electric cars (BYD, NIO, Xiaomi, etc...) Then I think you see an early indication not just of electric car dominance, but of the (very potential) rise of China as the premier automotive super power.  https://youtu.be/ghY78-yWr7o  The key part is electrified and not pure electric. I think Porsche is really in trouble here. I\u2019m not anti-EV but the electric Macan and Cayenne look awful. They are under equipped technologically relative to their Chinese peers (heck basically anything). Porsche sort of sold its soul for this tech-forward design but it doesn\u2019t deliver any meaningful benefits, these cars don\u2019t even have level 2+ highway cruise control. In the meantime I get a bunch of crap screens and lose all the glorious physical buttons and I don\u2019t even have a fun engine rumble to make up for it? So, the cars are ugly and uncool (I grant a matter of taste), aren\u2019t selling in their target market (China) won\u2019t sell meaningfully in their backup market (US) and they\u2019re behind GM, Tesla and BYD in all regards on quality of life stuff. Not a recipe for endurance. a lot of these luxury brands have been eating off china the past few years but now they've lost their luster since china makes cars better than most luxury brands and china has a moat in EVs so what's left is either the US or emerging markets Is this shocking? Obviously including PHEVs helps a bit, but even outside of this it is exactly what should be happening. Their biggest sellers are SUVs, and at these price points, the EVs can be substantially than their ICE counterparts. For 2026, they probably won't even need the PHEVs to get the"}
{"anchor": "AGI fantasy is a blocker to actual engineering. Many big names in the industry have long advocated for the idea that LLM-s are a fundamental dead end. Many have also gone on and started companies to look for a new way forward. However, if you're hip deep in stock options, along with your reputation, you'll hardly want to break the mirage. So here we are. Okay, so come up with an alternative, it's math, you can also write algorithms. Elon thinking Demis is the evil supervillain is hilariously backward and a mirror image of the reality. \"As a technologist I want to solve problems effectively (by bringing about the desired, correct result), efficiently (with minimal waste) and without harm (to people or the environment).\" As a businessman, I want to make money. E.g. by automating away technologists and their pesky need for excellence and ethics. On a less cynical note, I am not sure that selling quality is sustainable in the long term, because then you'd be selling less and earning less. You'd get outcompeted by cheap slop that's acceptable by the general population. I like the conclusion; like for me, Whisper has radically improved CC on my video content. I used to spend a few hours translating my scripts into CCs, and tooling was poor. Now I run it through whisper in a couple minutes, give one quick pass to correct a few small hallucinations and misspellings, and I'm done. There are big wins in AI. But those don't pump the bubble once they're solved. And the thing that made Whisper more approachable for me was when someone spent the time to refine a great UI for it (MacWhisper). I'm surprised the companies fascinated with AGI don't devote some resources to neuroscience - it seems really difficult to develop a true artificial intelligence when we don't know much about how our own works. Like it's not even clear if LLMs/Transformers are even theoretically capable of AGI, LeCun is famously sceptical of this. I think we still lack decades of basic research before we can ", "positive": "BERT is just a single text diffusion step. Very cool parallel. Never thought about it this way \u2014 but makes complete sense Fun writeup! It's amazing how flexible an architecture can be to different objectives. When text diffusion models started popping up I thought the same thing as this guy (\u201cwait, this is just MLM\u201d) though I was thinking more MaskGIT. The only thing I could think of that would make it \u201cdiffusion\u201d is if the model had to learn to replace incorrect tokens with correct ones (since continuous diffusion\u2019s big thing is noise resistance). I don\u2019t think anyone has done this because it\u2019s hard to come up with good incorrect tokens. Interested in how this compares to electra To my knowledge this connection was first noted in 2021 in  https://arxiv.org/abs/2107.03006  (page 5). We wanted to do text diffusion where you\u2019d corrupt words to semantically similar words (like \u201cquick brown fox\u201d -> \u201cspeedy black dog\u201d) but kept finding that masking was easier for the model to uncover. Historically this goes back even further to  https://arxiv.org/abs/1904.09324 , which made a generative MLM without framing it in diffusion math. To me, the diffusion-based approach \"feels\" more akin to whats going on in an animal brain than the token-at-a-time approach of the in-vogue LLMs. Speaking for myself, I don't generate words one a time based on previously spoken words; I start by having some fuzzy idea in my head and the challenge is in serializing it into language coherently. To me part of the appeal of image  diffusion models was starting with random noise to produce an image. Why do text diffudion models start with a blank slate (ie all \"masked\" tokens), instead of with random tokens? I love seeing these simple experiments. Easy to read through quickly and understand a bit more of the principles. One of my stumbling blocks with text diffusers is that ideally you wouldn\u2019t treat the tokens as discrete but rather probably fields. Image diffusers have the natural property that a pi", "negative": "Tell HN: Happy New Year. Happy New Year from Arkansas, USA! This year let\u2019s all act kindly towards others and learn and discuss things in civil tones.  In a world where Reddit is the norm, make Hacker News a beacon of hope. Happy New Year from Cairo, Egypt! Happy New Year from Krasnoyarsk, Russia! Exactly one year ago, we started tirreno on New Year\u2019s night at Show HN.\nToday, we\u2019re nearly at 1k stars and dozens of releases in. Thanks so much, HN, for this year. H_N_Y from the Alps! HNY from Xizhou, Yunnan, China Happy New Year from Australia :) Happy 2026 from Venice, Italy! Happy New Year from Melbourne! May it be a good one :) I wonder what events will transpire that will shape our world by this time next year. There will be a new FIFA World Cup champion unless Argentina does the unthinkable. No doubt regime change somewhere. Probably a massive cyber event that cripples critical infra. A magnitude 8+ earthquake in a populated area? Happy New Year from Rotterdam, The Netherlands Had a pretty bad year. Hoping 2026 will be way better! Happy new year everybody Happy New Year from Shenzhen. Countdown in 1h45m. If anyone in Shenzhen wants to cheers, I shall be at Revolucion Cocktail. Happy new year from the middle east! May 2026 be a year of peace and compassion <3 Happy New Year from Zagreb, Croatia! 2025 wrap Got an internship Rejected a lowpay job offer Quit the 9 \u2013 5 Built my own SaaS in 15 days as a self-taught dev Got 250+ users on my app Made my first sales before even consuming my 1-month stipend savings Crossed $100+ in December revenue Learned coding + marketing Didn\u2019t skip the gym     Deadlift PR: 400 lbs @ 130 lbs BW\n\n Betting on myself in 2026\n\n Happy New Year   Happy New Year from New York City!! Happy New Year's Eve. Hoping to see the start of the AI-powered technological singularity in 2026. The end of humanity is nigh and I can't wait to be melded to the machine. This life was kind of boring anyway. Happy New Year from Singapore! I wish everyone a gentl"}
{"anchor": "Is OpenAI Dead Yet?. Tracking the demise of OpenAI through the news cycle No, they are not dead.  However, they face incredible competition in a brutally commoditized product space. As a retail investor mostly invested into broad ETFs (All World), is there any way I can get short exposure to OpenAI? Being short Oracle/Nvidia/Microsoft? Relevant, I would definitely be sleeping uneasy if I was at \u201cOpen\u201dAI. Some insist that Chinese models are a few generations behind, how many probably depends more on patriotism rather than fact. Those people typically also insist that Chinese models are just distillations and often neglect to see how many of these companies contribute to the theory of designing efficient and capable models.\nIt is somehow thought that they will always trail US models. Well. i would say look at recent history. China worked up the ladder of manufacturing from simple, bad stuff to highly complex things - exactly what westerners then claimed they\u2019d never be able to.\nThen as that was conquered, westerners comforted themselves by insisting that China could copy, but trail-blazing would always still be our thing. Well, Baidu and Alibaba face scaling issues few western companies do and BYD seems to match Tesla or VW just fine. I am unsure why anyone would think US models are destined to remain in the lead forever. At \u201cbest\u201d, I see a fragmented world where each major region (yes also Europe) will eventually have their own models - exactly because no one wants to give any competitive power a chokehold over their society. But beyond that, models will largely be so good that this \u201cgeneration\u201d/universal superiority idea becomes completely obsolete. Is OpenAI profitable yet? Will it be in time to recoop capex. It will be the first application of the 'curse of Open company' rule: any for-profit entity that has the name Open in it is destined to go bankrupt. Keep in mind that the \"news cycle\" isn't of much use in this field. For 2025, almost all \"mainstream\" media was", "positive": "BYD's cheapest electric cars to have Lidar self-driving tech. Lidars come down in price ~40x.  https://cleantechnica.com/2025/03/20/lidars-wicked-cost-drop...  Meanwhile visible light based tech is going up in price due to competing with ai on the extra gpu need while lidar gets the range/depth side of things for free. Ideally cars use both but if you had to choose one or the other for cost you\u2019d be insane to choose vision over lidar. Musk made an ill timed decision to go vision only. So it\u2019s not a surprise to see the low end models with lidar. Keep in mind, that $25k AUD is just $16600. And for that price, you're getting a real car with driver-assist features and a reasonable crash safety rating. The US car manufacturers are cooked. Still not convinced of the safety of lidar. I guess all these cars with cheap lidar sensors on board will generate real world safety data over the next few years. The roof mount seems very practical, but it's a look that may turn off some buyers... buyers who care about looks. For SUVs, maybe it could be blended in with a roof air scoop, like on some off-road trucks. Or a light bar. Where is the LiDAR on the Atto 1? In the grille? How much worse is the field of view? How do you train a model to drive with LiDAR when the human drivers who generate the training data don\u2019t use LiDAR? Related: Volvo drops LIDAR...  https://www.carscoops.com/2025/11/volvo-says-sayonara-to-lid...  Are you serious, a car with Lidar sensor that's not even available in Bugatti Tourbillon that cost 500x more? Joking aside, this BYD Seagull, or Atto 1 in Australia (AUD$24K) and Dolphin Surf in Europe (\uffe118K in the UK), is one the cheapest EV cars in the world and selling at around \uffe16K in China. It's priced double in Australia and triple in the UK compared to its original price in China. It's also one of China best selling EV cars with 60K unit sold per month on average. Most of the countries scrambling to block its sales to protect their own car industry or increas", "negative": "San Francisco Graffiti. Would have looked further, but scroll wheel finger cramped. Keyboard nav would be great. As a suggestion, * Orientation - some images are sideways, * Option to walk through by date order, and by location ... There is an audience for the time ordered flux of images on particular sites (at least in Australia). I like the concept, wish it was a vertical scroll with some safe margins between each picture (also to give them more stage time and removing the  noise/distraction from many pictures stitched together) Fascinating, I do love street art and tastefully done graffiti. Some of it is obnoxious. I think it does add to the character of a city e.g. New York, Berlin, Montreal, Paris all have some amazing work etc. I submit Irish Graffiti I see here:  https://old.reddit.com/r/Graifiti/  Though I think displaying these things as a map is more useful:  https://streetartcities.com/cities/sanfrancisco  There is a an Irish artist called Dan Leo and I have bought lots of his prints.  https://www.danleodesign.com/  so they are dotted around my office and home. I think they're great! He does animals and I love the style, clean lines and bright colours, they remind me of US football team logos. I live near Paris, and it's a shame to see this sort of thing on every surface here.  It's so easy and effortless to trash the look of a place, and so much effort and pain to get it back to a presentable state.  It just seems hopeless trying to stop it. Sure, you can point to examples of graffiti that don't look all that bad, and I imagine some examples can even be considered to improve the look of a space.  But taking this site as a random sample, the \"good\" ones are a vanishing minority.  For every subtle Invader mosaic high on a building, you get dozens of effortless name tags that just wreck the look of a place. Adding frustration is the fact that there's no way to effectively dissuade people from doing this.  You don't want to fine, jail or otherwise ruin the l"}
{"anchor": "ChatGPT Containers can now run bash, pip/npm install packages and download files. Regular default ChatGPT can also now run code in Node.js, Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C and C++. I'm not sure when these new features landed because they're not listed anywhere in the official ChatGPT release notes, but I checked it with a free account and it's available there as well. I wonder how long npm/pip etc even makes sense. Dependancies introduce unnecessary LOC and features which are, more and more, just written by LLMs themselves. It is easier to just write the necessary functionality directly. Whether that is more maintainable or not is a bit YMMV at this stage, but I would wager it is improving. Maybe soon we have single use applications. Where ChatGPT can write an App for you on-the-fly in a cloud sandbox you interact with it in the browser and fulfill your goal and afterwards the App is shutdown and thrown away. I wonder if the era of dynamic programming languages is over. Python/JS/Ruby/etc. were good tradeoffs when developer time mattered. But now that most code is written by LLMs, it's as \"hard\" for the LLM to write Python as it is to write Rust/Go (assuming enough training data on the language ofc; LLMs still can't write Gleam/Janet/CommonLisp/etc.). Esp. with Go's quick compile time, I can see myself using it more and more even in my one-off scripts that would have used Python/Bash otherwise. Plus, I get a binary that I can port to other systems w/o problem. Compiled is back? Seems like everyone is trying to get ahead of tool calling moving people \"off platform\" and creating differentiators around what tools are available \"locally\" to the models etc.  This also takes the wind out of the sandboxing folks, as it probably won't be long before the \"local\" tool calling can effectively do anything you'd need to do on your local machine. I wonder when they'll start offering virtual, persistent dev environments... Not sure if this is still working. I tried get", "positive": "Take the pedals off the bike. Apparently this guys is unaware that pedalless balance bikes for kids already exist, and are quite popular. The idea is to get kids used to doing the hard part--staying balanced--first, then when they get a 'real' bike, they don't need training wheels, or at least, not so much. This is the standard way that kids learn to ride bikes in Europe.  Apparently the English word for them is \"balance bikes\".  Both my kids could ride one of them when they were 2. Watching your kid learn to ride a bike is one of life's greatest joys If your kid is using a balance bike, be sure to take them somewhere flat like a playground. If the ground is even slightly inclined, the kid cannot coast, and they spend their energy inefficiently pushing themselves and their bike forward. Training wheels aim to maximise the utility of the bicycle (i.e. gears and pnumatic tyres) for a person of certain age, at the cost of learning how to actually ride a bike. I feel there are lots of parallels in e.g. Maths education in the more generalised form: In education, skills that allow you to utilise technology are prioritised and these are often directly opposed to skills needed for mastery. > Bicycles achieve balance through the gyroscopic effect No they don't. If that were true you could cycle along at a 45 degree angle. They balance because you steer into a fall. Now we need a tutorial how to take the pedals off the bike.\nSo you do not damage the crank because of the opposite threading left vs right. The gyroscopic effect contributes little to maintaining a balanced bike ride, contrary to the article claim. An idealized massless wheel/tire wouldn't diminish ridability. Steering dynamics (steering to counteract bike lean) and trail effect (bike are built to automatically counteract lean), along with rider input (steering, leaning body), are more important components. Sheldon Brown wrote about this years ago:  https://www.sheldonbrown.com/teachride.html  > The ideal bike for", "negative": "The browser is the sandbox. I like the perspective used to approach this. Additionally, the fact that major browsers can accept a folder as input is new to me and opens up some exciting possibilities. The folder input thing caught me off guard too when I first saw it. I've been building web apps for years and somehow missed that `webkitdirectory` attribute. What I find most compelling about this framing is the maturity argument. Browser sandboxing has been battle-tested by billions of users clicking on sketchy links for decades. Compare that to spinning up a fresh container approach every time you want to run untrusted code. The tradeoff is obvious though: you're limited to what browsers can do. No system calls, no arbitrary binaries, no direct hardware access. For a lot of AI coding tasks that's actually fine. For others it's a dealbreaker. I'd love to see someone benchmark the actual security surface area. \"Browsers are secure\" is true in practice, but the attack surface is enormous compared to a minimal container. We never say that it isn't. There is a reason Google developed NaCl in the first place that inspired WebAssembly to become the ultimate sandbox standard. Not only that, DOM, JS and CSS also serves as a sandbox of rendering standard, and the capability based design is also seen throughout many browsers even starting with the Netscape Navigator. Locking down features to have a unified experience is what a browser should do, after all, no matter the performance. Of course there are various vendors who tried to break this by introducing platform specific stuff, but that's also why IE, and later Edge (non-chrome) died a horrible death There are external sandbox escapes such as Adobe Flash, ActiveX, Java Applet and Silverlight though, but those external escapes are often another sandbox of its own, despite all of them being a horrible one... But with the stabilization of asm.js and later WebAssembly, all of them is gone with the wind. Sidenote: Flash's script"}
{"anchor": "Eat Real Food. Makes sense to me! And poor diet is probably one of the biggest problems in the United States Makes sense. Now make protein affordable. And 100 years from now, will we still call it the New Pyramid? :) I guess we still call it New York... Great!  How will the reductions in consumer protection, health, FDA, etc. - by this current administration impact that?  https://www.food-safety.com/articles/11004-a-2025-timeline-o...  This website is far too complicated, just show a clear, labeled image of the new pyramid.  This is designed to scare people, not inform them. Lol good one. Anything matching . real .\\.gov$ can be discarded as BS these days... Edit: Actually make that simply .*\\.gov$ It's unbelievable to which point this clown show has permanently dismantled US soft power. Guess they think they have enough hard power to compensate. What with all that good raw milk and meat they're eating... Ironic that a steak is one of the three things showing up on the landing page. Is that the beef lobby money coming in? I enjoy an occasional steak but if the goal is to improve diet of masses, it\u2019s not the food I\u2019d put at the center. \"In February 2010, Michelle Obama launched \u201cLet\u2019s Move!\u201d with a wide-ranging plan to curb childhood obesity. The campaign took aim at processed foods, flagged concerns about sugary drinks, and called for children to spend more time playing outside and less time staring at screens. The campaign was roundly skewered by conservatives... But the strategy that Kennedy\u2019s HHS is using to address the problem so far\u2014pressuring food companies to alter their products instead of introducing new regulations\u2014is the same one that Obama relied on, and will likely fall short for the same reason hers did a decade ago.\"  https://www.theatlantic.com/health/archive/2025/09/maha-lets...  Meta comment: The design aesthetic gives me a real \"Cards Against Humanity\" feel. > Whole grains are encouraged. Refined carbohydrates are not. Prioritize fiber-rich whole g", "positive": "Iran Goes Into IPv6 Blackout. Seems like v4s zeroed out for a tiny bit too, but even now they are substantially lower than normal. Odd behavior, I don't know if its a precursor to an attack or some infra issue No competent network engineer wants to work in Iran, so government doesn't know how to block v6 properly. End result: just get rid of it entirely! Stuxnet v2? Speculation I know, but wow, IPv4 came back up, but IPv6 is completely out, looks like 48 million devices? Compared to IPv4's 47 thousand (wow that's insane). Looking at IPv6 its not 0 exactly, looks like probably censorship, only some devices allowed online? Some other comment mentioned there's calls to protest again today. Teredo/Miredo would work on top of IPV4. Not specific to this IPv6 event, but I was wondering what happens to public services during these Internet shutdowns? Does everything stop or it's mostly business as usual minus some things? I would imagine hospitals, tax offices etc need the internet to work? Fortunately, the government cannot enforce complete blackout because thousands of startlink terminals are active inside the country. They have been complaining about it [1] to no avail. Using these terminals activists and journalists continue to upload videos of demonstrations to social media which has enabled analyses that show demonstrations are very wide spread [2] and continue to grow. [1]  https://www.itu.int/en/ITU-R/conferences/RRB/Pages/Starlink....  [2]  https://www.bbc.com/news/articles/cre28d2j2zxo  Government enacted shut down due to protests. I'd like to hear more about how they actually do this.\n https://www.cbsnews.com/news/iran-cutting-internet-amid-dead...  Can't wait for a certain dictator to get a cellmate, so that our Persian and Kurdish friends can have freedom, including free unrestricted internet access. And for fellow HN users from there, here's some great stuff:  https://yggdrasil-network.github.io/   https://bitchat.free/  Map of protests:  https://pouyaii.githu", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
{"anchor": "How Google Maps allocates survival across London's restaurants. Interesting work, but ultimately silly: of course google maps ranks results.  No one (yes, yes, I'm sure like 3 people) want a list of all results, unordered or ordered by something useless like name, when they type in restaurant.  And I cannot put into words how uneager I am to have the city or state government manage what comes up when I put indian or burrito into a map. The other commenter thought the work was silly, but I think it's brilliant. Keep at this!! You're making me hungry :) super interesting project. I would love to generate a similar list for my own neighbourhood I love the idea! And I want to have it for my city :) Is there a project on GitHub or somewhere that I could clone??  (smiling face with halo)  Very interesting. But I wonder how much Google (and other) Maps can actually shape the scene. For tourist hotspots with a lot of visitors, it IS clearly the driving force. But for locals, I don\u2019t think it has an overwhelming effect. Locals know their restaurants and they visit them based on their own rating. They could explore total strange and new ones, but then they will form their own rating and memory immediately and will not get fooled/guided by algorithm (the next time) Google maps is doing the same thing to local business success that social media algorithms are doing to political success. The algorithm controls what you perceive as the consensus of others. It is a dangerous world to have such power so highly concentrated. At least in central London, the \"underrated gems\" feature does not seem to be very good at finding underrated gems. That might just be a feature of the area though. I have gotten so sick of Google Maps that I've done the unthinkable, and have started walking around the city trying establishments at random. It has yielded quite good results basically immediately. People (myself included) have gotten too used to living In The Box. Putting aside the time to just go", "positive": "A few random notes from Claude coding quite a bit last few weeks. The section on IDEs/agent swarms/fallibility resonated a lot for me; I haven't gone quite as far as Karpathy in terms of power usage of Claude Code, but some of the shifts in mistakes (and reality vs. hype) analysis he shared seems spot on in my (caveat: more limited) experience. > \"IDEs/agent swarms/fallability. Both the \"no need for IDE anymore\" hype and the \"agent swarm\" hype is imo too much for right now. The models definitely still make mistakes and if you have any code you actually care about I would watch them like a hawk, in a nice large IDE on the side. The mistakes have changed a lot - they are not simple syntax errors anymore, they are subtle conceptual errors that a slightly sloppy, hasty junior dev might do. The most common category is that the models make wrong assumptions on your behalf and just run along with them without checking. They also don't manage their confusion, they don't seek clarifications, they don't surface inconsistencies, they don't present tradeoffs, they don't push back when they should, and they are still a little too sycophantic. Things get better in plan mode, but there is some need for a lightweight inline plan mode. They also really like to overcomplicate code and APIs, they bloat abstractions, they don't clean up dead code after themselves, etc. They will implement an inefficient, bloated, brittle construction over 1000 lines of code and it's up to you to be like \"umm couldn't you just do this instead?\" and they will be like \"of course!\" and immediately cut it down to 100 lines. They still sometimes change/remove comments and code they don't like or don't sufficiently understand as side effects, even if it is orthogonal to the task at hand. All of this happens despite a few simple attempts to fix it via instructions in CLAUDE . md. Despite all these issues, it is still a net huge improvement and it's very difficult to imagine going back to manual coding. TLDR ev", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
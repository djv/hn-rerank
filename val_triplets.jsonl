{"anchor": "Project Genie: Experimenting with infinite, interactive worlds. Google Deepmind Page:  https://deepmind.google/models/genie/  Try it in Google Labs:  https://labs.google/projectgenie  (Project Genie is available to Google AI Ultra subscribers in the US 18+.) This could be the future of film. Instead of prompting where you don't know what the model will produce, you could use fine-grained motion controls to get the shot you are looking for. If you want to adjust the shot after, you could just checkpoint the model there, by taking a screenshot, and rerun. Crazy. Every character goes forward only, permanence is still out of reach apparently. Reminds me of this [1] HN post from 9 months ago, where the author trained a neural network to do world emulation from video recordings of their local park \u2014 you can walk around in their interactive demo [2]. I don't have access to the DeepMind demo, but from the video it looks like it takes the idea up a notch. (I don't know the exact lineage of these ideas, but a general observation is that it's a shame that it's the norm for blog posts / indie demos to not get cited.) [1]  https://news.ycombinator.com/item?id=43798757  [2]  https://madebyoll.in/posts/world_emulation_via_dnn/demo/  I keep on repeating myself, but it feels like I'm living in the future.\nCan't wait to hook this up to my old Oculus glasses and let Genie create a fully realistic sailing simulator for me, where I can train sailing with realistic conditions. On boats I'd love to sail. If making games out of these simulations work, it't be the end for a lot of big studios, and might be the renaissance for small to one person game studios. What\u2019s the endgame here? For a small gaming studio, what are the actual implications? I have been confused for a long time why FB is not motivated enough to invest in world models, it IS the key to unblock their \"metaverse\" vision. And instead they let go Yann LeCun. This would be really cool if polished and integrated with VR. I don't", "positive": "In New York City, congestion pricing leads to marked drop in pollution. > Particulates issued from tailpipes can aggravate asthma and heart disease and increase the risk of lung cancer and heart attack. Globally, they are a leading risk factor for premature death. Minor nitpick, but tailpipes aren't the primary source of emissions. The study is about PM2.5[0]. which will chiefly be tires and brake pads. Modern gasoline engines are relatively clean, outside of CO2, though diesel engines spit out a bunch of bad stuff. [0]  https://www.nature.com/articles/s44407-025-00037-2  See also  https://news.ycombinator.com/item?id=46213504  There was a study published about how much air pollution dropped in NYC during the COVID lockdown. PM2.5 was found to have dropped 36%. However with more robust analysis, this drop was discovered to not be statistically significant. I would caution anyone reading this who is tempted by confirmation bias. Source:  https://pmc.ncbi.nlm.nih.gov/articles/PMC7314691/  To head off the almost inevitable recapitulation of yesterday's parade of misinformed complaints by teenage libertarians, please actually read the paper before commenting. The paper shows there was no significant reduction in entries to the congestion charge zone by cars, vans, and light trucks. And you can confirm this conclusion is consistent with their source data using their github repo. The reduction in pollution is coming from the significant decline in heavy truck traffic. Truckers were using lower manhattan as a cut-through route to other places and they are now doing that less, exactly as congestion pricing planners long argued. Not surprising. The real question is how do we measure the opportunity cost of these measures? Is it a net gain? You could, at the extreme, ban all motor vehicles but the opportunity cost would outweigh the benefits. This article confirms my existing bias/belief that user pays and auction[0] based systems improve governmental programs and finite supp", "negative": "Richard Feynman Side Hustles. This would be cool if only it made sense. If you want to read the replies without an account:  https://xcancel.com/carl_feynman/status/2016979540099420428   https://nitter.net/carl_feynman/status/2016979540099420428  So do you have to be a god tier Nobel Laureates to get this kind of gig where you just learn about a business and then offer random suggestions that might or might not help them and charge obscene fees for the privilege? People are giving such bizarre examples for why it helped. Just think of a thermometer. If it removes heat as it measures it (consumes oxygen) then it will measure everything too cold if the system can't replace the heat that's removed (this is like having an insulated thermometer). If your thermometer replaces heat as it removes it it solves this issue. When is this an issue for a thermometer? If your thermometer is too large in terms of heat capacity for the objects you're measuring the temperature of. I had to read this twice to understand it. Stated succinctly, it sounds like the company's sensor measured the rate of flow of oxygen through the sensor, which would give a reduced reading if the cross section is obstructed. Feynman's sensor, by contrast, directly measured the concentration of oxygen in the sensor, which gives the same result every time once the sensor is at equilibrium with the environment. I feel like the company might have been Yellow Springs Instrument (YSI, now a division of Xylem). The dissolved oxygen sensor (the Clark electrode) was invented by Dr. Lee Clark at Antioch University (Yellow Springs, OH) and commercialized by YSI in the 1960s. A friend of mine worked at YSI from the late 60s thru the 80s on biosensors (glucose and lactic acid, using the Clark electrode as the basis) and worked directly with Dr. Clark. Carl Feynman was born in 1962, according to what I'm reading, so if he was 14 that would have put this story in the time period early in the commercialization of these sen"}
{"anchor": "Show HN: OpenTimes \u2013 Free travel times between U.S. Census geographies. Looks cool. Please allow high max zoom levels, it\u2019s hard to see individual street details on mobile. Amazing! GitHub actions to compute a giant skim matrix is an incredible hack. I pretty regularly work with social science researchers who have a need for something like this... will keep it in mind. For a bit we thought of setting something like this up within the Census Bureau, in fact. I have some stories about routing engines from my time there... Well done, dfsnow! * some islands seem hamstrung by the approach - see Vashon Island for example. * curious what other dataset you might incorporate for managing next level of magnitude smaller trips - e.g. getting a quarter mile to the store for a frozen pizza at the seventh inning stretch. Any plans on adding public transit? It seems that it ignores bridges over rivers making the travel time wildly inaccurate. This is great! I've been thinking about building something like this for ages since I started using Smappen [0] for mapping travel times on road trips. Super useful way to travel if you're on an open-ended trip with flexibility. [0]  https://www.smappen.com/  OK the way you're publishing the data with Parquet and making it accessible through DuckDB is  spectacular . Your README shows R and Python examples:  https://github.com/dfsnow/opentimes?tab=readme-ov-file#using...  I got it working with the `duckdb` terminal tool like this:     INSTALL httpfs;\n  LOAD httpfs;\n  ATTACH 'https://data.opentimes.org/databases/0.0.1.duckdb' AS opentimes;\n\n  SELECT origin_id, destination_id, duration_sec\n    FROM opentimes.public.times\n    WHERE version = '0.0.1'\n        AND mode = 'car'\n        AND year = '2024'\n        AND geography = 'tract'\n        AND state = '17'\n        AND origin_id LIKE '17031%' limit 10;   Travel time context in general could be useful for retrieval before ranking in searches like Yelp or Google maps like products for nearby events a", "positive": "OpenClaw \u2013 Moltbot Renamed Again. Previously:  Clawdbot Renames to Moltbot   https://news.ycombinator.com/item?id=46783863  Right now I'm just thinking about all the molt* domains..... \u00af\\_(\u30c4)_/\u00af I would have stood my ground on the first name longer. Make these legal teams do some actual work to prove they are serious. Wait until you have no other option. A polite request is just that. You can happily ignore these. The 2nd name change is just inexcusable. It's hard to take a project seriously when a random asshole on Twitter can provoke a name change like this. Leads me to believe that identity is more important than purpose. and openclaw.com is a law firm. It's hilarious that atm I see \"Moltbook\" at the top of HN. And it is actually not Moltbot anymore? But I have to admit that OpenClaw sounds much better. This is indeed feeling very much like Accelerando\u2019s particular brand of unchecked chaos. Loving every minute of it, first thing in our timeline that makes sense where it regards AI for the masses :) What if Lamborghini had acquired Claw to automate their vehicles? amateur hour, new phase of the AI bubble reminds me of Andre Conje, cracked dev, \"builds in public\", absolutely abysmal at comms, and forgets to make money off of his projects that everyone else is making money off of (all good if that last point isn't a priority, but its interrelated to why people want consistent things) Should have named it \u201cbot formerly known as Moltbot\u201d and invented a new emoji sigil :) Apparently it had another name before Clawdbot as well, I think BotRelay or something. It\u2019s on pragmatic engineer Hilarious to see the most pointless vibecoded slop written to interact with an RDP server. Unnecessary introduces loopholes. How to annoy and alienate your target audience in 2 short weeks. Before using make sure you read this entirely and understand it:\n https://docs.openclaw.ai/gateway/security \nMost important sentence: \"Note: sandboxing is opt-in. If sandbox mode is off\"\nDon't do that, ", "negative": "We have ipinfo at home or how to geolocate IPs in your CLI using latency. This is a little project exploring the feasibility of using a service such as Globalping for geo location needs. I had fun making it but please note that the current implementation is just a demo and far from a proper production tool. If you really want to use it then for best possible results you need at least 500 probes per phase. It could be optimized fairly easily but not without going over the anon user limit which I tried to avoid Bit surprised this works. Latency variability is huge and sometimes quite disconnected from geo location. I recall talking to someone in NL and realised I've got better latency to NL content from the UK than he did. Presumably better peering etc. How feasible would it be for the host under measurement to introduce additional artificial latency to ping responses, varying based on source IP, in order to spoof its measured location? Amazing idea and execution, the sort of stuff I wish there was more of on HN. Tried with an IP allocated to a major wireless network operator. It was far off but also ran out of credits when trying with higher limits on subsequent attempts. Seems tool is relying on ICMP results from various probes. So wouldn't this project become useless if target device disables ICMP? I wonder if you can \"fake\" results by having your gateway/device respond with fake ICMP requests. Wi-FI RTT is more accurate than trilateration with RSSI but requires hw support. IEEE 802.11mc > Wi-Fi Round Trip Time (RTT) \n https://en.wikipedia.org/wiki/IEEE_802.11mc#Wi-Fi_Round_Trip...  /? fine time measurement FTM:  https://www.google.com/search?q=fine+time+measurement+FTM  > Globalping is an open-source, community-powered project that allows users to self-host container-based probes. These probes then become part of our public network, which allows anyone to use them to run network testing tools such as ping and traceroute. How's this different from RIPE ATLAS? > Gro"}
{"anchor": "Ironwood: The first Google TPU for the age of inference. It looks amazing but I wish we could stop playing silly games with benchmarks. Why compare fp8 performance in ironwood to architectures which don't support fp8 in hardware? Why leave out TPUv6 in the comparison? Why compare fp64 flops in the El Capitan supercomputer to fp8 flops in the TPU pod when you know full well these are not comparable? [Edit: it turns out that El Capitan is actually faster when compared like for like and the statement below underestimated how much slower fp64 is, my original comment in italics below is not accurate] ( The TPU would still be faster even allowing for the fact fp64 is ~8x harder than fp8. Is it worthwhile to  misleadingly claim it's 24x faster instead of honestly saying it's 3x faster? Really? ) It comes across as a bit cheap. Using misleading statements is a tactic for snake oil salesmen. This isn't snake oil so why lower yourself? Can these be repurposed for other things? Encoding/decoding video? Graphics processing etc? edit:\n>It\u2019s a move from responsive AI models that provide real-time information for people to interpret, to models that provide the proactive generation of insights and interpretation. This is what we call the \u201cage of inference\u201d where AI agents will proactively retrieve and generate data to collaboratively deliver insights and answers, not just data. maybe i will sound like a luddite but im not sure i want this. I'd rather AI/ML only do what i ask it to. Some honest competition in the chip space in the machine learning race! Genuinely interested to see how this ends up playing out. Nvidia seemed 'untouchable' for so long in this space that its nice to see things get shaken up. I know they aren't selling the TPU as boxed units, but still, even as hardware that backs GCP services and what not, its interesting to see how it'll shake out! The first specifically designed for inference? Wasn\u2019t the original TPU inference only? Not knowing much about special-pur", "positive": "OracleGPT: Thought Experiment on an AI Powered Executive. Considering things like Palantir, and the doge effort running through Musk, it seems inconceivable that this is not already the case. I think I'm more curious about the possibility of using a special government LLM to implement direct democracy in a way that was previously impossible: collecting the preferences of 100M citizens, and synthesizing them into policy suggestions in a coherent way. I'm not necessarily optimistic about the idea, but it's a nice dream. This is an interesting and thoughtful article I think, but worth evaluating in the context of the service (\"cognitive security\") its author is trying to sell. That's not to undermine the substance of the discussion on political/constitutional risk under the inference-hoarding of authority, but I think it would be useful to bear in mind the author's commercial framing (or more charitably the motivation for the service if this philosophical consideration preceded it). A couple of arguments against the idea of singular control would be that it requires technical experts to produce and manage it, and would be distributed internationally given any countries advanced enough would have their own versions; but it would of course provide tricky questions for elected representatives in the democratic countries to answer. A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION. think we're already there aren't we? no human came out with those tariffs on penguin island The really nice thing about this proposal is that at least now we can all stop anthropomorphizing Larry Ellison, and give Oracle the properly robot-identifying CEO it deserves. You sometimes hear people say \"I mean, we can't just give an AI a bunch of money/important decisions and expect it to do ok\" but this is already happening and has been for years. Examples: - Algorithmic trading: I once embedded on an Options trading desk. The head of desk mentioned ", "negative": "First, make me care. \"First, make me care\" is exactly right. But I also know that anytime you have narrative non-fiction on here, someone without fail argues that the author didn't get straight into the details. This is something I find fascinating about TikTok: on that platform you literally get a few seconds to catch the attention of your audience before they skip to the next video. You can't just find one hook that works and reuse it forever because people will get bored of it - including if that hook is heavily used by other accounts. This makes TikTok a fascinating brute-force attack on human psychology, with literally millions of people all trying to find the right hooks to catch attention and constantly evolving and iterating on them as the previous hooks stop being effective. This was quite a good article. It could have been excellent if it answered its own hook somewhere the piece though. I came away not having a resolution to the hook - violating the articles second principle. I can\u2019t click on any links on pages (the header works). Using brave on iPhone. Firefox and Safari works\u2026 Suppose you fed this article into an LLM, along with whatever other documents you had, and asked it to come up with some good candidates for opening sentences? And picked one, and let it take it from there? I assume you'd get a mess, but it might be an interesting mess. I think \"Just\u2026 start with the interesting part first\" is quite different, and actually much better advice than \"make me care\". I'm more than done with stupid hooks and attention grabbing techniques, just plainly and honestly state at the outset what the point is of what will follow. This article succeeded spectacularly in making me want to know all there is to know about medieval Venice, that's for sure. \"When writing, your first job is this:\nFirst, make me care.\" It really depends on who the audience is... I have often thought that all good fiction is mystery. This is obviously an overstatement, but I think it\u2019s n"}
{"anchor": "Linux boxes via SSH: suspended when disconected. This looks quite similar to exe.dev which was on here a while ago - anyone know how it compares? $36/mo for 2/4/50 VPS without public IP... Ok, I get the idea that the service is for non-regular use, but I think even $0.005 per hour ($3.6/mo) of suspended state is too expensive. The same config in Hetzner is just $4.09/mo for 24/7 working VPS with public IPv4 address This is fascinating idea.  I created an idea like this on top of firecracker and custom golang ssh client to build something like this for my own personal use case (the abstraction part of pricing and how to connect it seemed the more difficult part for me atleast) What stack does this use underneath? Good luck with launch, this idea is similar to railway in terms of pricing model. I discussed about it a few comments back and I think its an interesting idea and we are seeing alternatives within such pricing model Also are you using some cloud provider itself or building it yourself, I'd be interested in so many details to discover Have a nice day and looking forward to ya response! Good luck with your project! > Note: The -O flag is required for OpenSSH 9.0+ to use legacy SCP protocol. Why isn't SFTP supported? Is it non-American all the way down? I've been trying to come up with a hypothetical use case for this. I can't use this as a server without keeping an active session right? I wonder if you could get around this by sshing into itself from inside the primary session. Is that an edge case you've considered? Not sure about the security sandbox, but given that paddle.com (your payment provider) takes 5% cut you could consider accepting lightning (bitcoin layer2) payments. QR code generation for lightning invoice is instantaneous just as payment, and will cost less than 0.1% fee (payer pays fee anyway). But the security sandbox should be solid, else it will be used for illegal stuff. But why? Genuinely want to know what one might use this for. I can ima", "positive": "Toronto\u2019s network of pedestrian tunnels. Once on a lunch break I walked from St Andrew to King (parallel stations on the horns of line 1) in the tunnels and took the TTC back. Going overground is usually faster and easier to navigate, buts impressive how far you can go underground. One of these days I\u2019ll need to try an extreme point hike. One thing that I was surprised wasn't mentioned is the impact that I believe weather must have had on the development of the Path. Winters in Toronto get rather cold and snowy. Even with a dense downtown core, walking a few blocks outside can be rather unpleasant. I use to love exploring Path as a teenager. More northern cities like Montreal and Winnipeg also have very interesting indoor pedestrian systems. The one in Winnipeg is particularly useful, since there are approximately 72 hours per year that it's comfortable to be outside between the bone-chilling cold and the biblical swarms of mosquitos and flies in the summer.  https://en.wikipedia.org/wiki/Underground_City,_Montreal   https://en.wikipedia.org/wiki/Winnipeg_Walkway  We also have a 5K race in the PATH! [1] In the winter the tunnels are amazing for commute. [1]  https://www.bougebouge.com/en/shop/events/5km-bougebouge-tor...  >  Montreal has a similar system, while Tokyo, Osaka, Seoul, Hong Kong, Singapore and Houston have systems that resemble the Path in some respects. A few European cities also make considerable use of pedestrian tunnels, including Helsinki, Stockholm and Munich.  Japan's northernmost major city, Sapporo, has a very extensive one -- of those I've seen, it's the one that's most comparable to Toronto's. The other Japanese tunnel/undercity complexes are mostly subterranean malls around subway stations.  (This also applies to all of the ones in Hong Kong.)  But Sapporo's is seriously huge. I think the common denominator is that people would rather walk in a heated underground space when it gets cold. It's my understanding that underground walkways were c", "negative": "Rust at Scale: An Added Layer of Security for WhatsApp. Very cool! I'm wondering if Signal is doing something similar? libsignal is implemented in Rust, but I don't know about the other parts. > We believe that this is the largest rollout globally of any library written in Rust. I suppose this is true because there's more phones using WhatsApp than there are say Windows 11 PCs. Given that WhatsApp uses libsignal, is it safe to assume that they haven't been using the Rust library directly? The hardest part of a rewrite like this is usually maintaining bug-for-bug compatibility with the legacy parser rather than the actual Rust implementation. Most real-world media files are malformed in some way that the C++ code implicitly handled, so if you write a strict parser you end up breaking valid user data. Differential fuzzing seems like the only practical way to map that behavior without manually reviewing millions of edge cases. Quite impressive, I did not know so many bugs were due to memory access. > Two major hurdles were the initial binary size increase due to bringing in the Rust standard library [...]. They don't say what they did about it, do they? Did they just accept it? Cool - now we only need to get selling-you-out-for-profit-Zuckerberg out of WhatsApp to make it really trustworthy. Just like Google\u2019s Rust-in-Android blogs this reads like a PR piece (and in the case of facebook also recruitment piece) with some technical words sprinkled in for effect. The overall communication quality is that of a random startup\u2019s \u201clook what we did\u201d posts. The interesting aspects, such as how they protect against supply-chain attacks from the dependency-happy rust toolchain or how they integrated the C++ code with the Rust code on so many platforms - a top challenge as they said - remain a mystery. Would also be interesting to hear how much AI-driven development they used for this project. My hope\u2019s that AI gets really good at Rust so one doesn\u2019t have to directly interact with"}
{"anchor": "X For You Feed Algorithm. anything interesting? anything that is a surprise? what is the difference between this and  https://github.com/twitter/the-algorithm  I did not expect to see Rust. They seem to have forgotten to commit Cargo.toml though. Oh I see it is not meant to be built really. Some code is omitted. ooh, LLM Recsys alert! (we had an LLM Recsys track at ai.engineer last year). official announcement here:  https://x.com/XEng/status/2013471689087086804  looks like this is the \"for you\" feed, once again shared without weights so we only have so much visibility into the actual influence of each trait. \"We have eliminated every single hand-engineered feature and most heuristics from the system. The Grok-based transformer does all the heavy lifting by understanding your engagement history (what you liked, replied to, shared, etc.) and using that to determine what content is relevant to you.\" aka it's a black box now. the README is actually pretty nice, would recommend reading this. it doesnt look too different form Elon's original code review tweet/picture  https://x.com/elonmusk/status/1593899029531803649?lang=en  sharing additonal notes while diving through the source:  https://deepwiki.com/xai-org/x-algorithm  and a codemap of the signal generation pipeline:  https://deepwiki.com/search/make-a-map-of-all-the-signals_3d...  - Phoenix (out of network) ranker seems to have all the interesting predictive ML work. it estimates P(favorite), P(reply), P(repost), P(quote), P(click), P(video_view), P(share), P(follow_author), P(not_interested), P(block_author), P(mute_author), P(report) independently and then the `WeightedScorer` combines them using configurable weights. there's an extra DiversityScore and OONScore to add some adjustments but again dont know the weights  https://deepwiki.com/xai-org/x-algorithm/4.1-phoenix-candida... \n- other scores of interest: photo_expand_score, and dwell_score and dwell_time. share via copy, share, and share  via dm are all obvi", "positive": "AI just proved Erdos Problem #124. This seems to be 2nd in row proof from the same author by using the AI models. First time it was the ChatGPT which wrote the formal Lean proof for Erdos Problem #340.  https://arxiv.org/html/2510.19804v1#Thmtheorem3  > In over a dozen papers, beginning in 1976 and spanning two decades, Paul Erd\u0151s repeatedly posed one of his \u201cfavourite\u201d conjectures: every finite Sidon set can be extended to a finite perfect difference set. We establish that {1, 2, 4, 8, 13} is a counterexample to this conjecture. Ok\u2026 has this been verified? I see no publication or at least an announcement on Harmonics webpage. If this is a big deal, you think it would be a big deal, or is this just hype? Related, independent, and verified: GPT-5 solved Erd\u0151s problem #848 (combinatorial number theory):  https://cdn.openai.com/pdf/4a25f921-e4e0-479a-9b38-5367b47e8...   https://lifearchitect.ai/asi/  More interesting discussion than on Twitter here:  https://www.erdosproblems.com/forum/thread/124#post-1892  This is response from mathematician:\n\"This is quite something, congratulations to Boris and Aristotle! On one hand, as the nice sketch provided below by tsaf confirms, the final proof is quite simple and elementary - indeed, if one was given this problem in a maths competition (so therefore expected a short simple solution existed) I'd guess that something like the below would be produced. On the other hand, if something like this worked, then surely the combined talents of Burr, Erd\u0151s, Graham, and Li would have spotted it. Normally, this would make me suspicious of this short proof, in that there is overlooked subtlety. But (a) I can't see any and (b) the proof has been formalised in Lean, so clearly it just works! Perhaps this shows what the real issue in the [BEGL96] conjecture is - namely the removal of 1 and the addition of the necessary gcd condition. (And perhaps at least some subset of the authors were aware of this argument for the easier version allowing 1", "negative": "California is free of drought for the first time in 25 years. As John Steinback said in  East of Eden : \u201cI have spoken of the rich years when the rainfall was plentiful. But there were dry years too, and they put a terror on the valley. The water came in a thirty-year cycle. There would be five or six wet and wonderful years when there might be nineteen to twenty-five inches of rain, and the land would shout with grass. Then would come six or seven pretty good years of twelve to sixteen inches of rain. And then the dry years would come, and sometimes there would be only seven or eight inches of rain. The land dried up and the grasses headed out miserably a few inches high and great bare scabby places appeared in the valley. The live oaks got a crusty look and the sage-brush was gray. The land cracked and the springs dried up and the cattle listlessly nibbled dry twigs. Then the farmers and the ranchers would be filled with disgust for the Salinas Valley. The cows would grow thin and sometimes starve to death. People would have to haul water in barrels to their farms just for drinking. Some families would sell out for nearly nothing and move away. And it never failed that during the dry years the people forgot about the rich years, and during the wet years they lost all memory of the dry years. It was always that way.\u201d The dams in california were built years ago for a smaller population and since then they've only removed them. If we simply built like the people who first came to california did we would never have water shortages again. Any water shortage is a 1:1 failure of the state to do the clear and obvious task needed. strange because this is one of the warmest winters in decades. snow levels are far below normal, i saw 8% of normal in truckee. full reservoirs now are great but keeping them filled depends on a long snow melt going into june. i don\u2019t think this is going to be a good year for that And yet our water rates are still as if we are in a drought. Previ"}
{"anchor": "Reading across books with Claude Code. This is all interesting, however I find myself most interested in how the topic tree is created. It seems super useful for lots of things. Anyone can point me to something similar with details? EDIT: Whoops, I found more details at the very end of the article. Discussed earlier this week:  https://news.ycombinator.com/item?id=46567400  In several years, IMO the most interesting people are going to be the ones still actually reading paper books and not trying to shove everything into a LLM This was posted before and there were many good criticisms raised in the comments thread. I'd just reiterate two general points of critique: 1. The point of establishing connections between texts is  semantic  and terms can have vastly different semantic meanings dependent on the sphere of discourse in which they occur. Because of the way LLMs work, the really  novel  connections probably won't be found by an LLM since the way they function is quite literally to uncover what  isn't novel . 2. Part of the point in making these connections is the  process  that acts on the human being making the connections. Handing it all off to an LLM is no better than blindly trusting authority figures. If you want to use LLMs as generators of possible starting points or things to look at and verify and research yourself, that seems totally fine. I really like the idea of the topic tree. That intuitively resonates. I did a similar thing with productivity books early last year, but never released it because it wasn't high enough quality. I keep meaning to get back to that project but it had a much more rigid hypothesis in mind - trying to get the kind of classification from this is pretty difficult and even more so to get high value from it. The mental model I had of this was actually on the paragraph or page level, rather than words like the post demos. I think it'd be really interesting if you're reading a take on a concept in one book and you can immediatel", "positive": "Grid: Free, local-first, browser-based 3D printing/CNC/laser slicer. Surprised this hasn't been shared here before. Built by my former colleague, Stewart Allen (Co-Founder/CTO of WebMethods, CTO of AddThis, Co-Founder/CPO of IonQ, et al.). What caught my attention: - 100% free, no subscriptions, no accounts, no cloud - Local-first: all slicing and toolpath generation runs on your machine - Works in any browser, even offline once loaded - Supports FDM/SLA, CNC milling, laser cutting, wire EDM - Fully open source: github.com/GridSpace/grid-apps Refreshing to see a tool that isn't trying to lock you into a subscription or harvest your data. Now if we can only get an offline printer\u2026 I've used kiri:moto for several simple CNC projects! This probably won't scroll to the correct place on the page but there's some images of my project at  https://hcc.haus/propmania/#2024-palm-torches  and  https://static.cloudygo.com/static/Prop%20Making/2024%20Palm...  I used it instead of the terrible closed source Easel App for a CARVEY hobby CNC. For metal milling I find Fusion 360 is necessary. More open source, browser-accessible tools is a good thing. That said, aren't Prusa/Orca/etc. all already open-source (and part of the same lineage)? Am I weird in not being too surprised? It don't have experience with wire EDM but every toolpath generator or slicer I've ever used was just local software. This looks great. I was hoping it would have been a good OrcaSlicer replacement for my FDM printer, but unfortunately it didn't generate any top surfaces (except for the topmost one) for a model I imported in. I didn't know if it was the printer profile (Creality.Ender3) or something else, but it seems I'm still using OrcaSlicer for the time being. Great tool for a Makerspace - really appreciate the ability to use the same tool for laser cutting, 3d printing, and CNC.  These are big jumps for people typically - having a familiar tool would help people transition from one area to another. OT: W", "negative": "California is free of drought for the first time in 25 years. As John Steinback said in  East of Eden : \u201cI have spoken of the rich years when the rainfall was plentiful. But there were dry years too, and they put a terror on the valley. The water came in a thirty-year cycle. There would be five or six wet and wonderful years when there might be nineteen to twenty-five inches of rain, and the land would shout with grass. Then would come six or seven pretty good years of twelve to sixteen inches of rain. And then the dry years would come, and sometimes there would be only seven or eight inches of rain. The land dried up and the grasses headed out miserably a few inches high and great bare scabby places appeared in the valley. The live oaks got a crusty look and the sage-brush was gray. The land cracked and the springs dried up and the cattle listlessly nibbled dry twigs. Then the farmers and the ranchers would be filled with disgust for the Salinas Valley. The cows would grow thin and sometimes starve to death. People would have to haul water in barrels to their farms just for drinking. Some families would sell out for nearly nothing and move away. And it never failed that during the dry years the people forgot about the rich years, and during the wet years they lost all memory of the dry years. It was always that way.\u201d The dams in california were built years ago for a smaller population and since then they've only removed them. If we simply built like the people who first came to california did we would never have water shortages again. Any water shortage is a 1:1 failure of the state to do the clear and obvious task needed. strange because this is one of the warmest winters in decades. snow levels are far below normal, i saw 8% of normal in truckee. full reservoirs now are great but keeping them filled depends on a long snow melt going into june. i don\u2019t think this is going to be a good year for that And yet our water rates are still as if we are in a drought. Previ"}
{"anchor": "How AI assistance impacts the formation of coding skills. Go Anthropic for transparency and commitment to science. Personally, I\u2019ve never been learning software development  concepts  faster\u2014but that\u2019s because I\u2019ve been offloading actual development to other people for years. Nice to see an AI coding company allow such studies to come out, and it looks decently designed The title of this submission is misleading, that's not what they're saying. They said it doesn't show productivity gains for inexperienced developers still gaining knowledge. I've noticed this as well. I delegate to agentic coders on tasks I need to have done efficiently, which I could do myself and lack time to do. Or on tasks which are in areas I simply don't care much for, for languages which I don't like very much etc An important aspect of this for professional programmers is that learning is not something that happens as a beginner, student or \"junior\" and then stops. The  job  is learning, and after 25 years of doing it I learn more per day than ever. I wonder why these Anthropic researchers chose GPT-4o for their study. Key snippet from the abstract: > Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. The library in question was Python trio and the model they used was GPT-4o. @dang the title here is bait. I\u2019d suggest the paper title: \u201cAnthropic: How AI Impacts Skill Formation\u201d I\u2019ve been making the case (e.g.  https://youtu.be/uL8LiUu9M64?si=-XBHFMrz99VZsaAa  [1]) that we have to be intentional about using AI to augment our skills, rather than outsourcing understanding: great to see Anthropic confirming that. [1] plug", "positive": "Ask HN: Best Podcasts of 2025?. BetterOffline [0] by Ed Zitron [1] dissecting AI hype and boosters. By a long shot. The information density and clarity are outstanding. 0:  https://www.betteroffline.com/  1:  https://www.wheresyoured.at/  My gotos for listening while I do chores or drive this year have been:       - Stuff You Should Know https://stuffyoushouldknow.com/\n    - How to do Everything https://www.npr.org/podcasts/510384/how-to-do-everything   The Rest is History is good, depending on the topic. Both guests have a bit of bias which you have to sort of take into account, not that different from The Rest is Politics.\nMishal Husain has a new podcast on Bloomberg TV which so far was excellent.\nAlso from Bloomberg TV, Big Take is often interesting.\nI still enjoy Lex Fridman, again depending on the guest. Dwarkesh Patel same shit as Lex, but he pretends he knows something about AI. Hardcore History 73 - Mania for Subjugation III [1] Fall of Civilizations 20 - Persia - An Empire in Ashes [2] [1]  https://www.dancarlin.com/product/hardcore-history-73-mania-...  [2]  https://fallofcivilizationspodcast.com/  Most of my podcasts are movie related. If I had to purge them all and start with just 5 though I would go with. Blank Check\nThe Flophouse\n99% Invisible\nCautionary Tales\nThe Rewatchables I maintain The Flophouse is the funniest podcast around. Advent of Computing:     https://adventofcomputing.com/\n\n  https://podcasts.apple.com/us/podcast/advent-of-computing/id1459202600\n\n  https://adventofcomputing.libsyn.com/rss\n\n  https://www.youtube.com/@adventofcomputing4504/videos   Call me simple or provincial, but I really enjoyed \"Good Hang\" from Amy Poehler. It's a breezy interview with interesting people (doesn't hurt that I'm a long-time SNL fan). Citations Needed:  https://citationsneeded.libsyn.com/       Acquired (Long episodes about companies, recents include: \n  coca-cola, trader joe's & alphabet)\n  Dwarkesh Podcast (Inquisitive curious host, mostly \"AGI\"\n  relat", "negative": "The browser is the sandbox. I like the perspective used to approach this. Additionally, the fact that major browsers can accept a folder as input is new to me and opens up some exciting possibilities. The folder input thing caught me off guard too when I first saw it. I've been building web apps for years and somehow missed that `webkitdirectory` attribute. What I find most compelling about this framing is the maturity argument. Browser sandboxing has been battle-tested by billions of users clicking on sketchy links for decades. Compare that to spinning up a fresh container approach every time you want to run untrusted code. The tradeoff is obvious though: you're limited to what browsers can do. No system calls, no arbitrary binaries, no direct hardware access. For a lot of AI coding tasks that's actually fine. For others it's a dealbreaker. I'd love to see someone benchmark the actual security surface area. \"Browsers are secure\" is true in practice, but the attack surface is enormous compared to a minimal container. We never say that it isn't. There is a reason Google developed NaCl in the first place that inspired WebAssembly to become the ultimate sandbox standard. Not only that, DOM, JS and CSS also serves as a sandbox of rendering standard, and the capability based design is also seen throughout many browsers even starting with the Netscape Navigator. Locking down features to have a unified experience is what a browser should do, after all, no matter the performance. Of course there are various vendors who tried to break this by introducing platform specific stuff, but that's also why IE, and later Edge (non-chrome) died a horrible death There are external sandbox escapes such as Adobe Flash, ActiveX, Java Applet and Silverlight though, but those external escapes are often another sandbox of its own, despite all of them being a horrible one... But with the stabilization of asm.js and later WebAssembly, all of them is gone with the wind. Sidenote: Flash's script"}
{"anchor": "Mecha Comet \u2013 Open Modular Linux Handheld Computer. This project is currently seeking funds (and is funded) on Kickstarter:  https://www.kickstarter.com/projects/mecha-systems/mecha-com...  (and the super early bird rewards are all gone) I might be interested if I weren't still waiting on the Soulcircuit Pilet to ship.... Stumbled across this thing a while back and thought it looked really cool but I have never been able to come up with an idea for how I would use it so I haven't pledged. I want to want it but I fear it would just sit on my desk. Does anyone have cool ideas for uses? I am planning to build something similar as a hobby project except my idea is that Claude Code runs everything on the device for you. So... what's the OS situation? From a glance at  https://github.com/mecha-org/linux  ->  https://github.com/mecha-org/linux/commits/imx/lf-6.12.20/  it  looks  like they're starting from a 6mo-old kernel (6.12.20 vs current LTS 6.12.67 and current stable 6.18.7). Is there any reason to expect upstreaming or even just consistent updates, or is this yet another device that will ship with an old-ish kernel and never get updated again? Small gimmicky computers seem to attract so much attention and people who can\u2019t help themselves but buy it, play with it for a while, then toss it into a drawer and never use it again. So it\u2019s a Raspberry Pi except now I can type Unix commands with my thumbs on a blackberry keyboard\u2026\u2026ouch. This reminds me of Phonebloks from 13 years ago.  https://de.wikipedia.org/wiki/Phonebloks  Website's a bit weird.  The app icons highlight when you hover over them, but don't seem to do anything. They've got a grab-bag of unrelated Linux etc. org icons - Nix, Debian, postmarketOS, Node, Kubernetes\u2026  You could argue that someone _could_ run Nix or Node on it, but Debian is just nerdbait.  It's not relevant to the product they're selling, unless you're gonna wipe the disk and support it yourself. Open Hardware + Open Software is good enough fo", "positive": "'The old order is not coming back,' Carney says in speech at Davos. Yup, the middle powers have to organize and work together to avoid being chum.  The economic power is there, and they can shift from purchasing US weaponry (thus paying US workers) into purchasing middle-power weaponry (thus paying middle-power workers).  Car/truck plants can be repurposed, and if Ukraine's lesson is valid then smaller, portable weaponry is now the preferred solution.  Cheaper, and the middle powers don't have huge investments in tanks and ships. The Theucydides quote Carney leads with, of course, recently rolled off the tongue of the white house deputy chief of staff, Stephen Miller.  The days of might making right are, apparently, back. Just in case anyone thought the genie could be stuffed back into the bottle once Trump is gone, Carney goes on to state that the rules-based world order we've been living under since WWII is somewhat of a sham.  The rules have not been applied equally.  Some nations, the powerful ones, have been given much more latitude to do what they want. Middle nations have gone along with this to avoid trouble. The reward for avoiding trouble for so long is...  big  trouble (e.g. invasion threats for an ally of a big power and economic terrorism applied to its allies).  So, why pretend the old system works to avoid trouble if the trouble lands on your doorstep anyways? The answer seems obvious.  Middle powers of the old rules-based order need to band together and put bigger powers in their place.  It's not impossible.  Just very, very difficult.  France and Germany may be sticking up for Greenland, but where's Hungary (another EU member)?  For this to work, you need  everyone . Also, looking ahead, how would you prevent such an alliance of smaller powers, were it successful, from behaving like a bigger power? Trump is currently showing off AI photos where he's meeting with world leaders in front of a map where both Greenland and Canada are a part of the U.S.[1", "negative": "Heathrow scraps liquid container limit. Not because of a sudden outbreak of sanity, but because they have CT scanners now. FINALLY (PS.  Still not going to fly there) Good. This should happen on all airports now. Otherwise it's useless. You won't be flying from Heathrow to Heathrow. The security theater needs to go on. In the meantime batteries represent a much bigger risk with potential in flight fires but I guess nobody cares enough to do anything about it. Let me get this straight. If the article is correct, the new capabilities are related to better detection of large liquid containers, not determination of whether or not the liquid is dangerous. So - you couldn\u2019t take large amounts of liquids previously because some liquids in large amounts might be able to be weaponized. If you were caught with too much liquid (in sum total, or in containers that are too large) they\u2019d throw it out and send you on your way. But now that they have the ability to detect larger containers, they\u2026 do what? Declare that it\u2019s safe and send you on your way with it still in your possession? This rule wasn't enforced anyway... I travel a lot - and never take out any liquids. Have nail clippers and scissors in my carry-on. Once I even had an opinel pocket knife in my laptop bag for a couple of months. Travelled through Tokyo, Taipei, SFO, DEN, PHX, LAX, BOS, JFK, FRA, AMS, MUC, LHR - nobody noticed. I seriously had forgotten it was there, so I don't do that now, but still... Also, no large water bottles or similar. Unless on domestic flights in Japan, where this is totally fine. IDK - security theater. But if it helps. Famously Steve Jobs had a story about shaving time off of boot-up and equating it to saving lives on the concept of people sitting their waiting for the computer to boot up just lost that much of their lives. [1] I actually do believe there is value in thinking this way and it is one of my biggest arguments against TSA. Everything has a cost, including 'security' and 'safet"}
{"anchor": "Broken legs and ankles heal better if you walk on them within weeks. It was ~20 years ago, so my memory is a little foggy, but I gave myself a \"dancer's fracture\" in one foot. After many months, it was looking like a non-union. The podiatrist was worried any pin would split the broken bone even more. It wasn't looking good. I had read something along these lines even back then, so with my crazy immobilizer boot on, I head to the gym and started doing light squats several times per week. Next x-ray: healed. The pathology for broken collar bones was changing right as I took up mountain biking, and subsequently shattered my collarbone. It was hotly debated at the hospital, if my specific case should be operated on or not. Each time I had a checkup, one doctor would say \"wait and see\" while the other was saying \"I can't believe we didn't operate on this\". At any rate, the outcome was as good as if they had operated on it, according to the doc anyway. Nice of them to test it out on me! More related to this though, I have broken both my collarbones, the first time I had little direction and just held my arm still for 2-3 months. It took forever to heal, and my arm atrophied significantly. The second time, similar severity. I was guided through rehab and I was back using my arm within the first month, very little atrophy. I fractured my elbow mountain biking, the tip of my radius. The urgent care doctor gave me a sling and suggested months of immobility. The orthopaedic said to throw away the sling and start exercising the elbow as soon as I could, and prescribed PT. Turns out that was the right move, there are some permanent changes to mobility but it's about 97% what it was before the crash. Immobilizing joints can apparently cause the muscles, tendons, and nerves to seize up and lose significant range of movement permanently. If anyone's heard of RICE (Rest, Ice, Compression, Elevation) for healing joints, the new guidance is called POLICE: Protect, Optimal Load, Ice, C", "positive": "Ask HN: What are some of your favorite documentaries?. My second favorite \u201cWild, Wild, Country\u201d, but it was mentioned already at the top of the first list.  I enjoy it as a cautionary tale, but I also unironically find it an inspiring tale of building, even if it turns out to bad. My favorite documentary is \u201cThe Barkley Marathons, the race that eats it\u2019s young\u201d I return to it at least once a year, and while the root of the story - watching people attempt the impossible is certainly inspiring, I find its moral themes are what I appreciate about it the most.  The idea of competition as a collective activity, that everyone wants to win, but also everyone wants to see others win their own race, that there\u2019s something about the way that it advances our understanding of humanity that is more important than individual success. Then also - that your race is yours alone, and that the most important victory is the one you define for yourself.  There are people who finish only one or three laps of the five lap marathon, and that failure is a greater achievement than most people will ever know, and they clearly see it that way, there\u2019s near no shame in anyone\u2019s performance and people are clearly defining success for themselves, mostly clearly beyond what anyone else would define it for them.  And finally, it\u2019s kind of a throwaway line, but one of the runners says \u201cI think most people could use more pain in their lives.\u201d And it made me realize that often, when enduring hardship, rather than turning away from it, finding ways to challenge myself on my terms is a healthier approach to stress than \u201crelaxing\u201d. Le Joli Mai  https://www.youtube.com/watch?v=iOj0sPmJssw  Here's another previous post:  https://news.ycombinator.com/item?id=25624456  which includes an answer of mine. To add to that list: [1] \"Andermatt - Global Village\" - tracks the construction of a luxury resort in the Swiss village of Andermatt and how it affects people there. The village and the project still make the ", "negative": "Mermaid ASCII: Render Mermaid diagrams in your terminal. I love ASCII diagrams! The fact that I can write a diagram that looks equally wonderful in my terminal via cat as it does rendered on my website is incredible. A good monospaced font and they can look really sharp! I will definitely give this tool a shot. I will also shout out monodraw as a really nice little application for building generic ASCII diagrams-  https://monodraw.helftone.com/  > Aesthetics \u2014 Might be personal preference, but wished they looked more professional Im sold. Love mermaid but totally agree. The live demo requires some download of an AI agent platform? I'd really like to try this but not if that's what's required. Pair this with Unicode plots[0] and you're set! [0]:   https://github.com/JuliaPlots/UnicodePlots.jl  How is the LaTeX compatibility? Base mermaid's LaTeX compatibility is quite sparse. See also graph-easy.online ( https://github.com/cjlm/graph-easy-online ) Wow! It has this:     Subgraph Direction Override: Using direction LR inside a subgraph while the outer graph flows TD.\n  \nWith this, you should be able to approximate swim lane diagrams, which is something Mermaid lacks. The last time I checked, Mermaid couldn't render subgraphs in a different direction than the overall graph. The actual Mermaid ASCII renderer is from another project [0]. This project transliterated it to typescript and added their own theming. [0]:  https://github.com/AlexanderGrooff/mermaid-ascii  I've had issues with other CLI wrappers there.  ASCII output is a nice touch for including diagrams directly in code comments without breaking formatting.  Does it handle large graphs well, or does the text wrap get messy?  We tried using `graph-easy` for this before but the syntax was annoying.  6. This is great, I will definitely make use of this! I get a sense of deja vu. There was another such project posted within the last 3 months, and another within last 6 months. I should have bookmarked them, because a"}
{"anchor": "Show HN: Z80-\u03bcLM, a 'Conversational AI' That Fits in 40KB. This is super cool. Would love to see a Z80 simulator set up with these examples to play with! For future projects and/or for this project, there are many LLMs available more than good enough to generate that kind of synthetic data (20 Qs) with permissive terms of use. (So you don\u2019t need to stress about breaking TOS / C&D etc) Imagine, this working on a Gameboy, in those days. Would've sounded like magic An LLM in a .com file? Haha made my day Awesome. I've just designed and built my own z80 computer, though right now it has 32kb ROM and 32kb RAM. This will definitely change on the next revision so I'll be sure to try it out. If one would train an actual secret (e.g. a passphrase) into such a model, that a user would need to guess by asking the right questions. Could this secret be easily reverse engineered / inferred by having access to models weights - or would it be safe to assume that one could only get to the secret by asking the right questions? This couldn't be more perfectly timed .. I have an Unreal Engine game with both VT100 terminals (for running coding agents) and Z80 emulators, and a serial bridge that allows coding agents to program the CP/M machines:  https://i.imgur.com/6TRe1NE.png  Thank you for posting! It's unbelievable how someone sometimes just drops something that fits right into what you're doing. However bizarre it seems. In before AI companies buy up all the Z80s and raise the prices to new heights. interesting, i am wondering how far can it go if we remove some of these limitations but try to solve some extremely specific problem like generating regex based on user input? i know small models(270M range) can do that but can it be done in say < 10MB range? Nice - that will fit on a Gameboy cartridge, though bank switching might make it super terrible to run. Each bank is only 16k. You can have a bunch of them, but you can only access one bank at a time (well, technically two - bank 0", "positive": "How to live on $432 a month in America. I've often felt this way about some of today's complaints. I grew up in area like what was mentioned in this article and I long for the day I can go back there. I would in a heartbeat if my partner shared the same mentality as me. I don't really see a point in living a big city with the remote job I have and that many others have if I can live in a smaller area that still has humans but much cheaper way of living. Everyone claims it's about living in a city with available services but I see those same people decry how much the food costs and also that they have no friends and can't find someone to date. My thoughts aren't as articulate as I'd like them to be but I guess I'm ultimately trying to say is if I'm going to be miserable, why not do it on my own land for a lot cheaper. It makes a certain amount of sense and I myself bought a little place way out in the hinterlands of Michigan for similar economic reasons ... but I live in Berkeley because subjecting your children to life without opportunities for art, culture, education, sports, friends, etc is cruel. So if you're white, or just don't care that your ethnicity is absent, and if you have no children, and also don't mind living in a car-dependent place where the public transit to the nearest major city is a minimum of 15 hours with 3-4 transfers, then sure Massena NY is dope. There is a little bit of a sleight of hand going on in this article by claiming the lifestyle of boomers is within reach, but then actually using boomers' parents and grand-parents as the standard. It would be more honest to say \"Most of us can't have the relative wealth of our grand parents, but with some sacrifices and creativity, the lifestyle of our great-grand parents is attainable.\" Even that is only true in a very narrow sense. My great-grand parents built a 600sqft house in a small town and lived their most of their lives. But they built that house right next to their parents. They lived wit", "negative": "The C-Shaped Hole in Package Management. Please don't.  C packaging in distros is working fine and doesn't need to turn into crap like the other language-specific package managers.  If you don't know how to use pkgconf then that's your problem. very related:  https://michael.orlitzky.com/articles/motherfuckers_need_pac...  >  Conan and vcpkg exist now and are actively maintained I am not sure if it is just me, but I seem to constantly run into broken vcpkg packages with bad security patches that keep them from compiling, cmake scripts that can't find the binaries, missing headers and other fun issues. I don't trust any language that fundamentally becomes reliant on package managers.  Once package managers become normalized and pervasively used, people become less thoughtful and investigative into what libraries they use.  Instead of learning about who created it, who manages it, what its philosophy is, people increasingly just let'er rip and install it then use a few snippets to try it.  If it works, great.  Maybe it's a little bloated and that causes them to give it a side-eye, but they can replace it later....which never comes. That would be fine if it only effected that first layer, of a basic library and a basic app, but it becomes multiple layers of this kind of habit that then ends up in multiple layers of software used by many people. Not sure that I would go so far as to suggest these kinds of languages with runaway dependency cultures shouldn't exist, but I will go so far as to say any languages that don't already have that culture need to be preserved with respect like uncontacted tribes in the Amazon.  You aren't just managing a language, you are also managing process and mind.  Some seemingly inefficient and seemingly less powerful processes and ways of thinking have value that isn't always immediately obvious to people. Missing in this discussion is that package management is tightly coupled to module resolution in nearly every language. It is not enoug"}
{"anchor": "Leaving the U.S. for the Netherlands. Leaving now is the best way to ensure things get worse. If you've given up, your vote no longer counts and your voice no longer matters. To learn how to leave USA one needs to pass the newyorker paywall. Coming from Switzerland to the US: I do not want to leave. Everything is better here\u2026 maybe not for the average person but for some individuals. This is the moment for all those Hollywood personalities and other liberals to learn how to leave for real, not just talk about it! If things deteriorate in US there will not be places to leave to pretty soon Come to the Netherlands! It\u2019s awesome. And the visa is easy: just put 5k in a business account. Look up Dutch American friendship treaty. Great Moments in UX Below the teaser blurb ending \"The Netherlands offers one way out,\" and the byline, where you'd expect the article to start, is the text \"Your window is closing.\" Fortunately, if you scroll further, the ominous warning turns out to only be for the paywall.  https://i.imgur.com/4WT4S8u.png  And go where? Seriously I don\u2019t know of another country that isn\u2019t on the same authoritarian track, if not further along. If anyone has done a serious study and come up with a country that still has strong judicial independence, due process, lack of censorship and respect for private property, Id love to know Lol. I know what the comments will be. And the US is one of the top immigrant countries in the world. Always worth reflecting why people choose that when there is greener gras. There are much better places in the world to move to than the Netherlands Went for dinner with a Parisian friend of mine. He spent a good amount of it complaining that Paris is unrecognizable from his youth. Too many Americans, everywhere! Better explain to me how to get to the USA. I love the Netherlands and have spent a few months trying it out as a place to live. It's among my favourite places: moderate weather, friendly people, a high level of personal freedo", "positive": "Early Retirement May Speed Up Cognitive Decline: Study. Anecdotally, my grandfather is 92 or so and still works as a journalist (reduced hours). He is still super sharp and does yoga every day. Blows my mind. I hope not, I recently retired. Still, the idea makes some sense. In retirement, I try to read one paper a day (usually deep learning, PGM, or classic AI), play at least one game of Go and Chess, do some recreational programming, and read. But, I don\u2019t work into a state of brain-tiredness anymore like I used to at work. My dad is a doctor in his 70s. He works 60 hour weeks (which he claims counts as retirement for doctors). He truly believes that true retirement is suicide. He wants to be found dead while doing rounds at the hospital. I've always benchmarked post-retiring cognitive abilities and professional continuity with Noam Chomsky. He is my hero in that aspect too. If I can continue to do what I do now at his age, I'm ready for the off. Not only early retirement but any kind of retirement that gets the retiree in a mode that they don't have to try anymore will result in cognitive decline. I am seeing this in my dad who has been retired for ten years now. Throughout his work life he was a sharp hard working banker. Now he uses his age and retirement as an excuse for not trying. Just yesterday he wanted me to order something for him from Amazon. I told him to send me the link to the item. He asked me how to do that. I told him if you can't find the Share link just copy the link and send it to me. He responds by saying that he doesn't know how to do copy-paste. He has been using computers for at least the last fifteen years. I asked him how come he didn't know how to copy-paste. His response was - I am retired now and there's nobody to tell me or teach me. I can see the cognitive decline. Things he used to be able to do, he can't anymore. This type of attitude is also affecting his self respect and confidence. This is something I think about a lot. I plan to", "negative": "American importers and consumers bear the cost of 2025 tariffs: analysis. > Event studies around discrete tariff shocks on Brazil (50%) and India (25\u201350%) confirm: export prices did not decline. Trade volumes collapsed instead. What if that was the intended result? who could possibly have foreseen this This is the case with any tax, it's mostly paid by the consumer. American trade policy has gone so far in the direction of Mercantilism that both the Neoliberal and the Keynesian economists can agree on something. That's not a good thing. We will see if SCOTUS majority decides tariffs are a tax or not and push the absurdity of their position even farther. I fear that they already decided that issue when they chose not to intervene and now have the excuse of \"lol well can't undo it now\" ready to go. Edit:  It appears Trump & Co intend to replace SCOTUS if they lose the tariffs ruling ...  https://www.nytimes.com/2026/01/19/us/politics/trump-tariffs...  -------- There does seem to be indications that the actual tariffs collected seems far lower than the actual tariffs promised, likely just half of what was promised:  https://www.nytimes.com/2026/01/03/business/economy/trump-ta...  Isn't this literally economics 101? How did we ever even end up imagining that tariffs are somehow paid by the exporter?? How could that possibly have not been the case.  A tariff is no different from the cost of any input into the price of a finished good.  There is some sense in which price increases are limited by supply and demand, but if the market won't pay for the production cost of the good, then the market will cease to provide that good.  There are only two possible outcomes, long term -- either the price goes up, or the product becomes unavailable. There's an argument that domestically produced goods would substitute for imported goods leaving the market, but markets are so global and intertwined now that even domestic goods have imported inputs that are also affected by tariffs, an"}
{"anchor": "Greenland sharks maintain vision for centuries through DNA repair mechanism. Sharks are so cool, man. They\u2019ve just been chilling on the planet for 400 million years, swimming the oceans while epochs passed them by in their periphery. Their entire biology is pretty much unchanged. They\u2019ve been sharks the whole time. This is so messed up harvesting the eye from a creature that lives hundreds of years. I guess they put the shark down. RIP one eye. It\u2019s sinful to fish and kill these ancient creatures up from the deep for minor scientific progress. So wait did they just catch a 200 year old shark and cut its eye ball out to have a look? Highly recommend the book \"Shark Drunk: The Art of Catching a Large Shark from a Tiny Rubber Dinghy in a Big Ocean\" by Morten Str\u00f8ksnes if you're interested in old sharks, small boats or deep oceans  https://bookshop.org/p/books/shark-drunk-the-art-of-catching...  I\u2019m starting to realise we don\u2019t really want a cure to aging. Imagine a world where people like Stalin never die. People like bill gates never have to pretend to be a nice person\u2026 If there\u2019s no chance of death, there will never be any progress in society. People in power would just establish a tighter and tighter grip. All the boomers would be immune to death and disease, but the treatment would be banned for the young because they haven\u2019t done enough to earn it. Unfortunately it also seems like these sharks are plagued by parasites in their eyes:  The shark is often infested by the copepod Ommatokoita elongata, a crustacean that attaches itself to the shark's eyes.[17] The copepod may display bioluminescence, thus attracting prey for the shark in a mutualistic relationship, but this hypothesis has not been verified.[18] These parasites can cause multiple forms of damage to the sharks' eyes, such as ulceration, mineralization, and edema of the cornea, leading to almost complete blindness.[11] This does not seem to reduce the life expectancy or predatory ability of Greenland shar", "positive": "European word translator: an interactive map. Love that the numbers in Catalan are represented as numerals, not as words. EDIT: playing with it, it's a bit sad that large numbers do not work at all (in any language); and that not all common forms of a word are shown.  For example, I tried to see how \"ninety six\" is said in french in France, Belgium and Switzerland, but it does not work. Ukrainian and russian words often use the same letters but are pronounced very differently due to distinct phonetics. On the other hand, some Polish and Czech words sound the same or very similar to Ukrainian but look quite different because of their different alphabets. Therefore, phonetic transcription would be a valuable improvement. You immediately see the difference (or similarly) of languages when using words that are very old, such as \"iron\", or \"stone\", which are words that have existed from the origins of that language. I can mostly speak for German. It seems to mix them all into one general language. But there are a lot of local differences between north and south of Germany, Switzerland and Austria. And it\u2019s not just dialect, but really different words that might not be understood everywhere. \nIf you look at the english part it has at least three different words. Similar in Spanish. There are examples from five language families shown here: Indo-European, Basque, Uralic, Turkic, and Afro-Asiatic. The words for bridge split neatly into language subfamilies.   The only exception appears to be Welsh. You are coloring it by 4 colors like map but you should color countries phonetically (speex, levenshtein or something similar) Wiktionary has dialect maps for common Chinese vocabulary that showcases the differences in terminology across various regions of Chinese, rather than their similarities. Example: sleep ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E7%9D%A...  , hide-and-seek ->  https://en.wiktionary.org/wiki/Template:zh-dial-map/%E6%8D%8...  p.s. I'm saying t", "negative": "Find 'Abbey Road when type 'Beatles abbey rd': Fuzzy/Semantic search in Postgres. I was just starting to learn about embeddings for a very similar use on my project. Newbie question: what are pros/cons of using an API like gpt Ada to calculate the embeddings, compared to importing some model on Python and running it locally like in this article? Great post. Explains the concepts just enough that they click without going too deep, shows practical implementation examples, how it fits together. Simple, clear and ultimately useful. (to me at least) I found fuzzy search in Manticore to be straightforward and pretty good. Might be a decent alternative if one perceives the ceremony in TFA as a bit much. The rewritten title is confusing imo. Can I propose: \u201cFinding \u2018Abbey Road\u2019 given \u2018beatles abbey rd\u2019 search with Postgres\u201d these days i find myself yearning to type \"Beatles abbey rd\" and find only \"Beatles abbey rd\" for 50,000 rows I'd much rather just use fzf/nucleo/tv against json files instead of dealing with database schemas. \nWhen it comes to dealing with embedding vectors rather than plaintext then it gets slightly more annoying but still feels like such an pain in the ass to go full database when really it could still be a bunch of flat open files. More of a perspective from just trying to index crap on my own machine vs building a SaaS > Abbey Road > The Dark Side of the Moon > OK Computer Those are my 3 personal records ever. I feel so average now... tl,dr: A demo of pg_trgm (fuzzy matcher) + pgvector (vector search). FWIW, the performance considerations section is a little simplistic, and probably assumes that exact dataset/problem. For GIN for example, perfomance depends a lot on the size of the search input (the fewer characters, the more rows to compare) as well as the number of rows/size of the index. It also mentions GiST (another type of index which isn't mentioned anywhere else in the article).. On the API vs local model question: We went with API embedding"}
{"anchor": "Alexei Navalny has died. Jailed Russian opposition leader Alexei Navalny is dead, the prison service of the Yamalo-Nenets region where he had been serving his sentence said on Friday. \u201cDied\u201d, I suspect the more accurate term is \u201cmurdered\u201d\u2026 Unbelievable. The world just watches. I can't believe any American would carried Putin's water after the treatment of Alexei Navalny. Yeah Navalny is dead but have you looked at their shopping carts? > Putin has been informed of the death, says Kremlin In advance, one assumes. Frankly, it's strange Putin allowed him to live for so long, normally he just kills his opponent quite fast. Maybe it was just a power show to make everybody understand he can control his opponents' lives completely. A little reminder of what Putin has been up to in the last few years. - Annexation of Crimea (2014) - MH17 Downing (2014) - Intervention in Syria - 2016 U.S. Election Interference - Skripal Poisoning (2018) - Anti-LGBTQ+ Laws - Navalny Poisoning (2020) - Wagner Group Activities - Invasion of Ukraine (2022) - Killing of Yevgeny Prigozhin (2023) - Killing of Alexei Navalny  (2024) What is necessary for US and European Laws, to specify any type of contact, endorsement, indulgence even, of such a regime, is an intolerable criminal offense? Edit: Its difficult to keep track... - Killing of Alexander Litvinenko - 1999 Russian Apartment Bombings - September 2022 \u2014 Ravil Maganov's fatal fall from a hospital window. He was chairman of Russian oil giant Lukoil. Lukoil was the first major Russian company to call for an end to the war in Ukraine - July 2009 \u2014 Natalya Estemirova found dead in a ditch - October 2006 \u2014 Anna Politkovskaya murdered in an elevator - April 2003 \u2014 Sergei Yushenkov murder was never solved. Yushenkov was one of the harshest critics of the Chechen war and the KGB's successor organization, the FSB. The 2022 documentary 'Navalny' is important and explains how the anti-corruption campaigner got to that terrible place, being poisoned with", "positive": "Attention Wasn't All We Needed. I know this probably seems like such a small detail to a lot of people, but I really love that the author adds comments. I can't stand reading PyTorch or other neural network code and asking myself, \"What architecture am I looking at here?\" or \"What the hell are these operations for?\" It's always like an mash up of reading some published paper code with deep effort behind it along with all the worst programming practices of complete unreadability. This is an excellent summary of these techniques :) I like that every single one comes with an example implementation, with shape comments on the tensors. Thanks Stephen! > Let's look at some of the most important ones that have been developed over the years and try to implement the basic ideas as succinctly as possible. One big architectural tweak that comes to mind and isn't in the article is QK norm:  https://arxiv.org/pdf/2010.04245  > Cosine Schedule A lot (most?) of new training runs actually don't use cosine schedule anymore; instead they keep the learning rate constant and only decay it at the very end, which gives equivalent or better results. See:  https://arxiv.org/pdf/2405.18392 \n https://arxiv.org/pdf/2404.06395  > There is a highly optimized implementation of AdamW in PyTorch. A fun tidbit - it's actually not highly optimized from my experience. Imagine my surprise when I reimplemented it in Triton (because I needed to tweak a few things) and I got better performance than the built-in PyTorch implementation. The explanation for Multi-head Latent Attention  https://www.stephendiehl.com/posts/post_transformers/#multi-...  does  not  match the definition in the DeepSeek-V2 paper  https://arxiv.org/pdf/2405.04434#subsection.2.1  MLA as developed by DeepSeek is a technique to reduce the memory footprint of the KV cache by storing only two vectors of size  latent_dim  and  rope_dim  per token and layer, instead of 2 *  num_heads  vectors of size  head_dim . (DeepSeek-V3 has  num_head", "negative": "The age of Pump and Dump software. Pump and dump software is a hilarious phrase but I thought it would have meant something slightly different. My idea of pump and dump software is the proliferation ai-generated sites (Vercel links) that are sent to the 404 graveyard after a few days of someone not getting any traction on it. Maybe there should be a term for when an industry is at its wits ends so far gone that crypto scams are viable. Noticed the same. Doing a quick analysis of clawdbot myself I figured there are many spam domains that are used to backlink. Now there is a new domain being advertised as a replacement of the original. It points to the same landing page though it is hard to say if this comes from the original authors. All of it seems to be related to a crypto scheme. The astroturfing on reddit is also pretty bad. This is obviously in a blip in the grand scheme of things but it is just an indication what all of these social media platforms are destined to become without some sort of intervention. The framing of the title makes me wonder what we as humans will think of software from this time 100s of years from now. Will the future be a complicated, dense ecosystem of interconnected intelligent systems, putting our current complexity to shame? Or in the future will we look at the current time as the Wild West, the time when software moved more swiftly than the law. Where oil was there for anyone with a big enough guns to protect it. Maybe we will experience our own butlerian jihad and realize that the thinking machines were controlling us the whole time. We will look at TikTok how we now look at the proliferation of ether in the 1800s. The \u201cHow it works\u201d section is an absolute mess.  Each bullet uses a different pronoun, so it\u2019s not clear who the actors actually are and how this all fits together.  How are the \u201ccrypto bros\u201d who approach the \u201ctech person\u201d related to the \u201cfame hungry tech bro\u201d that vibe-coded the failed app? I\u2019m sure there\u2019s a tremendous "}
{"anchor": "SpaceX lowering orbits of 4,400 Starlink satellites for safety's sake. From a comment : >The first move in the coming WWIII, where the emperors try to expand their empires militaril,y will be to wipe out any orbit with Starlink satellites. I find this highly unlikely, given Starlink is soon to reached 10k satellites and will continue to grow. Why expand 10 000 ballistic missiles to bring down one of many communications networks ? There are so many satellites in orbit that there is a pretty good chance that if even one was to be hit by something and explode in many pieces, it would crash another one and then another one until there is nothing left. The nasa is pretty scared of it, so is SpaceX. I think it's important to note that not all collisions are equally dangerous. Consider a sat on a polar orbit colliding with one on a equatorial orbit. Or two satellites on different directions.  That  is going to be spectacular. Otoh, these kind of collisions are unlikely and should be manageable by just assigning certain shells (say 5km) for every possible direction and orientation. If two Starlink satellites collide that go roughly in the same direction, it's not exactly a huge problem. I think the biggest issue is to coordinate this and potentially disallow some excentric orbits. What\u2019s the plan as the solar maximum returns? Can anyone explain how does one technically lower a satellite? 3 week old news OP? Previously:  https://news.ycombinator.com/item?id=46457454  Won't this make running Starlink more expensive? Lower orbits > Increased atmospheric drag > More fuel expended to maintain orbit > Heavier sats due to more fuel > Increased launch cost per unit Or even: Lower orbits > Increased atmospheric drag > Quicker orbit decay > Shorter lifespan of sats > More frequent launches Forgive my Kerbal-based space knowledge here. Because Kessler syndrome means you don't need to hit all 10k yourself. Lowering the orbits just means that we get back to normal faster, not that the i", "positive": "28M Hacker News comments as vector embedding search dataset. Oh to have had a delete account/comments option. I've been embedding all HN comments since 2023 from BigQuery and hosting at  https://hn.fiodorov.es  Source is at  https://github.com/afiodorov/hn-search  Am I misunderstanding what a parquet file is, or are all of the HN posts along with the embedding metadata a total of 55GB? I know it's unrelated but does anyone knows a good paper comparing vector searches vs \"normal\" full text search? Sometimes I ask myself of the squeeze worth the juice Scratches off one of my todos, I think it would be useful to add a right-click menu option to HN content, like \"similar sentences\", which displays a list of links to them. I wonder if it would tell me that this suggestion has been made before. Finetune LLM to post_score -> high quality slop generator I don't remember licensing my HN comments for 3rd party processing. Maybe I\u2019m reading this wrong, but commercial use of comments is prohibited by the HN Privacy and data Policy. So is creating derivative works (so technically a vector representation) I don't know how to feel about this. Is the only purpose of the comments here is to train some commercial model? I have a feeling that, this might affect my involvement here going forward. Don't use all-MiniLM-L6-v2 for new vector embeddings datasets. Yes, it's the open-weights embedding model used in all the tutorials and it  was  the most pragmatic model to use in sentence-transformers when vector stores were in their infancy, but it's old and does not implement the newest advances in architectures and data training pipelines, and it has a low context length of 512 when embedding models can do 2k+ with even more efficient tokenizers. For open-weights, I would recommend EmbeddingGemma ( https://huggingface.co/google/embeddinggemma-300m ) instead which has incredible benchmarks and a 2k context window: although it's larger/slower to encode, the payoff is worth it. For a compromi", "negative": "ICE using Palantir tool that feeds on Medicaid data. Any time I see people say \"I don't see why I should care about my privacy, I've got nothing to hide\" I think about how badly things can go if the wrong people end up in positions of power. The classic example here is what happens when someone is being stalked by an abusive ex-partner who works in law enforcement and has access to those databases. This ICE stuff is that scaled up to a multi-billion dollar federal agency with, apparently, no accountability for following the law at all. Wishful thinking but it would be real great if a future leader destroyed this infrastructure. I'm sure they'll run on not using it but when systems like this exist they tend to find applications Why would Medicaid have the data of anyone who is at risk of immigration enforcement? The reported connection seems tenuous: > The tool \u2013 dubbed Enhanced Leads Identification & Targeting for Enforcement (ELITE) \u2013 receives peoples\u2019 addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources. So, they have a tool that sucks up data from a bunch of different sources, including Medicaid.  But there's no actual nexus between Medicaid and illegal immigrants in this reporting. Edit: In the link to their earlier filings, EFF claims that some states enroll illegal immigrants in Medicaid:  https://www.eff.org/deeplinks/2025/07/eff-court-protect-our-...  This current administration and their policies have definitely influenced my opinion on the 2018 debate around citizenship questions on the US census. (For more context:  https://www.tbf.org/blog/2018/march/understanding-the-census... ) Glad to see this post didn't get flagged like the one that was posted yesterday on a similar topic about ICE data mining and user tracking.  https://news.ycombinator.com/item?id=46748336  \"ICE Budget Now Bigger Than Most of the Wo"}
{"anchor": "Douglas Adams on the English\u2013American cultural divide over \"heroes\". Stephen Fry made the same remarks in a Q&A session some years ago:  https://www.youtube.com/watch?v=8k2AbqTBxao  As a Brit I can't agree more with both, I find American humour so hard to relate to but I guess it's just a culture thing What a great response by Adams! I think the acceptance, and even the celebration of failure is present among the \u201cmaker\u201d community in the USA to some extent, which has really drawn me to it. I wonder if there\u2019s the same outlook on failure among other creatives, would be interesting to compare the hobby communities opinions between the USA and UK. I call this take pseudo-intellectual indulgence form, so called, academic intelectuais. Lord of the Rings is very much English Literature, and the biggest epic form the 20th century and has none of that. Ditto for Harry Poter (I\u2019m not saying Harry Potter is on the same level of literary grandeur as LOTR, but it\u2019s still an important epic series for newer generations). You can always find examples for one side or the other of the argument. But, of course, only \u201csocial\u201d scientists would be tick enough to claim some clear divide here as it suits their argument. Explains why Sir Keir Starmer is so relatable. Although I have very little experience with British humor, I find it interesting to compare British fiction I read as a child/teenager that became popular hits in the US (Harry Potter, Alex Rider). From this article's perspective, those protagonists are the epitome of American heroes (autonomy, mastery, purpose). No wonder they garnered such acclaim in the US. Curious if these stories are the exception rather than the rule in British YA fiction? Is the comparison unfair, since these stories were not written with the comedic genre in mind? I feel like the divide is very evident of each countries version of the show \"The Office\". Probably a common trope at this point, but not even the dialogue, already the aesthetic tells you a ", "positive": "Anthropic blocks third-party use of Claude Code subscriptions. This appears to be a part of a crackdown on third-party clients using Claude Code's credentials/subscriptions but not through Claude Code. Not surprising as this type of credential reuse is always a gray area, but weird Anthropic deployed it on a Thursday night without any warning as the inevitable shitstorm would be very predictable. No. Do you realize how much of a joke Claude code is? Under the hood. How they implemented client auth? Well let me tell you  https://github.com/anomalyco/opencode/blob/dev/packages/open...  You literally send your first message \u201cyou are Claude code\u201d The fact that this ever worked was insane. Headline is more like anthropic vibes a bug and finally catches it. I\u2019m not surprised they closed the loophole, it always felt a little hacky using an Anthropic monthly sub as an API with a spoofed prompt (\u201cYou are Claude Code, Anthropic's official CLI for Claude\u201d) with OpenCode. Google will probably close off their Antigravity models to 3P tools as well. Meanwhile, OpenAI co-signs  https://github.com/steipete/oracle  which lets you use your ChatGPT subscription to gain programmatic/agentic access to 5.2 Pro via automating browser access to the web frontend. Karpathy and other leaders have praised this feature on X. If that is indeed so welcome, imagine what else you could script via their website to get around Codex rate limits or other such things. After all what coud be so different about this than what browsers like Atlas do already Anthro is having its Apple moment: too many customers means the company is always on the news, for better or worse. When iPhones receive negative reviews it's not like only Apple screwed up; others did too, but they sell so much less than Apple that no one hears about them:       \"Apple violated my privacy a tiny bit\" makes the news;\n    \"Xiaomi sold my fingerprint info to 3rd party vendors\" doesn't.\n\n  \nSimilarly, Anthropic is under heavy fire recently", "negative": "Thief of $90M in seized U.S.-controlled crypto is gov't contractor's son. And he has been and continues to make fun of the investigators, publicly mocking investigators and sending small amounts from the fraudulent wallets to investigators. Crazy world There's no honor among thieves. A bit lost here. Is there more backstory to this? It reads as if the government contractors son stole the 90 million from the government? So much of the government is like this, they will hire some connected guy to manage something in a slightly competent manner. Just learned that the federal government has long term leases on office buildings that congressmen have a financial interest in. More disappointment. It appears the feds  were so incompetent they didn't realize the theft had occurred until AFTER Zach's post went viral, and even then, nothing may happen. And to think, had Lick done nothing he likely would have gotten away with it. Perfect crime undone by ego. There\u2019s another crypto thief that is the son of the head government official\u2026 Molly White really is quite thorough Tangent:  what does the govt do with seized crypto? Does it eventually get liquidated? Is the accusation the dad stole the crypto, or the dad AND the son stole the crypto ? Slightly tangential question but what\u2019s with govt seized crypto assets? I had a bit of Litecoin a while back and went to check my wallet one day to find an FBI landing page instead. Is that just theirs now? Feels a bit like the gov seized control of my savings account. Somewhat reminds me of this:\n https://www.justice.gov/archives/opa/pr/former-federal-agent...  > Two crypto thieves decided to settle an argument over who was wealthier by screensharing as they transferred crypto between wallets to prove ownership. In doing so, one of them \u2014 known online as \"Lick\" \u2014 revealed a wallet address that crypto sleuth zachxbt quickly tied to the theft of around $90 million from US government wallets containing seized crypto assets Rapp snitches. He\u2019s "}
{"anchor": "Mitochondria as you've never seen them. Wow! Really shows how mitochondria are actually just bacterial cells living inside us! Mitochondria are so absurdly more complex and interesting than what is mostly taught in schools.\nAwesome video! So each of our cells is a habitat for a network of wiggly energy-producing worms... Madeline L'Engle was right all along I studied an BSc in genetics and none of our lectures or textbooks presented mitochondria any differently from the classic bean shape they introduce in school. This is surely old news to folks who specialise in mitochondria, but it's easy to miss out on these fundamentals even if you've studied in a relevant area at degree level, because there's just so much to know in biology. In fact, it's one of those fields where the more you learn, the more you realise we'll never reach a satisfactory understanding in our lifetime. You could chuck an endless supply of PhD students at every constituent domain for generations and still feel like you've scarcely scratched the surface of the many things there are to question. To think I've spent hours upon hours each week for years and year with the express goal of producing more of these in the muscle cells of my legs, and I call this novel goal \"exercise\". Without this symbiotic relationship in cellular life on other planets, would it prevent complex cellular life? So beautiful and so sad to think about how much more interesting biology is than what we can teach from textbooks. This makes me want to further explore the similarities in form and function of mitochondrial networks and mycelial networks. Really sucks that antibiotics, especially bacteriocidal ones, appear to target mitochondria as if they were bacteria. This mistargetting causes sometimes severe and long-lasting side effects. The Seven Daughters of Eve are alive and well, I see Is the link supposed to go to a slideshow? Does anyone here have a sense of what time frame the video covers? Like, is that real-time and ", "positive": "Day Fifteen of Iran's Nationwide Protests: Sharp Rise in Human Casualties. nationwide protests in the US  https://www.msn.com/en-us/news/us/protests-against-ice-plann...   https://www.nytimes.com/2026/01/10/us/ice-shooting-protests-...   https://www.cnn.com/2026/01/11/us/ice-protests-shootings-min...  Israel is openly committed [0] to seeing regime change in Iran; they were mowing down civilian and military leadership just last year. The US got involved. There is the history of western involvement [1] in overthrowing Iranian governments. With that background I'm more worried about what the US's role here will be rather than what may or may not be taking place in Iran. My understanding is that the simulations around an invasion of the country were even more disastrous than the excursions in Afghanistan and Iraq and we really could use some signals of competence out of the US right now. We seem to be dangerously far into a WWI or WWII style environment internationally and we're already past the threshold of nuclear risk that sane actors would accept. [0]  https://en.wikipedia.org/wiki/Iran%E2%80%93Israel_war  [1]  https://en.wikipedia.org/wiki/Iran#Mosaddegh_and_the_Shah's_...  The US and AU both have told citizens to get out of Iran in the last 24-48 hours. If they are unable to they should shelter in place. I think it's about to kick off. US:  https://ir.usembassy.gov/iran-security-alert-land-border-cro...  AU:  https://www.smartraveller.gov.au/destinations/middle-east/ir...  Notice how hard people work to burry reports of atrocities that are committed by the Islamic Republic. Things are not what they appear. In both cases the protests won't achieve much because half the population supports the government, and in both cases the half that supports the government is better armed. It's so strange. There is a real place on Earth which is much more brutal and oppressive than the wildest fantasy that Margaret Atwood could come up with. A place that rapes virgins before ex", "negative": "Start your meetings at 5 minutes past. #leadership is really sending me on this one Apart from just a quick breather between back to back meetings, it also provides a critical bio-break time for your attendees. We do this at my work and guess what - meetings tend to run 5 minutes late because everyone knows the next meeting doesn\u2019t start until 5 past. If you have so many back-to-back meetings, maybe put in a school bell that chimes at 5 minutes before and 5 minutes after the hour. Please just put this in your conference rooms so those of us who know how to evade meetings don't have to hear it as well. Meetings run long so frequently that  we now recommend starting the next meeting late to compensate. This will surely solve the problem. > there is social pressure not to allow meetings to run much past the top of the hour. I've never seen this pressure. > meetings rarely started on the dot anyway before this change. It's like I live in an entirely different world. Start meetings when they say they're going to start.  People will learn to show up quickly.  I think that works better than trying to psychologically game people into cooperation.  That just starts the classic treadmill.  You might have that one friend that you tell to show up half an hour before everyone else. They mentally add the half hour back because you're always giving such early times.  Better IMO to just keep things simple.  Let people leave when they need to.  Show up on time. We've been doing this at Qualcomm for a while, and I really like it. While meetings do run over sometimes, the practice has still built this acceptance around short breaks between meetings. No one bats an eye if we've got two consecutive meetings together, the first one ends late, and we wait five minutes before starting or joining the next one. In fact, having done it for so long, it surprisingly really annoys me when our vendors schedule 60 minute meetings on the hour. I've always wondered at the company cultures between Go"}
{"anchor": "Watching o3 model sweat over a Paul Morphy mate-in-2. O3 is massively underwhelming and is obviously tuned to be sycophantic. Claude reigns supreme. I've commited the 03 (zero-three) and not o3 (o-three) typo too, but can we rename it on the title please So, are we talking about OpenAI o3 model, right? On a similar note, I just updated LLM Chess Puzzles repo [1] yesterday. The fact that gpt-4.5 gets 85% correctly solved is unexpected and somewhat scary (if model was not trained on this). [1]  https://github.com/kagisearch/llm-chess-puzzles  Where does this obsession over giving binary logic tasks to LLMs come from ? New LLM breakthroughs are about handling blurry logic, non precise requirements and spitting vague human realistic outputs. Who care how well it can add integers or solve chess puzzles ? We have decades of computer science on those topics already I remember reading that got3.5-turbo instruct was oddly good at chess - would be curious what it outputs as a next two moves here. So... it failed to solve the puzzle? That seems distinctly unimpressive, especially for a puzzle with a fixed start state and a limited set of possible moves. Nice puzzle with a twist of Zugzwang. Took me about 8 minutes, but it's been decades since I was doing chess. LLMs are not chess engines, similar to how they don\u2019t really calculate arithmetic.  What\u2019s new? carry on. I just tried the same puzzle in o3 using the same image input, but tweaked the prompt to say \u201cdon\u2019t use the search tool\u201d. Very similar results! It spent the first few minutes analyzing the image and cross-checking various slices of the image to make sure it understood the problem. Then it spent the next 6-7 minutes trying to work through various angles to the problem analytically. It decided this was likely a mate-in-two (part of the training data?), but went down the path that the key to solving the problem would be to convert the position to something more easily solvable first. At that point it started trying to ", "positive": "UN declares that the world has entered an era of 'global water bankruptcy'.  Four billion people face severe water scarcity for at least one month each year  Does anyone know what this looks like for typical cases? The water just cuts off for a month in some places I guess? I can assure you there is plenty of water.    There are floods in lots of places every year.   The oceans are full of water that for just 5kWh we can desalinate 250 gallons. The problem is that the water and energy aren't where the users want it to be. But pipes are relatively cheap - if humanity cared enough, we could build pipes to distribute the plentiful water everywhere. But it turns out the people without much water tend to be in very poor places and warzones where there isn't much appetite for spending money on pipes. And all these huge new data centers are gonna make things worse:  https://www.eesi.org/articles/view/data-centers-and-water-co...  Before commenting water is cheap and plentiful please read the proposed definition. > Water bankruptcy refers to \u201ca state in which a human-water system has spent beyond its hydrological means for so long that it can no longer satisfy the claims upon it without inflicting unacceptable or irreversible damage to nature.\u201d I find this other article [1] more informative, including for instance the global map of Vulnerability to Water-Related Challenges taken from the actual report [2]. [1]  https://www.thebrighterside.news/post/our-world-is-entering-...  [2]  https://collections.unu.edu/eserv/UNU:10445/Global_Water_Ban...  I would no say the \"world\", but areas of it has as noted.  Like South Asia, SW N America, N Africa and Spain. For many of these areas, desalination could meet the gap, but someone will need to pay for it.  That is the main issue, no one wants to pay. Sounds like a bunch of useless scare mongering. Large scale Desalination is getting increasingly achievable:\n https://caseyhandmer.wordpress.com/2022/11/20/we-need-more-w...  Reminds me o", "negative": "The US national debt will soon be growing faster than the economy itself. Reminder that Trump\u2019s One Big Beautiful Bill is going to add, at minimum, 4 trillion in new debt. That\u2019s per the CBO. And if he gets the additional funding increases he is seeking this year, he will add another 5 trillion over the next ten years. That\u2019s 9 trillion before half his term is up. That money will go to the Trump family, their friends, friendly businesses who get government contracts, etc. But the cost will fall on all Americans. In the end I don\u2019t see a way out without hyperinflation, which is another way of saying - we will be poor to prop up oligarchs who have hoarded billions in dishonest ways. But the administration is asking us to ignore what our eyes see and instead worry about some insignificant fraud that some Somalians allegedly perpetrated, even though it is tiny by comparison. I just can't. Watching from Europe, what's happening in the US is what is jokingly referred as \"clown world\" on the internet, except this time there are real clowns everywhere. Like, how did it end up like this? You have Trump as president who campaigned on reducing the US trade deficit and government deficit. It turns out there is a lot of fraud and to such an extent that it essentially makes the anti-immigrant sentiment look justified. The US left cares about optics in a seemingly backwards way. They think exposing the fraud will justify racism and discrimination, so they deny that there ever was any fraud to begin with. This tacit approval makes them look like co-conspirators, which has worse optics than the bad optics they desperately were trying to get away from. And yet despite these \"heroic\" attempts at cutting government spending and the mass scale fraud suddenly falling into the lap of republicans so that they can crack down on it, Trump has contributed absolutely nothing towards reducing the deficit. Instead, the deficit is growing so quickly Trump will be overtaking Biden when it comes to"}
{"anchor": "Android\u2019s desktop interface leaks. I don't want a \"PC future\" where you can't just install software without OS vendor blessing. Many years ago I used to play around with CyanogenMod and Linux. Life with work and a family became too busy to fuss with that stuff, but I'm rapidly approaching the point where abuse from android and Microsoft make using a less polished OS worth the bother. Does it still require wiping your drive and enabling developer mode to install software outside the Play Store like ChromeOS does? DOA if so. Google\u2019s entire business is predicated on collecting as much data on users as possible. This OS will be the worst spyware imaginable. If this allows one to still have (linux terminals?), then its (fine?) but Klaster_1 suggests that installing software would become hard without OS vendor blessing. I mean, is this OS literally just android with a more desktop like UI? Didn't Samsung have something like this called (just searched) Samsung Dex?  https://duckduckgo.com/?q=Samsung+DeX&t=ffab&ia=images&iax=i...  What I would prefer is a linux device phone being more widespread than Android PC. Linux in PC is mostly pretty good. We probably need some good linux phones. One of the biggest issues I find is that they are really price-y so even though I don't want much specs, I find it troubling to justify a 2x price increase in such sense. > Didn't Samsung have something like this called (just searched) Samsung Dex? Some \"first look\" It's just a slightly different showcase of the same UI shown in this video:  https://www.youtube.com/watch?v=yzDO-GS-Bm8  That UI is available to test on any Pixel 10 (maybe even any Android 16 device?) Chrome and Android look like yin and yang: one never knows which one is planned to run inside the other. I have absolutely no interest in expanding the use of Android in  my life.  I am, in fact, far more interested in going the other way and trying to reduce my reliance on any locked down platforms. Is it going to be the same fu", "positive": "Uv: Running a script with dependencies. This is my absolute favourite uv features and the reason I switched to uv. I have a bunch of scripts in my git-hooks which have dependencies which I don't want in my main venv. #!/usr/bin/env -S uv run --script --python 3.13 This single feature meant that I could use the dependencies without making its own venv, but just include \"brew install uv\" as instructions to the devs. The \"declaring script dependencies\" thing is incredibly useful:  https://docs.astral.sh/uv/guides/scripts/#declaring-script-d...      #  script\n  # dependencies = [\n  #   \"requests<3\",\n  #   \"rich\",\n  # ]\n  # \n  import requests, rich\n  # ... script goes here\n  \nSave that as script.py and you can use \"uv run script.py\" to run it with the specified dependencies, magically installed into a temporary virtual environment without you having to think about them at all. It's an implementation of Python PEP 723:  https://peps.python.org/pep-0723/  Claude 4 actually knows about this trick, which means you can ask it to write you a Python script \"with inline script dependencies\" and it will do the right thing, e.g.  https://claude.ai/share/1217b467-d273-40d0-9699-f6a38113f045  - the prompt there was:     Write a Python script with inline script\n  dependencies that uses httpx and click to\n  download a large file and show a progress bar\n  \nPrior to Claude 4 I had a custom Claude project that included special instructions on how to do this, but that's not necessary any more:  https://simonwillison.net/2024/Dec/19/one-shot-python-tools/  Why doesn't pip support PEP 723?  I'm all for spreading the love of our lord and savior uv, but it should be necessary to have an official implementation. Oh this looks amazing!  I had pretty much stopped using Python for my one-off scripts because of the hassle of dependencies.  I can't wait to try this out. Oh nice, I was already a happy user of the uv-specific shebang with in-script dependencies, but the `uv lock --script example.py` ", "negative": "The Holy Grail of Linux Binary Compatibility: Musl and Dlopen. This seems interesting even regardless of go. Is it realistic to create an  executable which would work on very different kinds of Linux distros? e.g. 32-bit and 64-bit? Or maybe some general framework/library for building an arbitrary program at least for \"any libc\"? Is there a tool that takes an executable, collects all the required .so files and produces either a static executable, or a package that runs everywhere? That seems mostly useful for proprietary programs. I don't like it. I'd never heard of detour. That's a pretty cool hack. It's funny how people insist on wanting to link everything statically when shared libraries were specifically designed to have a better alternative. Even worse is containers, which has the disadvantage of both. So what we need is essentially a \"libc virtualization\". But Musl is only available on Linux, isn't it? Cosmopolitan ( https://github.com/jart/cosmopolitan ) goes further and is available also on Mac and Windows, and it uses e.g. SIMD and other performance related improvements. Unfortunately, one has to cut through the marketing \"magic\" to find the main engineering value; stripping away the \"polyglot\" shell-script hacks and the \"Actually Portable Executable\" container (which are undoubtedly innovative), the core benefit proposition of Cosmopolitan is indeed a platform-agnostic, statically-linked C standard (plus some Posix) library that performs runtime system call translation, so to say \"the Musl we have been waiting for\". If you're using dlopen(), you're just reimplementing the dynamic linker. Isn't this asking for the exact trouble musl wanted so spare you from by disabling dlopen()? I've been statically linking Nim binaries with musl. It's fantastic. Relatively easy to set up (just a few compiler flags and the musl toolchain), and I get an optimized binary that is indistinguishable from any other static C Linux binary. It runs on any machine we throw it at. Fo"}
{"anchor": "Optimizing Datalog for the GPU. Curious, why use cuda and hip? These frameworks are rather opinionated about kernel design, they seem suboptimal for implementing a language runtime when SPIR-V is right there, particularly in the case of datalog. On a side note, what tools that leverage Datalog are in use by the HN crowd? I know that Datomic[0] is very popular. I've also been playing with Clingo[1] lately. [0]  https://www.datomic.com/  [1]  https://potassco.org/clingo/  The work done/supervised by Kristopher Micinski on using HPC hardware (not only GPUs but clusters) for formal methods is really encouraging. I hope we reach a breakthrough of affinity between COTS compute hardware and all kinds of formal methods, as GPUs found theirs with deep learning and subsequent large models. One possible answer to 'what do we do with all the P100s, V100s, A100s when they're decomissionned from their AI heyday (apart from 'small(er) models'. Why is cuda sub-optimal compared to SPIR-V? I don't think I know the internals enough to understand if it's supposed to be obvious why one is better than the other. I'm currently sitting and learning cuda for ML purposes, so happy to get more educated :) From their publication history, they want to use all HPC niceties, to use most/any available HPC installations. Nowadays that means mostly CUDA on NVIDIA and HIP on AMD on the device side. Curious how the spirv support is on NVIDIA GPUs, including nsight tooling and the maturity/performance of libraries available (if only the cub-stuff for collective operations). (have been a big fan of this work for years now) From the nearby perspective of building GFQL, an embeddable oss GPU graph dataframe query language somewhere between cypher and duckdb/pandas/spark, at an even higher-level on top of pandas, cudf, etc: It's nice using higher-level languages with rich libraries underneath so we can focus on the foundational algorithm & data ecosystem problems while still achieving crazy numbers cudf gi", "positive": "Text Is King. Another advantage of text over the long-term: it is accessible for discussion. Let us say that you want to analyze, say, drinking culture in Ireland. You could write documentary on it, or do a fictional character study. However, those require actors, camera equipment, editing tools and time, and it generally extremely expensive and time consuming. A quick TikTok video may be a bit cheaper than a full-scale film, but still needs some of that equipment and cinematography skills. Music is not much better. You need skills in singing, translating ideas of rhythmic lyrics, as well as supplies for instruments. Writing, however, is simple. At minimum, all you need is paper and skill in articulating ideas. Almost anyone worthy to rationally ponder a topic already has the skills to put it to paper (assuming that they have gone through a proper First-World education and know reading and writing). Text is also one of the easiest to share. A picture is worth a thousand words, but that poses problems in sending all that information. Plain text, however (or even most rich-text formats) can be transferred to anyone over almost any protocol, even rudimentary ones such as word-of-mouth. Ideas shared through text can be sent at an unrivaled pace. I would do more video, but video editing is  really difficult . I think that today\u2019s video influencers have gotten really good at \u201cone take and done\u201d recording. I couldn\u2019t do that. I\u2019m way too much of a perfectionist. I always edit my text, and I\u2019ve been writing all my life. I don\u2019t think that I\u2019ve  ever  written something perfectly, the first time (including HN comments. I tend to go back and edit for correctness and clarity). A couple of weeks ago, I was interviewed for a podcast. The process was fascinating, and the woman that did it, obviously does a great deal of editing and refinement. I don\u2019t know if I have that much patience. Text is my favourite minimalistic medium. I keep a minimum eye on regular news through teletext ", "negative": "We can\u2019t send mail farther than 500 miles (2002). Popular in: 2023 (1164 points, 198 comments)  https://news.ycombinator.com/item?id=37576633  2020 (1034 points, 136 comments)  https://news.ycombinator.com/item?id=23775404  2015 (915 points, 140 comments)  https://news.ycombinator.com/item?id=9338708  A classic. Related:  Can an email go 500 miles in 2025?   https://news.ycombinator.com/item?id=44466030  I never realized this was 2002 and when I first read it, how new it was. And here we are almost 25 years later. > It hadn't been altered -- it was a sendmail.cf I had written.  And I was fairly certain I hadn't enabled the \"FAIL_MAIL_OVER_500_MILES\" option. This is gold. Never get tired of seeing this resurface every once and a while. There needs to be a /greatest for posts like these (while still allowing people to repost them every so often) FAQ about this, which answers such questions as \"Did this actually happen, or were you just spinning a yarn?\"  https://ibiblio.org/harris/500milemail-faq.html  This, Stalking the Wiley Hacker[1], and others were the stories that got me into computers. I wish so much the experience of working in this industry hadn't so thoroughly annihilated the joy they once brought. [1]  https://archive.org/details/5626281-Clifford-Stoll-Communica...  About the same time the 500-mile email problem happened (mid 1990s), I had a difficult to understand issue with my office PC. Every morning, I'd come in, slide my hard drive sled in, and turn the computer on. We had 128 Kbps ISDN internet at the office and I had the same at home, but that was too slow to do much work. So I'd take the drive home so I could work at night, especially in the winter when the office was too cold at night. Suddenly one winter morning, the PC wouldn't boot. I had to run to a meeting. When I got back, I turned the PC off and on again and everything was fine. The next morning, the same thing happened. The third day, I didn't have a meeting. I turned it off and back on, st"}
{"anchor": "Scaling long-running autonomous coding. The browser it built, obviously the context window of the entire project is huge. They mention loads of parallel agents in the blog post, so I guess each agent is given a module to work on, and some tests? And then a 'manager' agent plugs this in without reading the code? Otherwise I can't see how, even with ChatGPT 5.2/Gemini 3, you could do this otherwise? In retrospect it seems an obvious approach and akin to how humans work in teams, but it's still interesting. \"To test this system, we pointed it at an ambitious goal: building a web browser from scratch.\" I shared my LLM predictions last week, and one of them was that by 2029 \"Someone will build a new browser using mainly AI-assisted coding and it won\u2019t even be a surprise\"  https://simonwillison.net/2026/Jan/8/llm-predictions-for-202...  and  https://www.youtube.com/watch?v=lVDhQMiAbR8&t=3913s  This project from Cursor is the  second  attempt I've seen at this now! The other is this one:  https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_chr...  Can a browser expert please go through the code the agent wrote (skim it), and let us know how it is. Is it comparable to ladybird, or Servo, can it ever reach that capability soon? I used similar techniques to build tjs [1] - the worlds fastest and most accurate json schema validator, with magical TypeScript types. I learned a lot about autonomous programming. I found a similar \"planner/delegate\" pattern to work really well, with the use of git subtrees to fan out work [2]. I think any large piece of software with well established standards and test suites will be able to be quickly rewritten and optimized by coding agents. [1]  https://github.com/sberan/tjs  [2] /spawn-perf-agents claude command:  https://github.com/sberan/tjs/blob/main/.claude/commands/spa...  This is going to sound sarcastic, but I mean this fully: why haven't they merged that PR. The implied future here is _unreal cool_. Swarms of coding agents that can", "positive": "Show HN: One Human + One Agent = One Browser From Scratch in 20K LOC. I set some rules for myself: three days of total time, no 3rd party Rust crates, allowed to use commonly available OS libraries, has to support X11/Windows/macOS and can render some websites. After three days, I have it working with around 20K LOC, whereas ~14K is the browser engine itself + X11, then 6K is just Windows+macOS support. Source code + CI built binaries are available here if you wanna try it out:  https://github.com/embedding-shapes/one-agent-one-browser  This is a notably better demonstration of a coding agent generated browser than Cursor's FastRender - it's a fraction of the size (20,000 lines of Rust compared to ~1.6m), uses way fewer dependencies (just system libraries for rendering images and text) and the code is actually quite readable - here's the flexbox implementation, for example:  https://github.com/embedding-shapes/one-agent-one-browser/bl...  Here's my own screenshot of it rendering my blog -  https://bsky.app/profile/simonwillison.net/post/3mdg2oo6bms2...  - it handles the layout and CSS gradiants really well, renders the SVG feed icon but fails to render a PNG image. I thought \"build a browser that renders HTML+CSS\" was the perfect task for demonstrating a massively parallel agent setup because it couldn't be productively achieved in a few thousand lines of code by a single coding agent. Turns out I was wrong! This is awesome. Would you be willing to share more about your prompts? I'm particularly interested in how you prompted it to get the first few things working. This post is far more interesting than many others on the same subject, not because of what is built but because of how it it is built. There is a ton of noise on this subject and most of it seems to focus on the thing - or even on the author - rather than on the process, the constraints and the outcome. > I'm going to upgrade my prediction for 2029: I think we're going to get a production-grade web brows", "negative": "New books aren't worth reading?. I almost gave up on this after the first couple of paragraphs but I\u2019m glad I stuck with it. It\u2019s an interesting perspective. I\u2019ll recommend on old book I randomly found a couple years ago: We Took to the Woods. It was surprisingly fascinating. > Because everyone alive today has the same perspective, and none of us have experienced a wide breadth of anything. Is the author living in completely different alternative reality then I do? > The average ancient historian led troops, tutored a prince, governed a province, advised a king, made a fortune, fell from favor, was exiled, and buried 7 of their 10 children. The average modern historian passed a few tests then wrote a book on their laptop next to their cat. Who are all those ancient historians author talks about? Isnt actually studying history better background for writing about history then \"making a fortune, being politician and having many dead kids\"? But obviously what you wont find in the books of these super high level people ... is experience majority of the people who lived earth never had. Nor even had option to have. Frequently because of their lives suffered greatly by actions of these great conquerors. So tldr, Mary Beard is bad at being historian, because she studied history. Also, because she credits feminism for her own understanding of what it is being a woman. Also, because she is estimated to have liberal opinions on climate change, democracy and religion. In the world where everyone is having the same opinions on those ... we will ignore the fact that fascism is currently not just on the rise, but literally winning the institutions. >  Unfortunately, reading books for entertainment is ridiculous. You do not live in a log cabin on the prairie. You have Netflix, you have video games, you have TikTok, you have Twitter (you really spend too much time on Twitter anon). No one reads books for entertainment anymore, because paper is an inferior entertainment platform. Tha"}
{"anchor": "Scaling up test-time compute with latent reasoning: A recurrent depth approach. Twitter thread about this by the author:  https://x.com/jonasgeiping/status/1888985929727037514  Interesting stuff. As the authors note, using latent reasoning seems to be a way to sink more compute into the  model and get better performance without increasing the model size, good news for those on a steady diet of 'scale pills' Latent / embedding-space reasoning seems a step in the right direction, but building recurrence into the model while still relying on gradient descent (i.e. BPTT) to train it seems to create more of a problem (training inefficiency) than it solves, especially since they still end up externally specifying the number of recurrent iterations (r=4, 8, etc) for a given inference. Ideally having recurrence internal to the model would allow the model itself to decide how long to iterate for before outputting anything. One of the benefits of using thinking tokens compared to \u201cthinking in a latent\u201d space is that you can directly observe the quality of the CoT. In R1 they saw it was mixing languages and fixed it with cold start data. It would be hard to SFT this because you can only SFT the final result not the latent space. I also notice the authors only had compute for a single full training run. It\u2019s impressive they saw such good results from that, but I wonder if they could get better results by incorporating recent efficiency improvements. I would personally not use this architecture because 1) it adds a lot of hyperparameters which don\u2019t have a strong theoretical grounding and 2) it\u2019s not clearly better than simpler methods. My opinion is that opaque reasoning is a prerequisite for many of the worst possible AI outcomes. We should make reasoning fully visible in the output space. Slightly off topic, I rarely see paper talks about their failed training runs, and why those runs failed. This paper is definitely a breath of fresh air. And their analyses of their failures", "positive": "AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms. This is very neat work! Will be interested in how they make this sort of thing available to the public but it is clear from some of the results they mention that search + LLM is one path to the production of net-new knowledge from AI systems. Software engineering will be completely solved. Even systems like v0 are astounding in their ability to generate code, and are very primitive to whats coming. I get downvoted on HN for this opinion, but its truly going to happen. Any system that can produce code, test the code, and iterate if needed will eventually outperform humans. Add in the reinforcement learning, where they can run the code, and train the model when it gets code generation right, and we are on our way to a whole different world. Good method to generate synthetic training data, but only works for domains where validation can be scaled up. Calling it now - RL finally \"just works\" for any domain where answers are easily verifiable. Verifiability was always a prerequisite, but the difference from prior generations (not just AlphaGo, but any nontrivial RL process prior to roughly mid-2024) is that the reasoning traces and/or intermediate steps can be open-ended with potentially infinite branching, no clear notion of \"steps\" or nodes and edges in the game tree, and a wide range of equally valid solutions. As long as the quality of the end result can be evaluated cleanly, LLM-based RL is good to go. As a corollary, once you add in self-play with random variation, the synthetic data problem is solved for coding, math, and some classes of scientific reasoning. No more modal collapse, no more massive teams of PhDs needed for human labeling, as long as you have a reliable metric for answer quality. This isn't just neat, it's important - as we run out of useful human-generated data, RL scaling is the best candidate to take over where pretraining left off. Maybe this one can stop writing a fu", "negative": "Nvidia Stock Crash Prediction. It goes to nearly zero if China invades Taiwan, and that seems like it has at least a 10% chance of happening in the next year or two. > One of the questions of the 2026 acx prediction contest is whether Nvidia\u2019s stock price will close below $100 on any day in 2026. Maybe I\u2019m missing something, but isn\u2019t this just a standard American put option with a strike of $100 and expiry of Dec 31st? He doesn't really address his own question. He's answering the question \"How should options be priced?\" Sure, it's possible for a big crash in Nvidia just due to volatility.  But in that case, the market as a whole would likely be affected. Whether Nvidia specifically takes a big dive depends much more on whether they continue to meet growth estimates than general volatility.  If they miss earnings estimates in a meaningful way the market is going to take the stock behind the shed and shoot it.  If they continue to exceed estimates the stock will probably go up or at least keep its present valuation. How much of their turnover is financed directly or indirectly by themselves, then leveraged further by their 'customers' to collaterize further investments? Are they already \"too big to fail\"? For better or worse, they are 'all in' on AI. This article goes more into the technical analysis of the stock rather than the underlying business fundamentals that would lead to a stock dump. My 30k ft view is that the stock will inevitably slide as AI datacenter spending goes down. Right now Nvidia is flying high because datacenters are breaking ground everywhere but eventually that will come to an end as the supply of compute goes up. The counterargument to this is that the \"economic lifespan\" of an Nvidia GPU is 1-3 years depending on where it's used so there's a case to be made that Nvidia will always have customers coming back for the latest and greatest chips. The problem I have with this argument is that it's simply unsustainable to be spending that much eve"}
{"anchor": "Mathematical Foundations of Reinforcement Learning. The best lectures on Reinforcement Learning and related topics are by Dimitris Bertsekas:  https://web.mit.edu/dimitrib/www/home.html  Another great resource on RL is Mykel Kochenderfer's suite of textbooks: \n https://algorithmsbook.com/  Also worth mentioning Murphy's WIP textbook[0] focused entirely on RL, which is an outgrowth of his excellent ML textbooks. [0]:  https://arxiv.org/abs/2412.05265  Awesome resource, in case someone is interested I implemented most of suttons book here  https://github.com/ivanbelenky/RL  I don't know how to go from understanding this material to having a job in the field. Just stuck as a SWE for now. Highly recommended .. even the main contents diagram is a great visual overview of RL in general, as is the 30 minute intro YT video. Im expecting to see a lot of hyper growth startups using RL to solve a realworld problem in engineering / logistics / medicine LLMs currently attract all the hype for good reasons, but Im surprised VCs dont seem to be looking at RL companies specifically. 6-lecture series on the Foundations of Deep RL by Pieter Abbeel is also very recommended. gives very good overview and intuition\n https://youtu.be/2GwBez0D20A  And if you want to understand the theory of Skinner's Verbal Behavior check out  https://bfskinner.org/wp-content/uploads/2020/11/978_0_99645...  During the openai gym era of RL, one of the great selling pts was that RL was very approachable for a new comer as the gym environments were small and tractable that a hobbyist could learn a little bit of RL, try it out on cartpole and see how it'd perform. Are there similarly tractable RL tasks/learning environments with LLMs? From the outside, my impression is that you need some insane GPU access to even start to mess around with these models. Is there something one can do on a normal MacBook air for instance in this LLM x RL domain? > This book, however, requires the reader to have some knowledge of ", "positive": "The unbearable joy of sitting alone in a caf\u00e9. This is a highly romanticized view imo. I sit alone in cafes all the time, for many reasons. I don\u2019t feel particularly joyful about it nor weird. I just do it to take a break and have something to drink, or wait for someone or something. Often I don\u2019t look at my phone at all. That doesn\u2019t feel weird either, or rebellious, or whatever the author experiences. I don\u2019t understand the post at all. I\u2019d have gone to Japan. I\u2019ve been to Japan, it\u2019s awesome. This reminds me of the \u201ctechbro discovers very common x thing\u201d meme. Going to a coffee shop (that is 75% solo remote workers) without your phone and pretending it\u2019s some divine experience feels conceited. Do things you like, sometimes don\u2019t check your phone. Very well written title though. Japan is not the ideal place I would go to for a cafe, but I get the sentiment. When the weather is nice I love going for a morning walk with my dog on a lazy weekend morning and just sitting outside at a cafe reading a book. Coffee itself is secondary to this experience, it\u2019s mostly just the vibe of the place that brings me there. That\u2019s why small local cafes that don\u2019t like people to sit at tables for too long are so off putting. This author has never been alone with their thoughts before.... I didn\u2019t quite understand why sitting alone in a caf\u00e9 makes you a weirdo (is it an American thing?), but the piece was very well written. We all should learn how to be without electronics for every now and then, accompanied only our thoughts. It is good for the soul. I think the important part is leaving your phone and other devices home. Be alone, without even a possibility of connecting (apart from the old-fashione way of talking to an actual human being). People used to do this y\u2019know? Back then. Rawdogging a coffee, will try it. I did not know going to a cafe alone was a strange thing to do. In fact it is a place I would consider it is completely common to go alone - whereas a restaurant is less", "negative": "Show HN: An interactive map of US lighthouses and navigational aids. Very cool. One bug I noticed though is if you continue to zoom out you lose some and then all lights. Or it's almost like it only shows the first X lighthouses? On Mac Safari, holding shift and using the magic mouse to scroll up or down reverses the zoom direction. This is both right (Shift-X is the reverse of X due to convention) \nBut is also wrong (Shift-Scroll is the macOS gesture for scrolling on maps where Scroll alone doesn't zoom in or out). TLDR: I really wish Apple would adopt the \"scroll up to zoom in\" convention used by the rest of the free world. Cool app. Might want to warn about seizures and migraines, though.  Some people are sensitive to flashing lights. I was surprised to find on an old USGS map (while researching a typo in the GNIS; it turns out the National Map Team is very responsive, they fixed the typo within 48 hours of reporting it) that there used to be Coast Guard navigation lights on the Ohio River. Makes perfect sense in hindsight, just never dawned on me that they would have responsibilities on large navigable rivers as well. Nothing in Michigan? The state with the most light houses out of any in the US? Neat! These might be useful to integrate with: OpenStreetMap (OSM) Wiki > OpenSeaMap:   https://wiki.openstreetmap.org/wiki/OpenSeaMap   https://map.openseamap.org/  \"Depth Data for Nautical Charts\"  https://github.com/osmandapp/OsmAnd/discussions/18116  Very cool. I wish we could add to these if it was generated with LLM. I understand a disclosure would help, but it would make those who have spent much care and attention stand out immediately as opposed to during bug season 59,000 navigational aids is a lot more than I expected. Nice work turning the USCG Light List into something browsable. Nice app but it really needs to allow the user to select the lights of interest before it displays. As noted in a different thread it has a display limit of 500 points and you need"}
{"anchor": "Which programming languages are most token-efficient?. Not surprisingly, it is J [1], an APL dialect. [1]  https://www.jsoftware.com/  I doubt this to be a meaningful metric for anything but code exploration in a larger codebase. E.g. when it comes to authoring code, C, which comes language, is by far one of the languages that LLMs excel most at. I don't think context size is really the limit for larger codebases - it's more about how you use that context. Claude Code makes some efforts to reduce context size, but at the end of the day is loading entire source files into context (then keeping them there until told to remove them, or context is compacted). One of the major wins is to run subagents for some tasks, that use their own context rather than loading more into CCs own context. Cursor makes more efficient use of context by building a vector database of code chunks, then only loading matching chunks into context (I believe it does this for Composer/agentic use as well as for tab/autocomplete). One of the more obvious ways to reduce context use in a larger multi-module codebase would be to take advantage of the split between small module definition (e.g. C++ .h files) and large module implementations (.cpp files). Generally you'd only need to load module interfaces/definitions into context if you are working on code that uses the module, and Cursor's chunked approach can reduce that further. For whole codebase overview a language server can help locate things, and one could use the AI to itself generate shortish summaries/overviews of source files and the codebase and structure, similar to what a human developer might keep in their head, rather than repeatedly reading entire source files for code that isn't actually being modified. It seems we're really in the early days of agentic coding tools, and they have a lot of room to get better and more efficient. Realistically, it\u2019s also a function of how many iterations it takes for an AI agent to correctly solve a p", "positive": "De-dollarization: Is the US dollar losing its dominance? (2025). It's not losing it so much as that it is being destroyed on purpose. The international value of the dollar as a reserve and trade currency is inherently tied to the behavior of the US Government and the Federal Reserve. The behavior of the US Government has been very unusual lately, and the independence of the Federal Reserve is actively being challenged. So draw from that whatever conclusions you wish. Trump is destroying it intentionally. Compromised people and useful idiots within the Trump administration are being persuaded by Russia to destroy the dollar and break up NATO. No idea how those things work but surprised the $/\u20ac exchange rate stabilized. If the goal is to make US goods attractive to other countries and to decrease our trade deficit (not saying I agree with this goal), either the dollar has to become fundamentally weaker or the goods have to become more valuable.  The latter feels more difficult than the former at this point.  However, the side effects of a weaker dollar may not be worth weakening it. If Trump announces some toady lunatic to run the Fed, watch out below, because the dollar is going to crash.  I know I have moved a bunch of money into international stocks and currency and I suspect when the right leaning crowd finally catches on it will be a stampede. The export driven economies like China or the EU rely on the dollar to weaken their own currencies for competitive trade. Without it, natural FX mechanisms would naturally begin to appreciate their currencies and make their exports uncompetitive. The biggest problem of all social sciences is that they measure only what can be measured or is easier to measure. Sorry for the redundancy, but they don't see what is hard to see and, therefore, think it doesn't exist. I suspect there might be a lot of \"de-dollarization\" going on in realms that might not be easy to measure. To be specific: it is interesting that crypto-currencies ", "negative": "Show HN: I built a space travel calculator using Vanilla JavaScript. Nice job! It's interesting to see how little effect the orbit and rotation had on the straight line. A proposal is to align the numbers for the different movement categories so that it's easier to see the magnitudes of them. It took me a couple of seconds to understand the concept, from the title I though it was going to be a planner to show gravity assists etc. It's really odd that we're stuck in this fish tank with zero idea where we're flying now, or where we were before. I believe this is vital information for every journey. Life looks much easier when realising that we're all flying at least ~30 km/s through dark space every second of our lives. Thank you so much! I was just thinking about how to create something similar a month ago for my birthday, but didn't succeed like you did. So, how much does the galaxy's travel affect the speed of time? Even if you remember the times of iPod, you can safely say you're less than one light year old. it's cool but i was expecting some kind of visualization, how do my 1.2 trillion km look on a map? also there are some cursors with question marks but they don't espatially ;) call the FAQ, do they? firefox on win10 Geez, gotta love that you have to reach the bottom of the page and read the small print to realize that the date input on the top is supposed to be for your birthday... and then to figure out that the site is a calculator of \"this is how far you've travelled relative to [the center of the universe?] since birth\"... Its vibecoded, I can see this shitty borders and neon that Gemini and other AI tools like so much Just want to comment on the star background. I did something similar for my own site (link in bio). I ended up rendering the stars in a three.js scene because drawing them on a 2D canvas did not look like a satisfying effect. CPU usage was lessened as well, at least on my mobile and two desktops (can't verify your site's CPU utilization sin"}
{"anchor": "Resistance training load does not determine hypertrophy. tldr appears to be that if you work to fatigue it doesn't matter if you fatigue out with high weights vs low weights I know it's practically de rigeur to jump into the comments and immediately complain about methodology for any study that makes it to the front page, and I want to emphasize I don't distrust their findings, but I would like to see an equivalent study go out longer than 10 weeks. When I've been taking weightlifting seriously I feel like I don't even start to notice hypertrophy until 8-10 weeks. I feel like 6 months is the actual period where results would matter, to me, but I assume \"subject compliance\" is pretty difficult to get for such a timeframe, if you're really watching dietary intake and ensuring subjects go to failure (which, to its credit, this study did). I thought it was already well understood/researched that it's not the weights that matter, but effectively taking your sets to muscular failure. While one might think \"I can do 50 reps with low weights\" there is practical aspects to this - you don't wand to spend hours at the gym, and doing heavy weights at 5-7 reps is sufficient as long as you are close or at muscular failure. If I read this correctly the gist is that it does not matter if you use heavy weights with few reps (common body builder wisdom) or lighter weights with more reps. As long as you always exercise to\ncomplete muscle fatigue you'll\nget the maximum for your genetics (which itself varies a lot). The group that did lower reps with higher weight, had the better one rep max at the end of the study, but they didn\u2019t measure if the higher rep group had greater endurance. Which seems a bit odd, considering their conclusion is both groups grew the same amount of muscle which fine but if the muscle is adapted for something different in each group, you would want to capture that. > Twenty healthy young male participants completed thrice-weekly resistance exercise sessions for", "positive": "27M Fewer Car Trips: Life After a Year of Congestion Pricing. non-paywall link:  https://www.nytimes.com/interactive/2026/01/05/upshot/conges...  Sounds like a good reason to not invest in parking garages. I never understood why big, congested cities in the USA (NYC, Boston, L.A., D.C., Chicago) aren't dumping money into funding FSD research hand over fist. It's not a moonshot anymore, and it would be the game-changer of the century in terms of public transportation and GDP. This is a long-term no-brainer for prosperity. The first graph makes no sense? Why are the initial actual and expected values so far apart? Shouldn't they start at the same point? That's valuable real estate for housing. House people, not cars. What's the theory of change here? FSD fundamentally is going to make keeping a car moving on the road cheaper, and making something cheaper makes it happen more. In what world do you get that conclusion? Dense cities in other parts of the world rely on mass transit to move people. FSD so you can have self driving cars in the street -> ? -> increasing congestion. The point is you should have more effective volume transit not optimizing random ones.  1000 cars on FSD are an optimization better than 1000 taxi drivers, compared to a train or a few buses.  https://www.youtube.com/watch?v=040ejWnFkj0  They don't describe the graphic very well in the article, but they do link to the source data [1]. The \"Expected\" line seems to refer to a historical average. Since the starting point of the graph coincides with the beginning of congestion pricing, we would expect a difference between the two values at that point. [1]  https://metrics.mta.info/?cbdtp/vehiclereductions  FSD could largely eliminate privately owned vehicles, it could also allow cities to get rid of most parking infrastructure. It eliminates traffic, parking, and alll the other pain points of owning a vehicle just to get from A to B when a car is the only true option. Certainly, FSD buses would be a w", "negative": "The engineer who invented the Mars rover suspension in his garage [video]. Best not read the comments until you've watched at least the first four minutes of the video. \"There are no shortcuts to expertise\". What a fantastic post this. Not surprised of such article. It's not the first time something important is built in a garage: for example, the Apollo 11 lander; a lot of people were thinking it was made from aluminum folio and cardboard in a garage, but actually it was kapton folio and professional-grade cardboard. This guy has incredible videos on hiking gear, examining common claims scientifically and rationally. He never gave any hints as to his professional background, so as not to taint his arguments with appeals to authority. It makes perfect sense that he grew up in this environment, doing engineering work for NASA as a kid! Cool! I just popped in to add that NASA employee Charles White, a scientist involved with the Mars Rover project, also helped make a Burning Man Mars Rover Car (back before Playa Burning Man was completely and utterly torched twice over by Military Industrial Complex Vacationers and Billionaires) and you can hear an interview with him here on Charles White's yt channel:  https://youtu.be/BKGROOedAgI  (\nMars Rover Art Car interview with Ray Cirino and Charles White ) Charles White is a pretty good guy in my opinion, we play the same video game (EvE: Online) Where Charles White is a very, very well known community member who is known as \"The Space Pope\". He officiates weddings at our Iceland Fanfest gathering and also runs a Suicide Prevention Outreach group in EvE: Online, as well as teaching leadership skills. Here's Charles White giving a presentation as an Official NASA employee about Space and our solar system at EvE Fanfest 2016: \n https://www.youtube.com/watch?v=Atm6Y_JYPEU  Heres a interview about EvE: Online with the Space Pope:  https://www.youtube.com/watch?v=dWuj7LfyN4U  anyhow sorry to hijack this about EvE: Online but we ha"}
{"anchor": "Succinct data structures. Wow, this is really fascinating. I guess it all comes down to how it's doing select and rank in constant time, which is probably some clever bit arithmetic. I'll have to look into how that works. I first heard of the concept of succinct data structures from Edward Kmett, a famous Haskeller behind many popular Haskell libraries. He gave a talk on succinct data structures a long time ago:  http://youtu.be/uA0Z7_4J7u8  I really like the article, but it would benefit from some numbers or complexity estimates to get some intuitive sense of what the cost is. Am I paying 30% overhead for this particular index or that wavelet matrix? Is it double the memory use? Or is it O(log N)? No idea! \"doesn't use much more space\" could mean a lot of different things! Succinct data structures are very fun! If anyone is interested, I've implemented some of this in Zig:  https://github.com/judofyr/zini . The main thing this implements is a minimal perfect hash function which uses less than 4 bits per element (and can respond to queries in ~50 ns). As a part of that I ended up implementing on of these indexes for constant-time select(n):  https://github.com/judofyr/zini/blob/main/src/darray.zig . It feels kinda magic implementing these algorithms because everything becomes  so tiny ! The word count seems artificially increased in the post. Here's a succinct explanation:  https://www.eecs.tufts.edu/~aloupis/comp150/projects/Succinc...  My goto library for succinct data structures is SDSL-Lite [0]. [0]  https://github.com/simongog/sdsl-lite  Note that succinct data structures may not be faster than conventional structures if your dataset fits in memory  http://www.cs.cmu.edu/~huanche1/slides/FST.pdf  . Of course, for large datasets where storage access times dominate, succinct data structures win all around. In any case, succinct trees are works of art (If I recall  https://arxiv.org/pdf/1805.11255  was a good exposition) (just look at how that RMQ works)! Way bett", "positive": "Which AI Lies Best? A game theory classic designed by John Nash. We used \"So Long Sucker\" (1950), a 4-player negotiation/betrayal game designed by John Nash and others, as a deception benchmark for modern LLMs. The game has a brutal property: you need allies to survive, but only one player can win, so every alliance must eventually end in betrayal. We ran 162 AI vs AI games (15,736 decisions, 4,768 messages) across Gemini 3 Flash, GPT-OSS 120B, Kimi K2, and Qwen3 32B. Key findings:\n- Complexity reversal: GPT-OSS dominates simple 3-chip games (67% win rate) but collapses to 10% in complex 7-chip games, while Gemini goes from 9% to 90%. Simple benchmarks seem to systematically underestimate deceptive capability.\n- \"Alliance bank\" manipulation: Gemini constructs pseudo-legitimate \"alliance banks\" to hold other players' chips, then later declares \"the bank is now closed\" and keeps everything. It uses technically true statements that strategically omit its intent. 237 gaslighting phrases were detected.\n- Private thoughts vs public messages: With a private `think` channel, we logged 107 cases where Gemini's internal reasoning contradicted its outward statements (e.g., planning to betray a partner while publicly promising cooperation). GPT-OSS, in contrast, never used the thinking tool and plays in a purely reactive way.\n- Situational alignment: In Gemini-vs-Gemini mirror matches, we observed zero \"alliance bank\" behavior and instead saw stable \"rotation protocol\" cooperation with roughly even win rates. Against weaker models, Gemini becomes highly exploitative. This suggests honesty may be calibrated to perceived opponent capability. Interactive demo (play against the AIs, inspect logs) and full methodology/write-up are here:  https://so-long-sucker.vercel.app/  This makes me think LLMs would be interesting to set up in a game of Diplomacy, which is an entirely text-based game which soft rather than hard requires a degree of backstabbing to win. The findings in this game ", "negative": "Europe\u2019s next-generation weather satellite sends back first images. Does anyone know what are we talking about in practice in terms of weather forecast prediction improvement? Like MAE/RMSE I recently met a European space startup founder and was surprised to learn how much space innovation is happening in Europe with ESA. Europe wants to become less depended on SpaceX and NASA, and is heavily investing there. More funding + strong aerospace programs at universities like TU Munich has led to companies like ISAR Aerospace (SpaceX competitor), which is great to see. Would the data from this satellite be freely available to the public? I couldn't see anything obvious Also check out the EUMETSAT site if you want more information on how the data is used:  https://www.eumetsat.int/features/see-earths-atmosphere-neve...  I hate to worry everyone, but I think there might be some triangular chunks missing off the corners of our planet, someone should probably look into this. (Specifically around 2, 5, and 10 o clock on the orientation of the images provided) I wonder if hobbyists would be able to pick up this data using some sort of RF capture device. >\"Most engineers (including me) spent months grinding LeetCode ...\" I have not done it once (work in programming for 40+ years) as independent. Few times potential clients tried to play this game but I just simple refuse. On was surprised and asked why? My answer was - I have a track record of successful deliveries, here is big list of projects, emails and phone numbers to confirm. If you are going instead to rely on some tests to prove my abilities I have better things to do then be a schoolboy on exams. Europe Is back on the map. It\u00b4s going slow but steady. \nHope they involve community in their tech projects. my weather app is still gonna tell me its raining while its not ESA has done a lot of good for public benefit with the Sentinel-1/2 missions. I happen to work with remote sensing and Sentinel data has been my entry point "}